{"meta":{"title":"blog","subtitle":"morty's blog","description":"morty的个人博客","author":"morty","url":"https://www.silenceboy.com","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2025-11-03T09:23:29.109Z","updated":"2021-09-27T07:27:40.000Z","comments":false,"path":"/404.html","permalink":"https://www.silenceboy.com/404.html","excerpt":"","text":""},{"title":"关于我","date":"2025-11-03T09:23:29.108Z","updated":"2025-08-11T12:10:13.000Z","comments":false,"path":"about/index.html","permalink":"https://www.silenceboy.com/about/index.html","excerpt":"","text":"简介 姓名：陈浩 17839718588（微信同） | silenceboychen@gmail.com https://github.com/silenceboychen 博客： https://www.silenceboy.com/ 河南大学 | 软件工程 | 本科 | 2012 - 2016 工作经验: 10年 核心技术能力 AI产品开发： 3年专业AI工具开发经验，掌握Function Calling、MCP（Model Context Protocol）、A2A（Agent-to-Agent）等核心技术的底层实现原理，能够基于这些技术构建智能化的产品解决方案 AI辅助开发： 熟练使用Cursor、Claude Code等前沿AI编码工具，通过AI辅助显著提升研发效率，在代码生成、调试优化、架构设计等环节实现智能化工作流 产品架构能力： 具备完整的产品全生命周期管理经验，包括产品需求分析、技术方案设计、业务架构规划等核心能力。近三年专注于AI产品领域，主导多个AI相关产品的规划设计和落地实施 产品创新能力： 具备从0到1构建AI产品的完整经验，能够将前沿AI技术转化为具有商业价值的产品方案 技术栈深度：10年+开发经验，以Node.js为核心技术栈，Python为辅助语言，在服务端架构、API设计、数据处理等方面具备一定的技术积累 工作经验亿咖通科技有限公司（2020.03-至今）经历过多次部门调整，参与和负责过不同的业务产品，主要分为3个阶段，按照时间倒序说明： 阶段三（AI工具链开发）负责全球研发中心的AI工具需求开发，目前担任AI工具组的Scrum Master。主要负责以下工作： 需求挖掘：与全球研发中心下的其他二级部门密切协作，基于目前的开发流程，分析可以使用AI工具提效的场景 知识分享：多次组织AI相关的技术分享会，推动研发对AI知识和工具的了解 产品规划：负责AI工具链产品规划、业务规划以及产品目标设定 业务洞察：深入理解业务逻辑和用户需求，将业务目标转化为技术实现方案 期间深入结合研发需求实现的产品有： 研发助手-代码生成详设：详细设计编写耗时耗力，结合研发的业务场景，深度分析后，搭建了代码生成详设项目，详设符合Aspice要求，目前支持C&#x2F;C++&#x2F;Java&#x2F;Kotlin 4种语言，已经正式投入使用。 研发助手-代码生成单元测试：Aspice对单测覆盖率要求100%，导致单测的编写工作量巨大，引入AI进行提效，目前针对Java&#x2F;Kotlin版本已经完成初步功能，部分研发在使用，C&#x2F;C++版本还在技术验证中。 Jira-MCP：使用MCP技术实现了jira系统的MCP服务，能够接入Cursor等AI工具进行语义化jira操作。 阶段二（工具链开发）负责OS研发中心工具链开发，虚线管理研发人员：10+。主要负责以下工作： 团队管理：负责服务端团队管理，工作分配和任务管理 架构设计：负责工具链产品的技术架构设计 产品规划：负责工具链产品规划、业务规划以及产品目标设定 绩效评估：建立客观的绩效评价体系，定期进行团队成员绩效反馈 期间深入结合研发需求实现的产品有： 日志智能分析工具：提出了结合jira系统的日志识别浏览器插件产品方案，快速吸引大量用户，并接入AI智能分析，jirabug自动查重等功能，目前日活：200+、月活400+用户 模拟器：使用qemu模拟器技术，实现了对设备的模拟，并能够基于模拟器控制台模拟车机信号数据，进行快速的研发自测，解决设备不足问题 WEB IDE：基于VSCode开源代码进行二次开发，符合安全要求，研发通过WEB IDE连接堡垒机后的服务器进行代码开发，提升研发效率。 代码规范检查和自动修复插件：基于MISRA-C规范进行代码自动扫描，接入gpt-4o模型，进行AI自动修复，提升研发手动修复效率。 阶段一（开发者平台）担任开发者平台研发部服务端开发经理，团队成员：15+。主要负责以下工作： 团队管理：负责服务端团队管理，工作分配和任务管理 架构设计：采用微服务架构设计思想，提升项目开发效率和可维护性 技术规范制定：制定代码规范、开发流程标准、技术选型原则，确保团队技术栈的统一性和代码质量 性能优化：监控系统性能指标，制定性能优化策略，确保系统稳定性和可扩展性 项目规划：参与产品需求评审，制定开发计划和里程碑，合理分配开发资源 风险管控：识别项目风险点，制定应对措施，确保项目按时交付 质量保障：建立完善的测试流程，推进自动化测试，确保交付质量 人才培养：制定团队成员成长计划，提供技术指导和职业发展建议 招聘面试：参与技术人员招聘，评估候选人技术能力和团队匹配度 绩效评估：建立客观的绩效评价体系，定期进行团队成员绩效反馈 杭州基本起源信息科技有限公司（2019.07-2020.03）线上购物app。主要负责以下工作： 团队管理：负责服务端团队管理，合理分配工作 架构设计：采用微服务架构设计思想，进行服务端架构设计 接口开发：负责系统核心业务接口开发 基础运维：负责公司项目的运维工作，搭建CI&#x2F;CD系统，所有项目docker化部署，自动运维 万汇互联（深圳）科技有限公司（2017.05-2019.05）贵金属外汇交易平台。主要负责以下工作： 团队管理：负责服务端团队管理，合理分配工作 架构设计：使用RPC技术实现微服务架构，Consul管理微服务 接口开发：负责平台大部分接口开发 基础建设：封装日志系统及RPC相关方法供全项目使用 容器化：负责所有Node.js项目Docker化 流程规范：按照dev→beta→production严格开发流程 技术分享：部门内多次技术分享 部署自动化：引入docker-compose技术，实现项目一键部署 深圳点猫科技有限公司（2015.09-2017.05）基于图形化的少儿编程在线教育平台。主要负责以下工作： 团队管理：负责服务端团队管理，合理分配工作 需求评审：参与产品功能评审，整理项目需求 架构设计：负责后端项目架构设计，数据库表结构设计 技术推广：项目内推广使用 TypeScript 文档管理：监督团队使用 API Doc 或 Swagger 编写接口文档 运维保障：负责维护服务正常访问和数据库连接 容器化部署：负责项目 Docker 化，编写 Dockerfile 和 docker-compose.yml 在校经历 担任计算机学院创新实验室负责人 负责学校多个学院的官网制作和维护 自我评价 热爱计算机行业，热爱编程，追求代码质量，不畏挑战。 求知欲强、为新技术和自我提升而自豪。 责任心强，高效且有质量的完成工作。"},{"title":"书单","date":"2025-11-03T09:23:29.109Z","updated":"2021-09-27T07:27:40.000Z","comments":false,"path":"books/index.html","permalink":"https://www.silenceboy.com/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2025-11-03T09:23:29.109Z","updated":"2021-09-27T07:27:40.000Z","comments":false,"path":"categories/index.html","permalink":"https://www.silenceboy.com/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2025-11-03T09:23:29.109Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"links/index.html","permalink":"https://www.silenceboy.com/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2025-11-03T09:23:29.089Z","updated":"2021-09-27T07:27:40.000Z","comments":false,"path":"repository/index.html","permalink":"https://www.silenceboy.com/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2025-11-03T09:23:29.109Z","updated":"2021-09-27T07:27:40.000Z","comments":false,"path":"tags/index.html","permalink":"https://www.silenceboy.com/tags/index.html","excerpt":"","text":""},{"title":"爱","date":"2025-11-03T09:23:29.089Z","updated":"2021-09-27T07:27:40.000Z","comments":false,"path":"love/index.html","permalink":"https://www.silenceboy.com/love/index.html","excerpt":"","text":""}],"posts":[{"title":"OpenClaw系统架构深度技术分析报告","slug":"OpenClaw系统架构深度技术分析报告","date":"2026-02-03T05:57:26.000Z","updated":"2026-02-03T06:06:34.063Z","comments":true,"path":"2026/02/03/OpenClaw系统架构深度技术分析报告/","permalink":"https://www.silenceboy.com/2026/02/03/OpenClaw%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E6%B7%B1%E5%BA%A6%E6%8A%80%E6%9C%AF%E5%88%86%E6%9E%90%E6%8A%A5%E5%91%8A/","excerpt":"","text":"本报告以系统架构师的视角，深入分析 OpenClaw 的技术架构设计 整体架构设计系统分层架构图graph TB subgraph \"客户端层 (Client Layer)\" Web[Web UI] Mobile[移动应用] CLI[CLI 工具] Desktop[桌面应用] end subgraph \"接入层 (Gateway Layer)\" GW[Gateway 网关服务器] WS[WebSocket 服务] HTTP[HTTP/REST API] Hooks[Webhooks 端点] end subgraph \"渠道抽象层 (Channel Layer)\" CM[Channel Manager] Telegram[Telegram 插件] Discord[Discord 插件] Slack[Slack 插件] WhatsApp[WhatsApp 插件] Signal[Signal 插件] IMessage[iMessage 插件] Extensions[扩展渠道插件...] end subgraph \"路由与控制层 (Routing & Control)\" Router[消息路由器] Allowlist[Allowlist 过滤] MentionGate[Mention 网关] CommandGate[Command 网关] SessionMgr[会话管理器] end subgraph \"代理层 (Agent Layer)\" AgentRunner[Agent Runner] PiCore[Pi Agent Core] ToolSystem[工具系统] ModelRegistry[模型注册表] Failover[容错回退机制] end subgraph \"媒体处理层 (Media Layer)\" MediaPipeline[媒体处理管道] ImageProc[图像处理] AudioProc[音频转录] VideoProc[视频理解] PDFProc[PDF 提取] MediaCache[媒体缓存] end subgraph \"基础设施层 (Infrastructure)\" DI[依赖注入容器] Config[配置管理] Auth[认证系统] Storage[持久化存储] Daemon[系统服务管理] end subgraph \"外部服务 (External Services)\" LLM[大语言模型 API] Vision[视觉模型 API] TTS[语音合成 API] Browser[浏览器控制] end %% 连接关系 Web --> GW Mobile --> GW CLI --> GW Desktop --> GW GW --> WS GW --> HTTP GW --> Hooks WS --> CM HTTP --> CM Hooks --> CM CM --> Telegram CM --> Discord CM --> Slack CM --> WhatsApp CM --> Signal CM --> IMessage CM --> Extensions Telegram --> Router Discord --> Router Slack --> Router WhatsApp --> Router Router --> Allowlist Allowlist --> MentionGate MentionGate --> CommandGate CommandGate --> SessionMgr SessionMgr --> AgentRunner AgentRunner --> PiCore PiCore --> ToolSystem PiCore --> ModelRegistry PiCore --> Failover AgentRunner --> MediaPipeline MediaPipeline --> ImageProc MediaPipeline --> AudioProc MediaPipeline --> VideoProc MediaPipeline --> PDFProc MediaPipeline --> MediaCache AgentRunner --> DI CM --> DI Router --> DI DI --> Config DI --> Auth DI --> Storage DI --> Daemon PiCore --> LLM MediaPipeline --> Vision ToolSystem --> Browser ToolSystem --> TTS style GW fill:#ff6b6b style CM fill:#4ecdc4 style Router fill:#45b7d1 style AgentRunner fill:#96ceb4 style MediaPipeline fill:#ffeaa7 style DI fill:#dfe6e9 消息流转完整链路图sequenceDiagram participant User as 用户 participant Channel as 渠道插件 participant Router as 路由器 participant Session as 会话管理 participant Agent as Agent Runner participant Pi as Pi Core participant Model as 模型 API participant Media as 媒体管道 participant Tool as 工具系统 User->>Channel: 1. 发送消息/附件 Channel->>Channel: 2. 归一化消息格式 Channel->>Router: 3. 消息入队 Router->>Router: 4. Allowlist 检查 alt Allowlist 未通过 Router-->>Channel: 丢弃消息 end Router->>Router: 5. Command-Gating 检查 alt 未授权控制命令 Router-->>Channel: 丢弃消息 end Router->>Router: 6. Mention-Gating 检查 alt 群组需要 @mention 但未 mention Router-->>Channel: 跳过处理 end Router->>Router: 7. 消息去重检查 alt 重复消息 Router-->>Channel: 跳过处理 end Router->>Session: 8. 路由到 Agent + Session Session->>Session: 9. 初始化/恢复会话 Session->>Agent: 10. 提交消息到 Agent alt 包含媒体附件 Agent->>Media: 11a. 媒体理解处理 Media->>Media: 下载/缓存媒体 Media->>Media: 图像描述/音频转录/视频理解 Media-->>Agent: 返回理解结果 end Agent->>Pi: 11b. 创建 Agent Session Pi->>Model: 12. 调用模型 API alt 模型调用失败 Model-->>Pi: 错误响应 Pi->>Pi: 13a. Thinking Level 回退 alt Thinking 回退成功 Pi->>Model: 重试低级别 Thinking else Auth Profile 切换 Pi->>Pi: 13b. 切换认证 Profile Pi->>Model: 重试不同 Profile else Model 回退 Pi->>Pi: 13c. 回退到候选模型 Pi->>Model: 重试候选模型 end end Model-->>Pi: 14. 流式响应 loop 流式处理 Pi-->>Agent: 15a. message_update 事件 Agent-->>Channel: 部分回复 (onPartialReply) Channel-->>User: 实时显示打字状态 end alt 工具调用 Pi->>Tool: 15b. tool_execution_start Tool->>Tool: 执行工具 (browser/canvas/message/...) Tool-->>Pi: 工具结果 Pi-->>Agent: tool_execution_end Agent-->>Channel: 工具结果消息 (onToolResult) Channel-->>User: 显示工具执行结果 Pi->>Model: 继续模型调用 end Pi-->>Agent: 16. message_end 事件 Agent->>Agent: 17. 提取最终文本 + reasoning Agent->>Agent: 18. 消息去重检查 Agent->>Session: 19. 更新会话状态 Agent-->>Channel: 20. 最终回复 (onBlockReply) alt 跨渠道路由 Channel->>Router: 21a. 路由到原始渠道 Router->>Channel: 转发到目标渠道 end Channel-->>User: 22. 发送最终消息 依赖注入与模块解耦架构graph TB subgraph \"CLI 层 (Command Layer)\" CMD[命令入口] CliDeps[CliDeps 类型] end subgraph \"业务逻辑层 (Business Logic)\" Action[MessageAction] OutboundDeps[OutboundSendDeps 类型] end subgraph \"基础设施层 (Infrastructure)\" WhatsApp[sendMessageWhatsApp] Telegram[sendMessageTelegram] Discord[sendMessageDiscord] Slack[sendMessageSlack] Signal[sendMessageSignal] IMessage[sendMessageIMessage] end subgraph \"依赖容器 (Dependency Container)\" Factory[createDefaultDeps 工厂] Converter[createOutboundSendDeps 转换器] end subgraph \"测试层 (Test Layer)\" TestDeps[Mock Deps] TestFactory[makeDeps 工厂] end CMD -->|调用| Factory Factory -->|创建| CliDeps CliDeps -->|引用| WhatsApp CliDeps -->|引用| Telegram CliDeps -->|引用| Discord CliDeps -->|引用| Slack CliDeps -->|引用| Signal CliDeps -->|引用| IMessage CMD -->|注入| Action Action -->|依赖| OutboundDeps CliDeps -->|转换| Converter Converter -->|创建| OutboundDeps TestFactory -->|创建| TestDeps TestDeps -->|Mock| WhatsApp TestDeps -->|Mock| Telegram TestDeps -.替换.-> CliDeps style Factory fill:#96ceb4 style Converter fill:#96ceb4 style TestFactory fill:#ffeaa7 style CliDeps fill:#a29bfe style OutboundDeps fill:#a29bfe 核心子系统深度分析Gateway 网关系统架构特性双协议支持： WebSocket：用于实时双向通信、事件推送、控制台连接 握手流程：connect.challenge → connect → hello-ok 消息格式：JSON-RPC 风格 (req&#x2F;res&#x2F;event) 连接管理：GatewayWsClient 维护连接状态 HTTP&#x2F;REST：用于 Webhooks、兼容性端点、工具调用 Control UI: / Hooks: /hooks/* (wake, agent, mappings) OpenAI 兼容: /v1/chat/completions Tools Invoke: /tools/invoke 请求路由机制： graph LR Request[请求] --> Parse[解析消息] Parse --> Validate[validateRequestFrame] Validate --> Handle[handleGatewayRequest] Handle --> Auth[authorizeGatewayMethod] Auth --> Handler[coreGatewayHandlers] Handler --> Respond[respond 回调] Auth -.检查.-> Roles[Role: operator/node] Auth -.检查.-> Scopes[Scope: admin/read/write] style Auth fill:#ff6b6b style Handler fill:#4ecdc4 核心特性： 方法注册：coreGatewayHandlers 合并核心方法与插件方法 权限模型：基于角色 (operator&#x2F;node) 和 scope 的细粒度授权 插件扩展：loadGatewayPlugins() 支持动态加载插件方法 会话管理策略两层持久化架构： 123451. Session Store (sessions.json) └─ SessionEntry: 会话元数据 + 配置覆盖 2. Transcript (*.jsonl) └─ 树形消息历史 (id + parentId) 会话键格式： 场景 Session Key 格式 示例 DM (main) agent:&#123;agentId&#125;:main agent:assistant:main DM (per-peer) agent:&#123;agentId&#125;:dm:&#123;peerId&#125; agent:assistant:dm:123456 DM (per-channel-peer) agent:&#123;agentId&#125;:&#123;channel&#125;:dm:&#123;peerId&#125; agent:assistant:telegram:dm:123456 Group agent:&#123;agentId&#125;:&#123;channel&#125;:group:&#123;groupId&#125; agent:assistant:telegram:group:abc 会话元数据管理： Token 统计：inputTokens, outputTokens, totalTokens, contextTokens 模型覆盖：providerOverride, modelOverride, authProfileOverride 压缩控制：compactionCount, memoryFlushAt 交付上下文：deliveryContext, lastChannel, lastTo 配置热重载机制1234567// 重载策略: off | restart | hot | hybrid (默认)const hotReloadRules = &#123; &quot;hooks.*&quot;: &quot;reload_hooks&quot;, // 热重载 hooks &quot;cron.*&quot;: &quot;restart_cron&quot;, // 重启 cron &quot;gateway.*&quot;: &quot;restart_gateway&quot;, // 重启整个 Gateway &quot;channels.*&quot;: &quot;reload_channels&quot;, // 热重载渠道配置&#125;; 防抖机制：默认 300ms，避免频繁重载 系统服务集成 平台 服务管理器 服务标签 安装路径 macOS launchd bot.molt.gateway ~/Library/LaunchAgents/ Linux systemd openclaw-gateway.service ~/.config/systemd/user/ 服务抽象层： src/daemon/service.ts 提供统一接口 支持：install, uninstall, stop, restart, isLoaded Channel 渠道抽象层插件化架构设计核心接口 - ChannelPlugin： 123456789101112131415type ChannelPlugin&lt;ResolvedAccount = any&gt; = &#123; id: ChannelId; // 渠道标识 meta: ChannelMeta; // 元数据（标签、文档） capabilities: ChannelCapabilities; // 能力声明 config: ChannelConfigAdapter; // 配置管理 outbound: ChannelOutboundAdapter; // 出站消息 status: ChannelStatusAdapter; // 状态检查 gateway: ChannelGatewayAdapter; // 生命周期管理 security: ChannelSecurityAdapter; // 安全策略 groups: ChannelGroupAdapter; // 群组策略 mentions: ChannelMentionAdapter; // 提及处理 threading: ChannelThreadingAdapter; // 线程处理 messaging: ChannelMessagingAdapter; // 消息目标解析 // ... 更多适配器&#125;; 适配器模式优势： 统一接口，不同实现 可选适配器，按需实现 类型安全，泛型约束 消息归一化流程graph LR Raw[原始消息] --> Clean[清理空白/大小写] Clean --> Prefix[识别前缀] Prefix --> Parse[解析格式] Parse --> Convert[转换为标准格式] Convert --> Output[统一输出] Prefix -.识别.-> TgPrefix[\"telegram:, tg:, t.me/\"] Prefix -.识别.-> DcPrefix[\"discord:, @username#1234\"] Prefix -.识别.-> SlPrefix[\"slack:, #channel, \"] Parse -.URL.-> ID1[提取 ID] Parse -.Username.-> ID2[解析为 ID] Parse -.Display Name.-> ID3[查找 ID] style Parse fill:#4ecdc4 style Convert fill:#96ceb4 归一化示例： 渠道 原始输入 归一化输出 Telegram t.me/username telegram:username Discord @user#1234 discord:user:1234 Slack &lt;@U123ABC&gt; slack:U123ABC WhatsApp +1234567890 whatsapp:1234567890 权限控制三层机制graph TB Message[消息到达] --> Layer1[第一层: Allowlist] Layer1 -->|检查| GroupAllow{群组 Allowlist} GroupAllow -->|未通过| Drop1[丢弃消息] GroupAllow -->|通过| SenderAllow{发送者 Allowlist} SenderAllow -->|未通过| Drop2[丢弃消息] SenderAllow -->|通过| Layer2[第二层: Command-Gating] Layer2 -->|检查| HasCmd{有控制命令?} HasCmd -->|否| Layer3[第三层: Mention-Gating] HasCmd -->|是| CmdAuth{命令授权?} CmdAuth -->|否| Drop3[丢弃消息] CmdAuth -->|是| Layer3 Layer3 -->|检查| IsGroup{是群组?} IsGroup -->|否| Process[处理消息] IsGroup -->|是| ReqMention{需要 @mention?} ReqMention -->|否| Process ReqMention -->|是| WasMentioned{被 @mention?} WasMentioned -->|否| CanBypass{可绕过?} WasMentioned -->|是| Process CanBypass -->|否| Skip[跳过处理] CanBypass -->|是| Process style Layer1 fill:#ff6b6b style Layer2 fill:#feca57 style Layer3 fill:#48dbfb style Process fill:#1dd1a1 Mention-Gating Bypass 条件： 12345678shouldBypass = isGroup &amp;&amp; requireMention &amp;&amp; !wasMentioned &amp;&amp; !hasAnyMention &amp;&amp; allowTextCommands &amp;&amp; commandAuthorized &amp;&amp; hasControlCommand; 插件发现与注册发现流程： 123456789101112131. 扫描 extensions/ 目录 └─ 查找 openclaw.plugin.json 2. 加载配置路径插件 └─ 从 config.plugins.paths 读取 3. 优先级去重 ├─ Workspace 插件 (优先级最高) ├─ Extensions 插件 └─ External 插件 (优先级最低) 4. 排序 └─ 按 meta.order 和 meta.label 排序 插件清单格式： 123456789&#123; &quot;id&quot;: &quot;telegram&quot;, &quot;type&quot;: &quot;channel&quot;, &quot;meta&quot;: &#123; &quot;label&quot;: &quot;Telegram&quot;, &quot;order&quot;: 10, &quot;docsPath&quot;: &quot;/channels/telegram&quot; &#125;&#125; Agent 代理引擎Pi Agent Core 集成架构graph TB subgraph \"OpenClaw 层\" Runner[Agent Runner] SessionInit[Session 初始化] ModelResolve[模型解析] ToolCreate[工具创建] end subgraph \"Pi Core 层\" PiSession[Agent Session] SessionMgr[SessionManager] EventStream[事件流] ToolExec[工具执行] end subgraph \"模型层\" ModelAPI[模型 API] AuthStorage[认证存储] ModelRegistry[模型注册表] end Runner --> SessionInit Runner --> ModelResolve Runner --> ToolCreate SessionInit --> SessionMgr ModelResolve --> AuthStorage ModelResolve --> ModelRegistry Runner -->|createAgentSession| PiSession PiSession --> EventStream PiSession --> ToolExec PiSession --> ModelAPI EventStream -->|订阅| Runner ToolExec -->|execute| ToolCreate ModelAPI -->|失败| Runner Runner -->|回退| ModelResolve style PiSession fill:#96ceb4 style SessionMgr fill:#a29bfe style ModelAPI fill:#ffeaa7 集成流程代码： 123456789101112131415161718192021222324252627// 1. 模型解析const &#123; model, authStorage, modelRegistry &#125; = resolveModel( provider, modelId, agentDir, config);// 2. 会话管理器初始化sessionManager = SessionManager.open(sessionFile);// 3. 创建 Agent Session (Pi Core)const &#123; session &#125; = await createAgentSession(&#123; cwd: workspace, agentDir, authStorage, modelRegistry, model, thinkingLevel: mapThinkingLevel(thinkLevel), tools: [...builtInTools, ...customTools], sessionManager,&#125;);// 4. 订阅事件流const subscription = subscribeEmbeddedPiSession(&#123; session, onPartialReply: (payload) =&gt; &#123; /* 流式回复 */ &#125;, onBlockReply: (payload) =&gt; &#123; /* 块回复 */ &#125;, onToolResult: (payload) =&gt; &#123; /* 工具结果 */ &#125;,&#125;); 多级容错机制graph TD Start[开始执行] --> Attempt[尝试调用模型] Attempt --> Check{调用成功?} Check -->|是| Success[返回结果] Check -->|否| ErrorType{错误类型?} ErrorType -->|Thinking 不支持| ThinkFallback[Thinking Level 回退] ErrorType -->|认证失败| AuthFallback[Auth Profile 切换] ErrorType -->|限流/配额| ModelFallback[Model 回退] ErrorType -->|超时/其他| ModelFallback ThinkFallback --> ThinkAvail{有可用级别?} ThinkAvail -->|是| Attempt ThinkAvail -->|否| AuthFallback AuthFallback --> AuthAvail{有可用 Profile?} AuthAvail -->|是| Attempt AuthAvail -->|否| ModelFallback ModelFallback --> ModelAvail{有候选模型?} ModelAvail -->|是| Attempt ModelAvail -->|否| Fail[抛出错误] style Success fill:#1dd1a1 style Fail fill:#ff6b6b style ThinkFallback fill:#feca57 style AuthFallback fill:#48dbfb style ModelFallback fill:#a29bfe Thinking Level 回退序列： 1xhigh → high → medium → low → minimal → off Model 回退配置： 123456789101112&#123; agents: &#123; defaults: &#123; model: &quot;openai/gpt-4o&quot;, fallbacks: [ &quot;anthropic/claude-3-5-sonnet-20241022&quot;, &quot;google/gemini-2.0-flash-exp&quot;, &quot;openai/gpt-4o-mini&quot; ] &#125; &#125;&#125; 错误分类机制： 错误类型 触发条件 回退策略 rate_limit 429 状态码 切换 Auth Profile 或回退模型 quota 配额耗尽 回退模型 auth 401&#x2F;403 状态码 切换 Auth Profile timeout 请求超时 回退模型 context_overflow 上下文溢出 压缩会话或回退模型 Thinking 模式技术实现Thinking Level 映射： Level 用途 成本 适用场景 off 无推理 最低 简单对话 minimal 最小推理 低 日常任务 low 低级推理 中低 编码任务 medium 中级推理 中等 复杂分析 high 高级推理 高 深度思考 xhigh 极高推理 最高 研究级任务 Reasoning Mode 处理： 123456789101112const reasoningMode = params.reasoningMode ?? &quot;off&quot;;if (reasoningMode === &quot;stream&quot;) &#123; // 流式输出 reasoning ctx.emitReasoningStream( extractThinkingFromTaggedStream(deltaBuffer) );&#125; else if (reasoningMode === &quot;on&quot;) &#123; // 在消息结束时发送完整 reasoning const reasoning = extractAssistantThinking(message); void onBlockReply(&#123; text: formatReasoningMessage(reasoning) &#125;);&#125; 提取与格式化： 12345// 从 &lt;thinking&gt;...&lt;/thinking&gt; 标签中提取extractAssistantThinking(msg: AssistantMessage): string// 格式化为用户可读格式formatReasoningMessage(rawThinking: string): string Canvas 实时渲染能力Canvas 工具架构： graph LR Agent[Agent] -->|调用| CanvasTool[canvas 工具] CanvasTool -->|node.invoke| Gateway[Gateway] Gateway -->|转发| Node[目标 Node] Node -->|执行| Action[Canvas 动作] Action -->|结果| Node Node -->|返回| Gateway Gateway -->|返回| CanvasTool CanvasTool -->|结果| Agent Action -.支持.-> Present[显示 Canvas] Action -.支持.-> Navigate[导航 URL] Action -.支持.-> Eval[执行 JS] Action -.支持.-> Snapshot[截图] Action -.支持.-> A2UI[A2UI 推送] style CanvasTool fill:#96ceb4 style Action fill:#ffeaa7 Canvas 操作类型： 操作 参数 返回 用途 present position, size - 显示 Canvas hide - - 隐藏 Canvas navigate url - 导航到 URL eval code result 执行 JavaScript snapshot format, quality base64 截图 a2ui_push jsonl - 推送 UI 定义 a2ui_reset - - 重置 A2UI A2UI 集成： JSONL 格式的声明式 UI 支持表单、按钮、列表等组件 实时响应用户交互 工具系统与插件架构工具分类体系： graph TB Tools[所有工具] Tools --> BuiltIn[内置工具] Tools --> OpenClaw[OpenClaw 工具] Tools --> Plugin[插件工具] BuiltIn --> FS[文件系统] BuiltIn --> Runtime[运行时] FS --> Read[read] FS --> Write[write] FS --> Edit[edit] FS --> Apply[apply_patch] Runtime --> Exec[exec] Runtime --> Process[process] OpenClaw --> Browser[browser] OpenClaw --> Canvas[canvas] OpenClaw --> Message[message] OpenClaw --> Sessions[sessions_*] OpenClaw --> Memory[memory_*] OpenClaw --> Web[web_*] OpenClaw --> Image[image] OpenClaw --> Cron[cron] OpenClaw --> Gateway[gateway] OpenClaw --> Nodes[nodes] Plugin --> Custom[自定义工具...] style BuiltIn fill:#ff6b6b style OpenClaw fill:#4ecdc4 style Plugin fill:#96ceb4 工具策略系统： 12345678910111213141516171819202122232425262728// 工具 Profile 定义type ToolProfileId = &quot;minimal&quot; | &quot;coding&quot; | &quot;messaging&quot; | &quot;full&quot;;const TOOL_PROFILES: Record&lt;ToolProfileId, ToolProfilePolicy&gt; = &#123; minimal: &#123; allow: [&quot;session_status&quot;] &#125;, coding: &#123; allow: [ &quot;group:fs&quot;, // 文件系统组 &quot;group:runtime&quot;, // 运行时组 &quot;group:sessions&quot;, // 会话组 &quot;browser&quot;, &quot;canvas&quot;, &quot;web_search&quot;, &quot;web_fetch&quot; ] &#125;, messaging: &#123; allow: [ &quot;group:messaging&quot;, // 消息组 &quot;sessions_list&quot;, &quot;web_search&quot;, &quot;web_fetch&quot; ] &#125;, full: &#123;&#125; // 允许所有工具&#125;; 工具策略层级： 123451. 全局策略: tools.global.allow/deny2. Provider 策略: tools.providers[provider].allow/deny3. Agent 策略: agents[agentId].tools.allow/deny4. Group 策略: channels[channel].groups[groupId].tools.allow/deny5. Subagent 策略: 继承父会话策略 插件工具解析： 12345678910111213const pluginTools = resolvePluginTools(&#123; context: &#123; config, workspaceDir, agentDir, agentId, sessionKey, messageChannel, sandboxed, &#125;, existingToolNames: new Set(tools.map(t =&gt; t.name)), toolAllowlist: options?.pluginToolAllowlist,&#125;); Media 媒体理解管道多模态处理架构graph TB Entry[applyMediaUnderstanding] --> Select[selectAttachments] Select --> Image{图像?} Select --> Audio{音频?} Select --> Video{视频?} Select --> PDF{PDF?} Image --> ImgCache[MediaAttachmentCache] Audio --> AudCache[MediaAttachmentCache] Video --> VidCache[MediaAttachmentCache] PDF --> PDFCache[MediaAttachmentCache] ImgCache --> Vision[Vision API] AudCache --> Transcribe[转录 API] VidCache --> VideoAPI[视频理解 API] PDFCache --> Extract[文本提取/渲染] Vision --> ImgDesc[图像描述] Transcribe --> AudText[音频文本] VideoAPI --> VidDesc[视频描述] Extract --> PDFText[PDF 文本/图像] ImgDesc --> Format[formatMediaUnderstandingBody] AudText --> Format VidDesc --> Format PDFText --> Format Format --> Output[输出到消息上下文] style Vision fill:#ffeaa7 style Transcribe fill:#ffeaa7 style VideoAPI fill:#ffeaa7 style Extract fill:#ffeaa7 处理能力顺序： 12const CAPABILITY_ORDER: MediaUnderstandingCapability[] = [&quot;image&quot;, &quot;audio&quot;, &quot;video&quot;]; 并发控制： 默认并发数：2 使用 runWithConcurrency() 限制并发 Worker 池模式，任务队列 提供商适配策略提供商注册表： 提供商 图像 音频 视频 默认模型 OpenAI ✅ ✅ ❌ gpt-4o-mini Anthropic ✅ ❌ ❌ claude-3-5-sonnet Google ✅ ✅ ✅ gemini-2.0-flash-exp Deepgram ❌ ✅ ❌ nova-3 Groq ❌ ✅ ❌ whisper-large-v3-turbo Minimax ✅ ❌ ❌ - 统一接口抽象： 12345678910111213type MediaUnderstandingProvider = &#123; id: string; capabilities: MediaUnderstandingCapability[]; describeImage?: (req: ImageDescriptionRequest) =&gt; Promise&lt;ImageDescriptionResult&gt;; transcribeAudio?: (req: AudioTranscriptionRequest) =&gt; Promise&lt;AudioTranscriptionResult&gt;; describeVideo?: (req: VideoDescriptionRequest) =&gt; Promise&lt;VideoDescriptionResult&gt;;&#125;; SSRF 攻击防护体系多层防护架构： graph TB Request[媒体请求] --> DNS[DNS 解析阶段] DNS --> CheckPrivate{私有 IP?} CheckPrivate -->|是| BlockList[检查黑名单] CheckPrivate -->|否| Resolve[解析域名] BlockList --> IsBlocked{在黑名单?} IsBlocked -->|是| Drop1[拒绝请求] IsBlocked -->|否| Resolve Resolve --> Pin[固定 IP] Pin --> Fetch[HTTP 请求阶段] Fetch --> Policy{SSRF 策略} Policy -->|禁止私有网络| CheckIP{目标是私有 IP?} Policy -->|允许私有网络| Execute[执行请求] CheckIP -->|是| Drop2[拒绝请求] CheckIP -->|否| Execute Execute --> Limit[大小限制] Limit --> Success[返回结果] style DNS fill:#ff6b6b style Fetch fill:#feca57 style Success fill:#1dd1a1 私有 IP 检测： 1234567891011const PRIVATE_IP_RANGES = [ &quot;10.0.0.0/8&quot;, // 私有 A 类 &quot;172.16.0.0/12&quot;, // 私有 B 类 &quot;192.168.0.0/16&quot;, // 私有 C 类 &quot;127.0.0.0/8&quot;, // 本地回环 &quot;169.254.0.0/16&quot;, // 链路本地 &quot;100.64.0.0/10&quot;, // 共享地址空间 &quot;fc00::/7&quot;, // IPv6 唯一本地 &quot;fe80::/10&quot;, // IPv6 链路本地 &quot;::1/128&quot;, // IPv6 回环]; 主机名黑名单： 12345678const BLACKLIST_HOSTNAMES = [ &quot;localhost&quot;, &quot;metadata.google.internal&quot;, &quot;169.254.169.254&quot;, // AWS/Azure 元数据 &quot;.local&quot;, &quot;.internal&quot;, &quot;.localhost&quot;]; 防护点： DNS 解析前检查主机名 DNS 解析后检查 IP 固定 IP 防止 DNS 重绑定 HTTP 请求前再次检查 响应大小限制 媒体转换与压缩图像处理管道： graph LR Input[原始图像] --> EXIF[EXIF 方向归一化] EXIF --> Format{格式?} Format -->|HEIC| Convert[转换为 JPEG] Format -->|其他| Resize[尺寸调整] Convert --> Resize Resize --> Compress[压缩] Compress --> Check{大小检查} Check -->|超限| Optimize[网格搜索优化] Check -->|合格| Output[输出] Optimize --> Reduce[降低尺寸/质量] Reduce --> Check style EXIF fill:#4ecdc4 style Compress fill:#ffeaa7 style Optimize fill:#ff6b6b 压缩策略： 格式 质量范围 最大尺寸 优化策略 JPEG 70-90 2048px 质量降级 PNG 压缩级别 6-9 2048px 网格搜索 WebP 70-90 2048px 质量降级 PNG 网格搜索优化： 123456789// 尺寸候选: 2048, 1600, 1200, 1000, 800// 压缩级别: 6, 7, 8, 9// 目标: 找到第一个满足大小限制的组合for (const maxDim of [2048, 1600, 1200, 1000, 800]) &#123; for (const compressionLevel of [6, 7, 8, 9]) &#123; const result = await resizeToPng(&#123; maxDim, compressionLevel &#125;); if (result.size &lt;= maxBytes) return result; &#125;&#125; PDF 处理策略： 12345678910111213// 1. 文本优先const textContent = await extractPdfText(buffer);if (textContent.length &gt;= MIN_TEXT_LENGTH) &#123; return &#123; text: textContent &#125;;&#125;// 2. 图像回退const images = await renderPdfPages(buffer, &#123; maxPages: 4, maxPixels: 4_000_000, scale: calculateScale(pdfWidth, pdfHeight, maxPixels),&#125;);return &#123; images &#125;; 缓存策略三级缓存架构： graph TB Request[媒体请求] --> L1[L1: 内存缓存] L1 --> Hit1{命中?} Hit1 -->|是| Return1[返回 Buffer] Hit1 -->|否| L2[L2: 磁盘缓存] L2 --> Hit2{命中?} Hit2 -->|是| Return2[返回文件路径] Hit2 -->|否| L3[L3: 临时文件] L3 --> Download[下载媒体] Download --> Store[存储到临时文件] Store --> Cleanup[注册清理] Cleanup --> Return3[返回文件路径] Return1 --> Use[使用] Return2 --> Use Return3 --> Use Use --> Done{处理完成?} Done -->|是| Delete[删除临时文件] style L1 fill:#1dd1a1 style L2 fill:#feca57 style L3 fill:#ff6b6b MediaAttachmentCache 结构： 12345678type AttachmentCacheEntry = &#123; buffer?: Buffer; // 内存 Buffer resolvedPath?: string; // 已解析的本地路径 tempPath?: string; // 临时下载路径 statSize?: number; // 文件大小&#125;;const cache = new Map&lt;number, AttachmentCacheEntry&gt;(); 媒体服务器缓存： 存储目录：~/.openclaw/media/ TTL：默认 2 分钟 单次使用：响应后延迟 50ms 删除 大小限制：5MB 自动清理：定期清理过期文件 清理策略： 12345678// 1. 响应后清理setTimeout(() =&gt; fs.unlink(mediaPath), 50);// 2. 定期清理setInterval(() =&gt; cleanOldMedia(), CLEANUP_INTERVAL);// 3. 进程退出清理process.on(&quot;exit&quot;, () =&gt; cleanup()); 路由与控制层路由决策树graph TB Message[消息到达] --> Parse[解析上下文] Parse --> Filter[过滤 Bindings] Filter --> Match1{Peer 匹配?} Match1 -->|是| Route1[路由到 Peer Agent] Match1 -->|否| Match2{Parent Peer 匹配?} Match2 -->|是| Route2[路由到 Parent Agent] Match2 -->|否| Match3{Guild 匹配?} Match3 -->|是| Route3[路由到 Guild Agent] Match3 -->|否| Match4{Team 匹配?} Match4 -->|是| Route4[路由到 Team Agent] Match4 -->|否| Match5{Account 匹配?} Match5 -->|是| Route5[路由到 Account Agent] Match5 -->|否| Match6{Channel 匹配?} Match6 -->|是| Route6[路由到 Channel Agent] Match6 -->|否| Default[路由到默认 Agent] Route1 --> Init[初始化会话] Route2 --> Init Route3 --> Init Route4 --> Init Route5 --> Init Route6 --> Init Default --> Init style Match1 fill:#ff6b6b style Match2 fill:#feca57 style Match3 fill:#48dbfb style Match4 fill:#a29bfe style Match5 fill:#4ecdc4 style Match6 fill:#96ceb4 style Default fill:#dfe6e9 路由优先级表： 优先级 匹配类型 匹配字段 使用场景 示例 1 Peer match.peer 特定对话绑定 &#123; kind: &quot;dm&quot;, id: &quot;123&quot; &#125; 2 Parent Peer match.peer (parent) Thread 继承 回复线程消息 3 Guild match.guildId Discord 服务器 guild-abc123 4 Team match.teamId MS Teams team-xyz789 5 Account match.accountId (非 *) 账户级别 default 6 Channel match.accountId &#x3D; * 通道级别 所有 Telegram 账户 7 Default 无匹配 默认 Agent main 消息去重机制去重键格式： 12const dedupeKey = `$&#123;provider&#125;|$&#123;accountId&#125;|$&#123;sessionKey&#125;|$&#123;peerId&#125;|$&#123;threadId&#125;|$&#123;messageId&#125;`; 去重缓存配置： 12345&#123; ttl: 20 * 60 * 1000, // 20 分钟 maxEntries: 5000, // 最大 5000 条 cleanupInterval: 60000 // 1 分钟清理一次&#125; 去重检查时机： 12345// 在 dispatchReplyFromConfig 入口if (shouldSkipDuplicateInbound(ctx)) &#123; recordProcessed(&quot;skipped&quot;, &#123; reason: &quot;duplicate&quot; &#125;); return; // 跳过处理&#125; 会话隔离策略隔离级别对比： 隔离级别 Session Key 格式 DM 隔离 群组隔离 跨渠道 main agent:&#123;id&#125;:main 所有共享 按群组 是 per-peer agent:&#123;id&#125;:dm:&#123;peerId&#125; 按发送者 按群组 否 per-channel-peer agent:&#123;id&#125;:&#123;ch&#125;:dm:&#123;peerId&#125; 按渠道+发送者 按渠道+群组 否 per-account-channel-peer agent:&#123;id&#125;:&#123;ch&#125;:&#123;acc&#125;:dm:&#123;peerId&#125; 按账户+渠道+发送者 按账户+渠道+群组 否 身份链接： 1234567891011&#123; session: &#123; identityLinks: &#123; canonical: [ &quot;telegram:123456&quot;, &quot;discord:user#1234&quot;, &quot;slack:U123ABC&quot; ] &#125; &#125;&#125; 当消息来自链接的身份时，会路由到同一个会话。 回复分发流程sequenceDiagram participant Agent as Agent Runner participant Dispatcher as ReplyDispatcher participant Router as ReplyRouter participant Channel as Channel Plugin participant User as 用户 Agent->>Dispatcher: onToolResult (工具结果) Dispatcher->>Dispatcher: 串行化 (sendChain) Dispatcher->>Router: 检查路由 alt 跨渠道路由 Router->>Channel: routeReply (原始渠道) else 当前渠道 Router->>Channel: dispatcher.send end Channel->>User: 发送工具结果 Agent->>Dispatcher: onBlockReply (块回复) Dispatcher->>Dispatcher: 应用人工延迟 Dispatcher->>Router: 检查路由 Router->>Channel: 发送块回复 Channel->>User: 发送块回复 Agent->>Dispatcher: onBlockReply (最终回复) Dispatcher->>Router: 检查路由 alt TTS 启用 Router->>Router: maybeApplyTtsToPayload end Router->>Channel: 发送最终回复 Channel->>User: 发送最终回复 Dispatcher->>Dispatcher: waitForIdle (等待分发完成) ReplyDispatcher 特性： 串行化分发：sendChain Promise 链确保顺序 人工延迟：block 回复之间插入延迟（模拟打字） 响应前缀：responsePrefix 模板化前缀 心跳条带：onHeartbeatStrip 处理长时间等待 跨渠道路由条件： 1234const shouldRouteToOriginating = isRoutableChannel(originatingChannel) &amp;&amp; originatingTo &amp;&amp; originatingChannel !== currentSurface; 当用户在 Telegram 发起对话，但当前回复渠道是 Slack 时，会路由回 Telegram。 设计亮点与技术特色架构设计亮点分层解耦架构 清晰的职责边界：Gateway、Channel、Agent、Media 各司其职 依赖注入模式：通过 CliDeps 和 OutboundSendDeps 实现模块解耦 适配器模式：ChannelPlugin 适配器统一不同渠道的接口 工厂模式：createDefaultDeps 和 createOutboundSendDeps 管理依赖创建 插件化扩展能力 渠道插件：extensions/ 目录支持动态加载新渠道 工具插件：Agent 工具系统支持插件工具 Gateway 插件：Gateway 方法可通过插件扩展 模型插件：模型注册表支持自定义模型 容错与高可用 三级回退机制：Thinking Level → Auth Profile → Model 错误分类：精确识别错误类型并采取相应策略 配置热重载：部分配置变更无需重启 健康检查：Gateway 健康端点、Channel 状态探测 安全防护体系 多层 SSRF 防护：DNS 固定 + 私有 IP 检测 + 主机名黑名单 权限控制三层机制：Allowlist → Command-Gating → Mention-Gating 会话隔离：支持多级隔离策略 路径遍历防护：openFileWithinRoot 和 ID 格式验证 性能优化 并发控制：媒体处理并发限制 流式处理：大文件流式读取、Agent 流式回复 三级缓存：内存 + 磁盘 + 临时文件 智能压缩：PNG 网格搜索优化 技术创新点消息流转编排 统一归一化：不同渠道消息统一为标准格式 智能路由：7 级优先级路由决策 去重机制：基于组合键的 TTL 缓存去重 跨渠道路由：支持消息在不同渠道间路由 Agent 执行引擎 Pi Core 深度集成：无缝集成 Pi Agent Core 事件驱动架构：基于事件订阅的流式处理 Block Reply 系统：支持中间回复和最终回复 消息去重：避免 messaging tool 重复发送 媒体理解管道 多模态统一抽象：图像&#x2F;音频&#x2F;视频&#x2F;PDF 统一处理 提供商适配：支持多种 Vision&#x2F;Audio API 智能压缩：自动优化媒体大小 SSRF 防护：多层次安全防护 工具系统设计 工具策略分级：全局 → Provider → Agent → Group → Subagent 工具组抽象：group:fs、group:runtime 等组概念 插件工具：支持外部插件提供工具 Canvas 集成：实时渲染和 A2UI 支持 可扩展性新增渠道12345678910111213141516171819// 1. 创建插件目录extensions/new-channel/ ├── openclaw.plugin.json ├── index.ts └── src/ ├── channel.ts └── runtime.ts// 2. 实现 ChannelPlugin 接口export const newChannelPlugin: ChannelPlugin = &#123; id: &quot;new-channel&quot;, capabilities: &#123; chatTypes: [&quot;dm&quot;, &quot;group&quot;] &#125;, config: &#123; /* 配置适配器 */ &#125;, outbound: &#123; /* 出站适配器 */ &#125;, // ... 其他适配器&#125;;// 3. 自动发现和注册// 无需修改核心代码 新增工具1234567891011121314// 在插件中定义工具export function createMyTool(): AnyAgentTool &#123; return &#123; name: &quot;my_tool&quot;, description: &quot;My custom tool&quot;, parameters: MyToolSchema, execute: async (toolCallId, params) =&gt; &#123; // 工具实现 return jsonResult(&#123; status: &quot;ok&quot; &#125;); &#125;, &#125;;&#125;// 工具会自动被 resolvePluginTools 发现 新增媒体提供商12345678910111213// 在 src/media-understanding/providers/ 添加export const myProvider: MediaUnderstandingProvider = &#123; id: &quot;my-provider&quot;, capabilities: [&quot;image&quot;, &quot;audio&quot;], describeImage: async (req) =&gt; &#123; // 实现图像描述 &#125;, transcribeAudio: async (req) =&gt; &#123; // 实现音频转录 &#125;,&#125;;// 在 buildMediaUnderstandingRegistry 中注册 可测试性依赖注入测试12345678910// 轻松替换依赖const makeDeps = (overrides: Partial&lt;CliDeps&gt; = &#123;&#125;): CliDeps =&gt; (&#123; sendMessageWhatsApp: vi.fn(), sendMessageTelegram: vi.fn(), ...overrides,&#125;);const deps = makeDeps(&#123; sendMessageWhatsApp: vi.fn().mockResolvedValue(&#123; success: true &#125;),&#125;); Mock 替换12345678// 在测试中 Mock 整个模块vi.mock(&quot;../cli/deps.js&quot;, async () =&gt; &#123; const actual = await vi.importActual(&quot;../cli/deps.js&quot;); return &#123; ...actual, createDefaultDeps: () =&gt; mockDeps, &#125;;&#125;); 单元测试覆盖 Gateway：请求路由、会话管理、权限检查 Channel：消息归一化、权限控制、路由决策 Agent：模型回退、工具执行、流式处理 Media：SSRF 防护、压缩优化、提供商适配 运维友好系统服务集成 macOS launchd：自动启动、日志管理 Linux systemd：用户服务、自动重启 Docker 支持：容器化部署 可观测性 健康检查：Gateway /health 端点 状态探测：Channel status --probe 日志系统：统一日志格式、macOS 统一日志集成 事件广播：Gateway 事件实时推送 配置管理 热重载：部分配置无需重启 配置迁移：自动迁移旧版本配置 配置验证：加载时验证配置正确性 环境变量：支持环境变量覆盖 总结核心优势 架构清晰：分层设计，职责明确，易于理解和维护 高度解耦：依赖注入和适配器模式实现模块解耦 强扩展性：插件化架构，支持渠道&#x2F;工具&#x2F;模型扩展 容错健壮：多级回退机制，确保系统稳定运行 安全可靠：多层安全防护，权限控制完善 性能优化：并发控制、流式处理、多级缓存 可测试性：依赖注入支持，单元测试覆盖完善 技术栈特点 语言：TypeScript (ESM)，类型安全 运行时：Node 22+，Bun 支持 AI 集成：Pi Agent Core，多模型支持 消息渠道：10+ 种渠道，插件化扩展 媒体处理：多模态统一处理，多提供商适配 系统集成：launchd、systemd、Docker 适用场景 AI 助手平台：多渠道统一接入的 AI 助手 自动化工具：支持复杂工作流的自动化 多模态应用：需要处理图像&#x2F;音频&#x2F;视频的应用 企业集成：需要集成多种通讯工具的企业应用 OpenClaw 的架构设计展现了现代 AI 应用的最佳实践，通过清晰的分层、强大的扩展能力和完善的安全机制，构建了一个健壮、灵活、易于维护的 AI 工具链平台。 附录关键文件索引Gateway 网关系统 src/gateway/server.impl.ts - Gateway 主实现 src/gateway/server-methods.ts - 请求路由与授权 src/gateway/server-ws-runtime.ts - WebSocket 运行时 src/gateway/session-utils.ts - 会话管理工具 Channel 渠道抽象层 src/channels/plugins/types.plugin.ts - ChannelPlugin 接口定义 src/channels/plugins/catalog.ts - 插件发现与加载 src/channels/allowlist-match.ts - Allowlist 匹配逻辑 src/channels/command-gating.ts - Command-Gating 实现 src/channels/mention-gating.ts - Mention-Gating 实现 Agent 代理引擎 src/agents/pi-embedded-runner/run.ts - Agent Runner 主入口 src/agents/pi-embedded-runner/model.ts - 模型解析 src/agents/failover-error.ts - 容错错误分类 src/agents/model-fallback.ts - 模型回退机制 src/agents/pi-tools.ts - 工具创建 src/agents/tools/canvas-tool.ts - Canvas 工具 Media 媒体理解管道 src/media-understanding/apply.ts - 媒体理解入口 src/media-understanding/providers/index.ts - 提供商注册表 src/infra/net/ssrf.ts - SSRF 防护 src/media/image-ops.ts - 图像处理 src/media/fetch.ts - 媒体获取 路由与控制层 src/routing/resolve-route.ts - 路由决策 src/auto-reply/reply/session.ts - 会话初始化 src/auto-reply/reply/dispatch-from-config.ts - 回复分发 src/auto-reply/reply/reply-dispatcher.ts - ReplyDispatcher src/auto-reply/reply/inbound-dedupe.ts - 消息去重 依赖注入 src/cli/deps.ts - CliDeps 定义 src/cli/outbound-send-deps.ts - 依赖转换 src/infra/outbound/deliver.ts - OutboundSendDeps 参考链接 官方文档：https://docs.openclaw.ai GitHub 仓库：https://github.com/openclaw/openclaw Pi Agent Core：https://github.com/badlogic/pi-mono","categories":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/tags/AI/"}]},{"title":"AI 代码生成工具效能度量指南","slug":"AI-代码生成工具效能度量指南","date":"2026-01-28T08:33:47.000Z","updated":"2026-01-28T08:45:36.816Z","comments":true,"path":"2026/01/28/AI-代码生成工具效能度量指南/","permalink":"https://www.silenceboy.com/2026/01/28/AI-%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90%E5%B7%A5%E5%85%B7%E6%95%88%E8%83%BD%E5%BA%A6%E9%87%8F%E6%8C%87%E5%8D%97/","excerpt":"","text":"本指南专注于AI代码生成工具（如 Cursor、GitHub Copilot、通义灵码等商业IDE辅助工具）的效能度量。必须跳出”代码行数”的陷阱，建立一个覆盖 采用（Adoption）、效能（Productivity）、质量（Quality）、体验（Experience）与成本（Cost） 的立体评估模型。 核心度量维度体系我们将度量指标分为五个层级，分别服务于不同的管理视角： 采用与活跃度 (Adoption &amp; Engagement) —— 解决”有没有在用” 指标名称 定义/计算公式 行业基准/说明 工具渗透率(Penetration Rate) 授权并激活工具人数 / 团队总人数 衡量推广广度。目标：&gt;80% 深度活跃度(Deep Usage) 日均调用AI次数 &gt; N次 的用户占比 仅看DAU不够，需关注高频用户（Power Users）比例。重点关注周活跃留存。建议：N=20次/天 功能热度分布(Feature Heatmap) 代码补全 vs Chat问答 vs 块生成 vs 代码解释的调用比率 分析开发者最依赖哪个功能。通常”补全”频次最高(60-70%)，”Chat”解决复杂问题价值最大(20-30%) 跨编程语言使用率(Language Coverage) 各编程语言使用AI的频率分布 识别AI在哪些语言表现更好。通常：Python/JS/TS 采纳率最高，C++/Rust 相对较低 激活速度(Time to First Value) 从安装到首次接受AI建议的平均时长 衡量上手门槛。目标：&lt;30分钟 生产力与效能 (Productivity &amp; Flow) —— 解决”是不是快了” 指标名称 定义/计算公式 行业基准/说明 代码采纳率(Acceptance Rate) (Tab键采纳次数 + Chat应用次数) / AI推荐总次数 核心指标。• Copilot基准：20%-35% 为正常，&lt;15% 说明推荐质量差或干扰大。• 需区分”单行补全”（20-30%）与”块生成”（10-20%） AI代码贡献占比(AI Contribution Ratio) 最终入库的AI生成代码行数 / 新增代码总行数 直接反映AI的生产力贡献。⚠️ 注意：需结合Git提交数据，剔除生成后立即被删除的代码。行业均值：15-30% 编码速度提升(Coding Velocity) 对比使用AI前后的代码提交频率/单位时间代码产出 GitHub研究：平均提升55%编码速度。建议按语言、任务类型分层统计 响应时延(Latency) AI建议从触发到展示的平均耗时 直接影响体验。目标：P50&lt;300ms，P95&lt;800ms。超过1秒会严重影响心流 首Token时间(Time to First Token) Chat模式下，从发送请求到首个字符返回的时间 衡量对话流畅度。目标：P95&lt;500ms 完整性生成效率(Generation Completeness) 完整函数/类一次生成成功率 衡量块生成能力。目标：&gt;70%无需重新生成 任务完成时间缩减(Task Time Reduction) 特定任务（如CRUD、单元测试）使用AI前后的耗时对比 建立任务基准库，定期AB测试。典型场景：单元测试生成节省60-80%时间 Context切换减少(Context Switch Reduction) 使用AI后查阅文档/搜索引擎的频率下降比例 反映AI知识库价值。目标：减少40%+ 外部查询 质量与可靠性 (Quality &amp; Accuracy) —— 解决”是不是好了” 指标名称 定义/计算公式 行业基准/说明 AI代码留存率(Code Retention) T+14天后仍保留的AI代码行数 / AI生成时的总行数 比”采纳率”更真实。如果代码被采纳但很快被重写，说明AI生成的是”低质量代码”。目标：&gt;85% 代码缺陷密度(Defect Density) AI生成代码关联的Bug数 / AI生成代码千行数 对比人工代码的Bug率。Merico研究：AI代码可能在安全性和边缘情况处理上较弱。需长期跟踪 安全漏洞检测率(Security Vulnerability Rate) AI生成代码中检测出的安全漏洞数 / AI生成代码总量 结合SAST工具(如SonarQube)分析。重点关注：SQL注入、XSS、硬编码密钥 代码重复率(Code Duplication) AI生成代码的重复块占比 过高说明AI在”复制粘贴”。需与代码库平均水平对比 测试覆盖率影响(Test Coverage Impact) 使用AI生成测试代码后，代码覆盖率的变化 AI生成测试用例的价值度量。目标：覆盖率提升10-20% Lint合规率(Linting Pass Rate) AI生成代码首次通过静态检查的比率 衡量代码规范性。目标：&gt;90% 首次通过 API使用正确性(API Accuracy) AI建议的API调用无需修改的比率 尤其针对第三方库/框架。反映训练数据时效性 逻辑复杂度(Cyclomatic Complexity) AI生成代码的平均圈复杂度 vs 人工代码 评估可维护性。AI生成的代码不应更复杂 开发者体验 (Developer Experience) —— 解决”爽不爽” 指标名称 定义/计算公式 行业基准/说明 感知效能提升(Perceived Velocity) 问卷调研：”AI帮你节省了多少时间？” 主观数据往往比客观数据更能预测工具的长期留存。目标：&gt;70%用户感知显著提升 心流干扰度(Flow Disruption) 主动关闭/拒绝推荐的次数 / 总弹窗次数 衡量AI是否”太烦人”。干扰度&gt;50%会导致开发者禁用插件 NPS（净推荐值）(Net Promoter Score) 标准NPS问卷：0-10分，是否愿意推荐给同事 目标：NPS&gt;40为优秀 功能满意度(Feature Satisfaction) 针对不同功能（补全/Chat/重构）的满意度分项打分 识别改进优先级 学习曲线(Learning Curve) 新用户从安装到达到平均采纳率的时间 衡量易用性。目标：&lt;3天 错误容忍度(Error Tolerance) 用户遇到N次错误建议后，仍继续使用的比例 衡量产品粘性。健康阈值：3次内错误不影响继续使用 成本效益分析 (Cost &amp; ROI) —— 解决”值不值” 指标名称 定义/计算公式 行业基准/说明 单位代码成本(Cost per Line) API订阅总成本 / AI生成并入库的代码行数 计算AI生成每行代码的真实成本 ROI（投资回报率）(Return on Investment) (节省的人力成本 - 工具订阅成本) / 工具订阅成本 关键业务指标。需结合人效、交付速度综合评估 Token使用效率(Token Efficiency) 采纳代码的Token消耗 / 总Token消耗 优化Prompt策略，减少无效Token浪费 人效提升(Productivity Gain) 使用AI后，人均产出代码量/功能点的增长比例 行业均值：15-35%人效提升 招聘成本节约(Hiring Cost Reduction) 因效率提升而减少的招聘需求对应的成本节约 间接价值度量 场景化深度度量方法针对AI代码生成工具的不同使用模式，采用差异化度量策略。 场景 A：行内代码补全 (Inline Completion / Ghost Text)重点：微观交互数据的精确采集 代码采纳率 (Acceptance Rate) 进阶版 区分模式： 单行补全：通常为变量名、函数调用补全 多行补全：跨行的代码块建议 公式优化： 说明：过滤掉展示时间极短（误触）或字符极少（无意义）的建议。 上下文理解准确度 (Context Accuracy) 逻辑：分析AI建议是否符合当前文件的代码风格、命名规范、业务逻辑 人工抽样评估：每周随机抽取100个建议，人工评分（1-5分） 目标：平均分&gt;4.0 补全完整度 (Completion Sufficiency) 公式：无需修改直接使用的采纳次数总采纳次数 解读：&gt;80%为优秀，表明AI建议即用即得 场景 B：对话式代码生成 (Chat / Inline Chat)重点：对话质量与生成准确性 首次正确率 (First-time-right Rate) 定义：无需重新生成或修改Prompt，首次生成即满足需求的比率 目标：&gt;60% Prompt迭代次数 (Prompt Iterations) 公式：平均单个任务的Prompt重试次数 解读：&lt;2次为优秀，&gt;4次说明AI理解能力不足 代码解释准确性 (Explanation Accuracy) 场景：用户选中代码请求解释 人工评估：准确性、完整性、可理解性三个维度 多轮对话连贯性 (Context Retention) 测试方法：在对话中引用前几轮内容，验证AI是否记住 目标：5轮内上下文保持准确 场景 C：代码重构与优化 (Refactoring)重点：改进质量与安全性 重构有效性 (Refactoring Effectiveness) 度量维度： 代码复杂度下降比例 性能提升（如执行耗时减少） 可读性改善（Lint评分提升） 向后兼容性 (Backward Compatibility) 测试：重构后原有测试用例通过率 目标：100% 测试通过（无破坏性变更） 场景 D：单元测试生成 (Test Generation)重点：测试覆盖与质量 测试覆盖率提升 (Coverage Improvement) 公式：使用AI后覆盖率 - 使用AI前覆盖率 目标：单次生成提升15%+ 覆盖率 测试有效性 (Test Validity) 指标： 生成的测试能否真正发现Bug 测试断言的准确性 边界条件覆盖情况 评估方法：引入已知Bug，验证AI生成的测试能否检出 测试可维护性 (Test Maintainability) 标准：测试代码是否遵循Given-When-Then模式、命名是否清晰 数据采集架构实施方案要实现上述度量，建议搭建”端+云”结合的数据链路： 端侧数据采集 (Client - IDE Plugin Telemetry)实施方案： IDE Extension Hook：如果使用 VSCode，可开发轻量级 Extension 或利用企业版 Telemetry API 关键事件监听： onInlineCompletionShown: 记录推荐弹出时刻及 Metadata（语言、上下文长度、触发方式） onInlineCompletionAccepted: 记录用户接受动作（Tab / Click Apply） onInlineCompletionRejected: 记录拒绝方式（手动删除 / ESC / 继续输入覆盖） textDocument/didChange: 记录代码实际变更量 chatRequest/response: 记录Chat对话内容、Token消耗、响应时延 隐私合规： 仅采集元数据，不上传完整代码内容 敏感信息脱敏（如密钥、内部域名） 符合GDPR/隐私保护法规 反作弊机制： 在 Metadata 中记录 session_id、user_id、trigger_type 检测异常高频调用（防止脚本刷数据） 过滤机器人行为（如固定时间间隔的重复操作） 平台侧数据处理 (Gateway &amp; Analytics Pipeline)架构组件： LLM 网关（API Gateway）： 拦截所有 AI 请求，记录 Prompt Token 和 Completion Token 计算实时成本（基于Token单价） 按团队/用户分组统计配额使用情况 Git 数据分析（Git Analysis）： 在 CI/CD 流水线中集成分析脚本 解析 Commit Message 中的 AI 标识（如 Co-authored-by: GitHub Copilot） 利用 git blame 追踪 AI 代码的留存情况 分析 Code Churn（代码流失率）与 Bug 关联 质量门禁集成（Quality Gate）： 接入 SonarQube、ESLint 等工具 对比 AI 代码与人工代码的质量分数 自动标记高风险AI生成代码（如安全漏洞） 效能驾驶舱 (Analytics Dashboard)数据源整合： IDE 埋点数据（JSON格式，实时上报） Git 仓库数据（定时同步，增量分析） CI/CD 流水线数据（构建时间、测试通过率） 项目管理数据（Jira/Task完成速度） 展示层设计： CTO / 管理层视图： ROI仪表盘：节省成本 vs 订阅成本 DORA 指标趋势：部署频率、变更前置时间、MTTR 团队效能对比：不同团队的AI采用情况 Team Leader 视图： 团队渗透率与活跃度 代码采纳率与留存率趋势 AI 生成代码的 Review 耗时变化 功能热度分布（补全 vs Chat） 开发者个人视图： 个人AI使用统计（类似GitHub Contribution Graph） 效能提升报告（节省时间估算） 技能成长建议（AI擅长的领域 vs 短板） 推荐技术栈： 数据存储：ClickHouse（高性能时序分析）/ PostgreSQL 可视化：Grafana / Metabase / 自研Dashboard 流处理：Kafka + Flink（实时数据处理） 避坑指南与最佳实践在行业实践案例中，以下是常见的度量误区与应对策略： ❌ 错误做法 1：仅考核”生成代码行数”后果： 开发者会利用 AI 生成大量冗余注释、样板代码刷绩效 导致代码库膨胀（Code Bloat），增加技术债务 代码质量下降，可维护性变差 正确做法： 必须结合”代码留存率”（14天后仍保留的比例） 引入”代码复杂度”指标（圈复杂度、重复率） 按有效功能点交付量考核，而非代码行数 ❌ 错误做法 2：忽略”思考时间”的价值现象： AI 使得编码变快了，但开发者花更多时间在： 设计 Prompt Review AI 的代码 修复 AI 引入的Bug 正确做法： 使用”任务总交付周期”（End-to-End）作为最终裁判 不仅看”编码阶段耗时”，还要看”测试、Review、部署”全流程 关注”Context切换”减少情况（减少查文档、搜索的次数） ❌ 错误做法 3：盲目追求高采纳率现象： 采纳率 80% 并不一定是好事 可能意味着AI只在生成简单、重复的代码（价值低） 开发者为了”完成KPI”，接受所有建议（包括错误的） 正确做法： 分析采纳代码的 AST（抽象语法树）结构 高价值采纳应包含：逻辑控制流、算法实现、复杂业务逻辑 低价值采纳：简单变量定义、import语句、注释 建立采纳价值分：复杂代码采纳权重更高 ❌ 错误做法 4：只看短期数据，不看长期留存现象： 初期新鲜感导致高使用率 3个月后使用率断崖下跌（工具被遗忘或主动禁用） 正确做法： 建立Cohort分析：按安装时间分组，追踪留存曲线 识别流失原因：性能问题？质量问题？体验问题？ 设定健康阈值：6个月留存率&gt;70% ❌ 错误做法 5：数据采集侵犯隐私风险： 上传完整代码到云端，泄露商业机密 记录开发者个人信息，违反隐私法规 正确做法： 最小化原则：只采集必要的元数据（事件类型、时间戳、语言） 本地化处理：敏感数据在客户端脱敏后再上报 透明化：告知开发者采集了哪些数据，提供退出选项 合规审查：符合GDPR、CCPA等隐私保护法规 ✅ 最佳实践 1：建立对照组实验方法： A/B测试：部分团队使用AI，部分团队不使用 对比两组的交付速度、代码质量、Bug率 消除”幸存者偏差”（只有高意愿的人用AI） ✅ 最佳实践 2：分层度量策略： 按编程语言分层：Python vs Java vs Go 按任务类型分层：新功能开发 vs Bug修复 vs 重构 按开发者经验分层：Junior vs Senior vs Architect 识别AI在哪些场景价值最大，针对性优化 ✅ 最佳实践 3：建立反馈闭环机制： 在IDE中提供”点赞/点踩”按钮（对AI建议快速反馈） 定期问卷调研（每季度NPS调研） 开发者访谈（深度了解痛点） 将反馈数据用于模型微调和产品优化 ✅ 最佳实践 4：设定合理的北极星指标推荐北极星指标： 核心目标：开发者效能提升（综合指标） 公式：其中 ，权重根据组织目标调整 避免单一指标陷阱：不能只看速度或只看质量 度量报告模板123456789101112131415161718192021222324252627282930313233343536# AI代码生成工具效能月报 - YYYY年MM月## 一、核心数据概览- 工具渗透率：XX%（环比±X%）- 活跃用户数：XXX人（DAU/MAU）- 代码采纳率：XX%（目标：&gt;25%）- AI代码贡献：占新增代码XX%## 二、生产力提升- 编码速度提升：XX%- 任务完成时间缩减：平均节省XX分钟/任务- Context切换减少：XX%## 三、质量表现- 代码留存率：XX%（14天）- Bug密度：AI代码 X.X bugs/KLOC vs 人工代码 X.X bugs/KLOC- 安全漏洞：发现X个，已修复X个- 测试覆盖率：提升XX%## 四、开发者体验- NPS评分：XX（目标：&gt;40）- 满意度：XX%开发者认为显著提升效率- 主要反馈： - 优点：... - 痛点：...## 五、成本与ROI- 工具订阅成本：¥XXX- 估算节省人力成本：¥XXX- ROI：XX%- Token消耗：XXX万（环比±X%）## 六、下月优化计划1. ...2. ...3. ... 总结通过这套AI代码生成工具度量体系，您可以： 证明价值：从”AI写了多少代码”进化到”AI为业务交付带来了多大加速与质量提升” 识别问题：发现AI在哪些场景表现不佳，针对性优化 优化投入：计算ROI，决策是否扩大部署或调整策略 提升体验：基于开发者反馈，持续改进产品体验 核心原则： 不追求单一指标极致，而是多维度平衡 不只看短期数据，更看长期价值与留存 不只看工具本身，更看对研发全流程的影响 不只看数字，更听开发者真实声音 记住：度量的目的不是为了考核，而是为了持续改进。让AI真正成为开发者的得力助手，而非负担。","categories":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/tags/AI/"}]},{"title":"Base、Chat与Reasoning模型全解析","slug":"Base、Chat与Reasoning模型全解析","date":"2026-01-22T06:58:30.000Z","updated":"2026-01-22T07:27:12.011Z","comments":true,"path":"2026/01/22/Base、Chat与Reasoning模型全解析/","permalink":"https://www.silenceboy.com/2026/01/22/Base%E3%80%81Chat%E4%B8%8EReasoning%E6%A8%A1%E5%9E%8B%E5%85%A8%E8%A7%A3%E6%9E%90/","excerpt":"","text":"目前模型主要分为三类：Base 版和 Instruct/Chat 版和 Reasoning Models（推理模型）。 这三个版本究竟代表了什么？它们是如何训练出来的？在实际应用中又该如何选择？本文将基于大模型技术演进的事实逻辑，带你彻底看懂这三者的进化之路。 Base模型：博学的“续写者”Base模型（基座模型） 是所有大模型的起点，也是“地基”。 核心定义：概率预测机Base 模型在海量（万亿级 token）的互联网文本数据上进行预训练（Pre-training）。它的核心训练目标只有一个：Next Token Prediction（预测下一个词）。 行为特征它是极致的“文字接龙”高手。 输入：“牛顿发现了…” Base输出：“…万有引力定律。他在1687年发表的论文中…” 输入：“中国的首都是哪里？” Base输出：“美国的首都是哪里？法国的首都是哪里？”（因为它认为你在列清单，而不是提问） 典型代表 Llama-3 Base Qwen-2.5 Base GPT-3 (原始 Davinci) 关键点Base 模型拥有极其丰富的知识，但它不懂人类的交互指令，它只负责把文本顺畅地写下去。 Chat模型：懂礼貌的“助手”为了让 Base 模型变得好用，研究人员引入了指令微调（Instruction Tuning）和人类对齐（Alignment），诞生了 Chat模型（或 Instruct 模型）。这是目前市面上最常见的模型形态。 核心定义：指令遵循者Chat 模型在 Base 模型的基础上，经历了两个关键阶段： SFT（监督微调）：学习“提问-回答”的格式。 RLHF&#x2F;DPO（人类偏好对齐）：学习什么样的回答是安全的、有帮助的、语气恰当的。 它不仅学会了知识，更学会了 “此时此刻，我是一个助手，我要回答用户的问题，而不是续写它。” 行为特征它是高情商的客服。 输入：“中国的首都是哪里？” Chat输出：“中国的首都是北京。” 输入：“如何制造毒药？” Chat输出：“对不起，我不能提供相关帮助，因为这违反了安全准则…” 典型代表 ChatGPT (GPT-4o) Claude 3.5 Sonnet Llama-3-Instruct 局限性Chat 模型本质上是在模仿人类的回答模式。遇到复杂的逻辑问题（如高难度奥数题），它往往凭直觉（系统1思维）快速作答，因此容易出现“幻觉”或一本正经地胡说八道。 Reasoning模型：沉默的“思考者”Reasoning模型（推理模型） 是大模型领域的最新范式转移，代表了从“快思考”向“慢思考”的进化。 核心定义：思维链（CoT）内化Reasoning 模型引入了 强化学习（RL） 的大规模应用，专门奖励模型在输出最终答案前进行 思维链（Chain of Thought） 推导。 与 Chat 模型不同，Reasoning 模型在回答问题之前，会先在内部进行长时间的“思考”：把复杂问题拆解、验证每一步逻辑、如果发现错误会自我纠正，最后才输出结果。业界称之为Test-time Compute（测试时计算&#x2F;推理侧计算），即用更多的时间换取更高的智能。 行为特征它是严谨的数学家或逻辑学家。 输入：“9.11 和 9.8 哪个大？” Chat模型可能秒回：“9.11 大。”（因为它像看版本号一样看数字，且追求快） Reasoning模型反应： (内部思考过程)：用户问的是数字大小比较。首先看整数部分，都是9。再看小数部分，0.11 和 0.8。0.8 等于 0.80。0.80 明显大于 0.11。 最终输出：“9.8 比 9.11 大。” 典型代表 OpenAI o1 (o1-preview, o1-mini) DeepSeek-R1 关键差异Reasoning 模型最显著的特征是慢。它不是卡顿，而是在思考。对于简单的”你好”类问候，它的效率不如 Chat 模型；但在数学、编程、逻辑谜题上，它的准确率呈指数级上升。 推理成本警示：看不见的”思考税”Reasoning 模型的强大能力背后，是高昂的计算成本： Token消耗陷阱：一个复杂的数学题，Chat 模型可能消耗 200 tokens，而 Reasoning 模型可能消耗 5000+ tokens（内部思考 3000+ + 最终输出 2000）。成本差异可达 25 倍。 可控性差异：模型自主决定思考时间（最长可达 1 分钟） 三者横向对比表 维度 Base模型 (基座) Chat模型 (对话&#x2F;指令) Reasoning模型 (推理) 思维模式 直觉补全 (Autocomplete) 交互响应 (Response) 深度思考 (Deliberate Thinking) 核心训练 预训练 (Pre-training) SFT + RLHF (对齐) 大规模强化学习 (RL on CoT) 响应速度 极快 快 较慢 (首字延迟高) 擅长领域 下游微调、文本续写 文案写作、摘要、对话、翻译 数学、复杂代码、科研推理 计算消耗 训练时大，推理时小 训练时中，推理时小 训练时大，推理时也大 人类形象类比 读过万卷书但呆板的书呆子 训练有素、反应快的金牌客服 严谨、深思熟虑的老教授 开发者与用户该如何选择？在当前的技术环境下，选择模型不再是一刀切，而是根据场景决定： 场景 A：你需要构建一个垂直行业的知识库助手（RAG） 首选：Chat 模型（如 GPT-4o, Llama-3-Instruct）。 理由：你需要模型听得懂指令，且不仅是逻辑推理，更多是语言理解和归纳。Reasoning 模型在这里可能“杀鸡用牛刀”，且延迟过高影响体验。 场景 B：你需要解决高难度的算法竞赛题、分析复杂的法律逻辑或进行科学研究 首选：Reasoning 模型（如 o1, DeepSeek-R1）。 理由：Chat 模型容易产生幻觉，而 Reasoning 模型通过内部的自我博弈和纠错，能大幅提高准确率。 场景 C：你是一个大模型研究员，想要训练自己的模型 首选：Base 模型。 理由：你需要一张白纸。Chat 模型已经被”洗脑”成了对话模式，很难再调整去执行非对话类的特殊任务（如纯粹的文本补全或特定格式生成）。 场景 D：你在构建企业级 AI 应用，需要兼顾成本与性能 推荐：路由器 + 多模型混合架构。 架构设计：1234用户请求 → 意图分类器(轻量Chat模型) ├─ 简单问答/闲聊 → Chat模型(快速响应) ├─ 复杂计算/推理 → Reasoning模型(高精度) └─ 领域特定任务 → 微调的Base模型 实际收益： 成本降低 60%（大部分请求用便宜的 Chat 模型） 用户体验提升（简单问题不卡顿） 准确率提升（关键问题用 Reasoning 模型兜底） 结语：我们正站在 AI 能力跃迁的节点大模型的进化史，就是一部从”模仿人类说话”到”模仿人类思考”的历史。 Base 模型让我们见证了知识的压缩——万卷书装进千亿参数。 Chat 模型让我们实现了人机的流畅交互——AI 第一次真正”听懂”人话。 Reasoning 模型则开启了 AI 像System 2那样进行深思熟虑的新时代——从”快思考”到”慢思考”。 未来展望2026 年，我们正站在一个关键节点： 小模型的崛起：7B 参数的模型（如 Qwen2.5-7B-Instruct）性能已逼近早期 70B 模型，边缘设备推理成为可能。 Reasoning 能力的平民化：DeepSeek-R1 的开源，让每个开发者都能训练自己的推理模型。 混合架构成为主流：未来的 AI 应用将不再依赖单一模型，而是像人脑一样，根据任务类型动态调用不同的”思维模式”。 记住：没有”最好”的模型，只有”最合适”的选择。理解三类模型的本质差异，是用好 AI 的第一步。","categories":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/tags/AI/"}]},{"title":"Tokenizer 和 Embedding 的区别","slug":"Tokenizer-和-Embedding-的区别","date":"2026-01-21T09:29:34.000Z","updated":"2026-01-21T10:04:00.680Z","comments":true,"path":"2026/01/21/Tokenizer-和-Embedding-的区别/","permalink":"https://www.silenceboy.com/2026/01/21/Tokenizer-%E5%92%8C-Embedding-%E7%9A%84%E5%8C%BA%E5%88%AB/","excerpt":"","text":"在接触大语言模型（LLM）或自然语言处理（NLP）时，Tokenizer（分词器） 和 Embedding（嵌入） 是两个出现频率极高，却常被初学者混淆的概念。它们似乎都在做“把文字变成数字”的工作，但它们在模型中的角色、原理和目的却截然不同。 简单来说：Tokenizer 负责“认字”（将文本切分为离散的符号），而 Embedding 负责“理解”（将符号转化为包含语义的数学向量）。 Tokenizer（分词器）：LLM 的“字典”大模型无法直接阅读我们看到的文字（如中文汉字或英文字母），它只能处理数字。Tokenizer 的工作就是充当翻译前的“查字典”步骤。 核心任务Tokenizer 的作用是将非结构化的 原始文本（Raw Text） 切分成模型能处理的最小单位——Token，并将这些 Token 映射为唯一的整数索引（Input IDs）。 工作流程 切分（Splitting）： 依据特定的规则（如空格、标点、或更高级的算法如 BPE、WordPiece），将句子切开。 映射（Mapping）： 在一个预先构建好的词表（Vocabulary）中查找每个片段对应的编号。 举个例子假设我们的输入是：“我爱AI” Tokenizer 切分： [&quot;我&quot;, &quot;爱&quot;, &quot;AI&quot;] 查表映射： 假设在词表中，“我”是 5，“爱”是 20，“AI”是 101。 Tokenizer 输出： [5, 20, 101] 关键特征 输出是离散的整数： 比如 101 代表 “AI”，102 可能就代表完全不相关的 “香蕉”。在 Tokenizer 的阶段，数字之间没有数学上的关联（101 和 102 仅仅是索引的先后，没有语义上的远近）。 它不包含语义： Tokenizer 不知道”我”和”你”是相似的代词，它只知道它们在词表中位于不同的位置。 工作流程可视化graph LR A[原始文本'我爱AI'] --> B[预处理Normalization] B --> C[切分算法BPE/WordPiece] C --> D[\"Token序列['我','爱','AI']\"] D --> E[词表查找Vocabulary Lookup] E --> F[\"Token IDs[5, 20, 101]\"] style A fill:#e1f5ff style F fill:#fff4e1 常见的Tokenizer算法现代大语言模型主要使用以下几种分词算法： 算法 代表模型 特点 优势 BPE (Byte Pair Encoding) GPT系列 基于字节对频率合并 能有效处理未登录词，压缩率高 WordPiece BERT 基于似然最大化 平衡了词汇量和表达能力 SentencePiece Llama、T5 直接处理原始文本 语言无关，支持多语言 Unigram mBART 基于概率模型 可以生成多种分词方案 特殊Token说明在词表中，除了普通词汇，还有一些特殊的Token用于控制模型行为： [PAD]：填充符，用于对齐不同长度的序列 [UNK]：未知词，处理词表外的词汇 [CLS]：句子开始符（BERT） [SEP]：句子分隔符 [MASK]：掩码符（用于MLM任务） &lt;|endoftext|&gt;：文本结束符（GPT） Embedding（嵌入层）：LLM 的“大脑连接”如果说 Tokenizer 只是把文字变成了身份证号，那么 Embedding 就是把这些身份证号变成了包含丰富特征的个人档案。 核心任务Embedding 是一个查找表（Lookup Table）或神经网络层，它将 Tokenizer 输出的整数 ID 转换为一个高维稠密向量（Dense Vector）。这个向量是一串浮点数，代表了该词在语义空间中的位置。 工作流程 接收输入： 拿到 Tokenizer 传来的 ID（例如 101）。 向量化： 在高维空间中找到 101 对应的向量表示。 举个例子继续上面的“我爱AI”，输入 ID 为 [5, 20, 101]。 Embedding 转化： 5 (“我”) → [0.12, -0.58, 0.99, ...] 20 (“爱”) → [0.77, 0.23, -0.11, ...] 101 (“AI”) → [-0.45, 0.88, 0.02, ...] 关键特征 输出是连续的浮点数向量： 比如 768 维或 4096 维的数组。 它包含语义（Semantic Meaning）： 这是 Embedding 最神奇的地方。在训练过程中，模型学会了将含义相近的词放在向量空间中靠近的位置。 例如：在向量空间中，”猫”和”狗”的距离会非常近，而”猫”和”冰箱”的距离会很远。 经典的例子：King（国王） - Man（男人） + Woman（女人） ≈ Queen（女王）。这种数学运算只有在 Embedding 层之后才能实现，在 Tokenizer 阶段的整数 ID 上是做不到的。 语义空间可视化graph TB subgraph 动物语义空间 A1[猫Cat] A2[狗Dog] A3[老虎Tiger] end subgraph AI技术语义空间 B1[机器学习ML] B2[深度学习DL] B3[神经网络NN] end subgraph 日常用品语义空间 C1[冰箱Fridge] C2[桌子Table] C3[椅子Chair] end A1 -.语义相近.-> A2 A2 -.语义相近.-> A3 B1 -.语义相近.-> B2 B2 -.语义相近.-> B3 C2 -.语义相近.-> C3 A1 ~~~|语义距离远| C1 style A1 fill:#ffd6d6 style A2 fill:#ffd6d6 style A3 fill:#ffd6d6 style B1 fill:#d6e5ff style B2 fill:#d6e5ff style B3 fill:#d6e5ff style C1 fill:#e1ffe1 style C2 fill:#e1ffe1 style C3 fill:#e1ffe1 Embedding 维度的选择不同模型使用不同的向量维度，这是性能与效率的权衡： 模型 Embedding维度 参数量影响 BERT-base 768 较小，训练快 BERT-large 1024 中等 GPT-3 12288 极大，表达能力强 Llama-2-7B 4096 平衡性能与效率 维度越大： ✅ 表达能力越强，能捕捉更细微的语义差异 ✅ 模型容量更大，理解能力更强 ❌ 计算成本更高，内存占用更大 ❌ 更容易过拟合（在小数据集上） Position Embedding值得注意的是，除了词嵌入（Token Embedding），模型还会加入 位置嵌入（Position Embedding） 来告诉模型词的先后顺序： 1最终输入 = Token Embedding + Position Embedding 这是因为 Transformer 架构本身不具备序列顺序的概念，需要通过位置编码来补充位置信息。 核心区别对比表为了更直观地理解，我们可以从以下几个维度对比： 维度 Tokenizer (分词器) Embedding (嵌入层) 输入 原始文本字符串 (Text) 整数索引 (Token IDs) 输出 整数索引列表 (如 [101, 205]) 高维浮点数向量 (如 [0.1, -0.9...]) 数据性质 离散的 (Discrete)：数字仅代表位置 连续的 (Continuous)：数字代表特征 是否包含语义 不包含：只做硬性映射 包含：捕捉词义、词性、关联 可训练性 通常在预训练前固定 (如 BPE 算法统计得出) 是模型参数的一部分，随模型训练不断优化 类比 字典索引：查到“苹果”在第 10 页 对苹果的理解：圆的、红的、水果、甜的 它们是如何协作的？（Pipeline）在任何一个现代大语言模型（如 GPT-4, Llama 3, BERT）中，数据流向都是固定的串行关系： 用户输入： What is tokenizer? Tokenizer 层： 切分并映射为 ID → [2054, 2003, 19204, 1029] Embedding 层： 将 ID 转化为向量 → [[0.1, ...], [0.5, ...], ...] Transformer 层（Attention）： 模型在向量基础上进行复杂的数学运算（注意力机制等），理解上下文。 完整数据流转过程graph TD A[用户输入文本'What is tokenizer?'] --> B[Tokenizer 预处理] B --> C[Token切分What/is/token/izer/?] C --> D[\"ID映射[2054, 2003, 19204, 1029]\"] D --> E[Embedding层查找表Lookup] E --> F[\"Token向量矩阵Shape: [4, 768]\"] F --> G[+ Position Embedding] G --> H[最终输入向量] H --> I[Transformer层Self-Attention] I --> J[前馈神经网络FFN] J --> K[Layer Normalization] K --> L[...重复N层] L --> M[输出层Language Model Head] M --> N[预测结果] style A fill:#e1f5ff style D fill:#fff4e1 style F fill:#ffe1f5 style H fill:#e1ffe1 style N fill:#ffe1e1 classDef tokenizer fill:#fff4e1,stroke:#ffa500 classDef embedding fill:#ffe1f5,stroke:#ff1493 classDef model fill:#e1ffe1,stroke:#00ff00 class B,C,D tokenizer class E,F,G,H embedding class I,J,K,L,M model 数据形状变化让我们追踪一个具体的例子，看数据是如何逐步变化的： 阶段 数据内容 数据类型 数据形状 原始输入 &quot;What is tokenizer?&quot; String（字符串） - Tokenizer输出 [2054, 2003, 19204, 1029] List[int]（整数列表） [4] Embedding输出 [[0.12, -0.58, ...], [...], ...] Tensor（浮点张量） [4, 768] +Position Embedding [[0.15, -0.50, ...], [...], ...] Tensor（浮点张量） [4, 768] 经过Transformer [[0.88, 0.23, ...], [...], ...] Tensor（浮点张量） [4, 768] 输出层 [[0.01, 0.85, 0.03, ...]] Tensor（概率分布） [4, 50257] 关键观察： Tokenizer → Embedding：从离散变为连续 Embedding → Transformer：维度保持不变，但语义理解加深 最后输出：从隐藏表示转换为词表概率分布（用于预测下一个词） 常见误区与易混淆点在学习过程中，很多初学者会产生以下误解： ❌ 误区1：认为 Tokenizer 的输出已经包含语义错误理解： “既然 Token ID 101 代表 ‘AI’，那它不就已经有意义了吗？” 正确理解： Token ID 只是索引编号，就像身份证号一样。身份证号 330106 和 330107 相邻，但这不代表这两个人有任何关系。只有经过 Embedding 层，模型才能理解词与词之间的语义关联。 ❌ 误区2：混淆 Token ID 和 Embedding 向量错误理解： “Token ID [101, 102] 不就是二维向量吗？” 正确理解： Token ID [101, 102] 是两个整数，不是向量 Embedding 后是 两个高维向量，如 [[0.1, 0.2, ..., 0.9], [0.5, 0.3, ..., 0.1]]，每个向量有 768 或更多维度 ❌ 误区3：认为 Embedding 是固定不变的错误理解： “Embedding 就是个查找表，训练前就确定好了。” 正确理解： Embedding 层的权重矩阵是可训练的。在模型训练过程中，它会不断调整，使得语义相近的词在向量空间中越来越接近。这是模型”学习”语义的核心机制。 ❌ 误区4：认为 Tokenizer 和 Embedding 可以互换错误理解： “我可以用不同的 Tokenizer 配同一个模型吗？” 正确理解： 不可以！ Tokenizer 的词表和 Embedding 层的权重矩阵必须严格对应： 词表中第 101 个位置是 “AI” Embedding 矩阵的第 101 行存储的就是 “AI” 的向量表示 如果 Tokenizer 改变，Embedding 矩阵也必须重新训练 ✅ 正确的理解框架graph LR A[Tokenizer文字→数字索引] --> B[Embedding索引→语义向量] B --> C[模型计算向量→理解] A -.不可分离.-> B style A fill:#fff4e1 style B fill:#ffe1f5 style C fill:#e1ffe1 Tokenizer 和模型的 Embedding 层是绑定的，不能分离 Tokenizer 处理的是符号映射（Symbol Mapping） Embedding 处理的是语义表示（Semantic Representation） 两者配合完成从”文字”到”机器可理解的数学对象”的转换 总结Tokenizer 和 Embedding 缺一不可，但分工明确： Tokenizer 解决了 “如何把无限的自然语言文本映射到有限的数字集合” 的问题。它是数据进入模型的入口。 Embedding 解决了 “如何让计算机理解这些数字背后的含义和关系” 的问题。它是模型进行思考的基础。 实践：用代码看清两者理论理解后，让我们用实际代码演示整个过程： 使用 Transformers 库演示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from transformers import AutoTokenizer, AutoModelimport torch# 1. 加载预训练的Tokenizer和模型（以BERT为例）model_name = &quot;bert-base-uncased&quot;tokenizer = AutoTokenizer.from_pretrained(model_name)model = AutoModel.from_pretrained(model_name)# 2. 原始文本text = &quot;What is tokenizer?&quot;# 3. Tokenizer阶段：文本 → Token IDstokens = tokenizer.tokenize(text) # 切分print(f&quot;切分后的Tokens: &#123;tokens&#125;&quot;)# 输出: [&#x27;what&#x27;, &#x27;is&#x27;, &#x27;token&#x27;, &#x27;##izer&#x27;, &#x27;?&#x27;]token_ids = tokenizer.encode(text, add_special_tokens=False)print(f&quot;Token IDs: &#123;token_ids&#125;&quot;)# 输出: [2054, 2003, 19204, 17629, 1029]print(f&quot;数据类型: &#123;type(token_ids)&#125;, 数据形状: &#123;len(token_ids)&#125;&quot;)# 输出: 数据类型: &lt;class &#x27;list&#x27;&gt;, 数据形状: 5# 4. Embedding阶段：Token IDs → 向量input_ids = torch.tensor([token_ids]) # 转为Tensorwith torch.no_grad(): # 模型内部会自动调用Embedding层 outputs = model(input_ids) embeddings = outputs.last_hidden_state # 获取Embedding输出print(f&quot;\\nEmbedding向量形状: &#123;embeddings.shape&#125;&quot;)# 输出: Embedding向量形状: torch.Size([1, 5, 768])# 解释: [batch_size=1, sequence_length=5, hidden_dim=768]print(f&quot;第一个词&#x27;what&#x27;的向量前10维: &#123;embeddings[0][0][:10]&#125;&quot;)# 输出: tensor([ 0.1234, -0.5678, 0.9012, ...])# 5. 验证语义相似性text1 = &quot;cat&quot;text2 = &quot;dog&quot;text3 = &quot;car&quot;def get_embedding(text): ids = tokenizer.encode(text, add_special_tokens=False) with torch.no_grad(): output = model(torch.tensor([ids])) return output.last_hidden_state[0][0] # 获取第一个token的向量vec_cat = get_embedding(text1)vec_dog = get_embedding(text2)vec_car = get_embedding(text3)# 计算余弦相似度from torch.nn.functional import cosine_similaritysim_cat_dog = cosine_similarity(vec_cat.unsqueeze(0), vec_dog.unsqueeze(0))sim_cat_car = cosine_similarity(vec_cat.unsqueeze(0), vec_car.unsqueeze(0))print(f&quot;\\n&#x27;cat&#x27; 和 &#x27;dog&#x27; 的相似度: &#123;sim_cat_dog.item():.4f&#125;&quot;)print(f&quot;&#x27;cat&#x27; 和 &#x27;car&#x27; 的相似度: &#123;sim_cat_car.item():.4f&#125;&quot;)# 输出示例:# &#x27;cat&#x27; 和 &#x27;dog&#x27; 的相似度: 0.8234# &#x27;cat&#x27; 和 &#x27;car&#x27; 的相似度: 0.3521 关键观察 Tokenizer 输出是列表：[2054, 2003, ...] - 简单的整数 Embedding 输出是张量：torch.Size([1, 5, 768]) - 高维浮点向量 语义相似性只在Embedding之后才有意义：cat和dog的向量相似度明显高于cat和car 直接访问Embedding层：12345678910# 直接获取模型的Embedding矩阵embedding_matrix = model.embeddings.word_embeddings.weightprint(f&quot;Embedding矩阵形状: &#123;embedding_matrix.shape&#125;&quot;)# 输出: Embedding矩阵形状: torch.Size([30522, 768])# 解释: [词表大小=30522, 向量维度=768]# 查看特定Token的Embeddingtoken_id = 2054 # &#x27;what&#x27;embedding_vector = embedding_matrix[token_id]print(f&quot;Token ID &#123;token_id&#125; 的Embedding向量: &#123;embedding_vector[:10]&#125;&quot;) 这个矩阵就是Embedding层的本质：一个 [词表大小 × 向量维度] 的查找表，每一行对应词表中一个词的向量表示。 延伸阅读与资源 经典论文： Word2Vec: Efficient Estimation of Word Representations BERT: Pre-training of Deep Bidirectional Transformers Attention Is All You Need (Transformer) 推荐工具： Hugging Face Transformers - 最流行的预训练模型库 SentencePiece - Google的语言无关Tokenizer tiktoken - OpenAI的高效Tokenizer 可视化工具： Embedding Projector - 可视化词向量空间 BertViz - 可视化注意力机制","categories":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/tags/AI/"}]},{"title":"E.I.O.S.知识管理方法论","slug":"E-I-O-S-知识管理方法论","date":"2026-01-14T07:33:21.000Z","updated":"2026-01-14T07:54:04.153Z","comments":true,"path":"2026/01/14/E-I-O-S-知识管理方法论/","permalink":"https://www.silenceboy.com/2026/01/14/E-I-O-S-%E7%9F%A5%E8%AF%86%E7%AE%A1%E7%90%86%E6%96%B9%E6%B3%95%E8%AE%BA/","excerpt":"","text":"E.I.O.S. 是指目前在个人知识管理（PKM）和”超级个体”圈子中新兴的一套方法论，全称为 Evolution Island Operating System（进化岛操作系统）。 这套方法论可以被视为 “第二大脑”的 AI 进化版。如果说 Tiago Forte 的第二大脑（BASB）是教你如何做一个更好的”图书管理员”（整理、分类），那么 E.I.O.S. 则是教你如何利用 AI 建立一个”自动化工厂”。 为什么叫”进化岛”？Island（岛屿）的隐喻： 在信息的海洋中，你需要一座属于自己的岛屿（独立的知识操作系统） 这座岛不是用来”堆积”信息的仓库，而是一个可持续进化的生态系统 岛上的”居民”是 AI Agent，它们帮你处理信息、生成产出 Operating System（操作系统）的隐喻： 传统知识管理是”应用软件”（你需要手动操作） E.I.O.S. 是”操作系统”（在后台自动运转，你只需要下指令） 它不是一个具体的工具，而是一套底层的工作流和思维范式 核心理念对比graph TB subgraph \"传统知识管理\" A1[信息输入] --> A2[手动整理] A2 --> A3[分类归档] A3 --> A4[偶尔查阅] A4 -.->|熵增| A5[最终遗忘] end subgraph \"E.I.O.S. 系统\" B1[信息输入] --> B2[AI 液化] B2 --> B3[自动路由] B3 --> B4[产品化输出] B4 --> B5[系统进化] B5 -.->|反馈循环| B2 end style A5 fill:#ff6b6b style B5 fill:#4ecdc4 核心差异： ❌ 传统方法：收藏 → 整理 → 存储 → 遗忘（单向衰减） ✅ E.I.O.S.：捕获 → 液化 → 产出 → 进化（循环增强） 核心定义：从”管理”到”进化”E.I.O.S. 认为传统知识管理（如收藏、打标签、做笔记）往往陷入”知识囤积“的陷阱。 典型场景：你收藏了 1000 篇文章，做了 500 条笔记，但真正用到的不到 5%。剩下的只是心理安慰。 E.I.O.S. 的三大转变： 🎯 目的转变：从”存储知识”到”生产资产” 🤖 手段转变：从”人工整理”到”AI 代理驱动” 💎 价值转变：从”拥有多少”到”产出什么” 核心理念：通过 AI Agent 将信息流快速转化为可执行的资产（代码、SOP、模板、清单），而非静态的笔记。 核心运作模型：L.A.P.E.这是 E.I.O.S. 的灵魂，对应知识流转的四个阶段： graph TB Input[信息输入视频/文章/代码/对话] --> L subgraph \"L.A.P.E. 循环\" L[🌊 Liquid液态化] --> A[🎯 Action行动路由] A --> P[📦 Product产品化] P --> E[🚀 Evolution进化] end L -->|AI 溶解| L1[标准化文本/数据] A -->|AI 判断| A1{价值评估} A1 -->|低价值| A2[Resources] A1 -->|高价值| A3[Tasks/Projects] P -->|AI 生成| P1[可执行资产代码/SOP/清单] E -->|持续优化| E1[系统迭代] E1 -.->|反馈| L style L fill:#64b5f6 style A fill:#81c784 style P fill:#ffb74d style E fill:#e57373 style P1 fill:#ffd54f,stroke:#f57c00,stroke-width:3px L - Liquid (液态化&#x2F;溶解)痛点：传统的笔记在不同软件（微信、网页、PDF）之间有壁垒，且格式固化。 操作：利用 AI（如 GPT&#x2F;Claude）作为”溶解剂”。无论是一段 1 小时的视频、一篇长论文还是一段代码，先丢给 AI。 目的：将所有异构信息瞬间转化为标准化的、可编辑的文本&#x2F;数据（结构化数据），存入统一的 Inbox。你不再手动摘抄，而是只负责”捕获”。 实战 Prompt 示例： 📺 处理视频内容 12345678我刚看了一个关于[主题]的视频，请帮我：1. 提取 3-5 个核心观点2. 每个观点用一句话总结3. 标注哪些观点可以立即应用到我的[当前项目]4. 生成结构化的 Markdown 输出视频文字稿/要点：[粘贴内容或让 AI 从链接提取] 📄 处理学术论文 123456789请将这篇论文液化为可操作的知识：1. 核心假设是什么？2. 关键方法论（用 3 句话）3. 对我的[领域]有什么启发？4. 提取 5 个可以直接引用的金句5. 生成一个 executive summary（100字以内）论文内容：[粘贴 PDF 内容或关键段落] 💬 处理碎片想法 12345678910我有一些零散的想法，请帮我整理成结构化笔记：- [想法1]- [想法2]- [想法3]请：1. 找出这些想法的共同主题2. 按逻辑重组3. 补充可能缺失的环节4. 生成一个可扩展的大纲 A - Action (行动路由)理念：信息不应该按”主题”分类（如”营销资料”），而应按”行动强度”分类。 操作：AI 或你根据内容决定它的去向： 低价值&#x2F;未来用 → 自动沉淀入 Resources（作为潜在养料） 高价值&#x2F;现在用 → 转化为 Tasks（具体任务）或 Projects（项目） 区别：这一步类似于 PARA 的整理，但强调由 AI 辅助判断优先级。 行动路由矩阵： 内容特征 行动强度 去向 AI 辅助动作 与当前项目直接相关 ⚡⚡⚡ 极高 立即行动 生成 Task + Deadline 可以改进现有流程 ⚡⚡ 高 Projects 生成实施方案草稿 长期有用但不紧急 ⚡ 中 Resources 打标签 + 写摘要 有趣但不确定价值 💤 低 暂存&#x2F;丢弃 AI 评估是否值得保留 实战 Prompt 示例： 123456789101112131415我刚刚液化了这些信息，请帮我路由：内容：[粘贴液化后的笔记]当前上下文：- 我正在做的项目：[项目列表]- 我近期的目标：[目标列表]- 我的专业领域：[领域]请分析：1. 这个信息对我的行动价值（1-10分）2. 如果分数 ≥7，生成具体的下一步行动（Next Action）3. 如果分数 &lt;7，建议存入哪个 Resource 分类4. 是否有可以立即应用的点子？ P - Product (产品化&#x2F;资产化)核心差异：这是 E.I.O.S. 与传统笔记法最大的不同。 理念：“不输出等于没学”。阅读不是为了记忆，是为了生产。 操作：在获取信息的当下，立即利用 AI 生成一个”最小可行性产品”（MVP）： 读了编程文章 → 让 AI 写一段可运行的代码片段（Snippet） 看了营销理论 → 让 AI 生成一份针对你公司的 SOP（标准作业程序） 学了沟通技巧 → 让 AI 生成一个话术清单 结果：你得到的不是一篇”笔记”，而是一个能直接用的”工具”或”技能”。 产品化类型矩阵： 信息类型 传统笔记 E.I.O.S. 产品化 技术教程 摘抄步骤 ✅ 可运行的代码模板 + 注释 管理方法 复制理论 ✅ 定制化的 SOP 文档 沟通技巧 记录要点 ✅ 场景化话术脚本 数据分析 截图图表 ✅ 可复用的分析模板（Excel&#x2F;SQL） 营销案例 保存案例 ✅ 改编为自己的营销文案草稿 设计灵感 收藏图片 ✅ 生成 Figma&#x2F;CSS 实现方案 实战 Prompt 示例： 💻 技术学习 → 代码资产 1234567891011121314我刚学习了[技术概念/框架]，请帮我产品化：学习内容：[粘贴笔记或关键点]请生成：1. 一个最小可运行的代码示例（&lt; 50 行）2. 关键部分的逐行注释3. 3 个常见使用场景4. 1 个可以直接复用的函数模板5. 保存为 Snippet 的标题和标签建议编程语言：[Python/JavaScript/etc.]我的技术栈：[列出相关技术] 📊 商业方法 → SOP 文档 123456789101112131415161718我学到了一个[商业方法论]，请帮我产品化为 SOP：方法论内容：[粘贴理论或步骤]我的业务背景：- 行业：[行业]- 团队规模：[人数]- 当前痛点：[描述]请生成：1. 标准操作流程（SOP）文档2. 每个步骤的具体操作清单3. 需要的工具和资源4. 关键指标和检查点5. 常见问题应对方案输出格式：Markdown 表格 + 清单 ✍️ 写作技巧 → 内容模板 12345678910111213我学了一个写作框架，请帮我产品化：框架内容：[描述框架要点]请生成：1. 可填空的写作模板（包含提示问题）2. 针对[我的领域]的 3 个实际案例3. 每个部分的字数建议4. 吸引人的开头公式 × 35. 强有力的结尾公式 × 3我的写作场景：[博客/营销文案/技术文档/等] 🎯 问题解决 → 决策清单 1234567891011我遇到一个[问题类型]，基于这个信息请生成决策框架：问题：[描述问题]学到的知识：[粘贴相关信息]请生成：1. 决策树（用 Mermaid 语法）2. 每个决策点的评估标准3. 风险矩阵（高/中/低）4. 具体的下一步行动清单5. 需要收集的额外信息 关键原则： ⚡ 即时产品化：不要等到”需要的时候”，现在就生成 🎯 场景化定制：不要通用模板，要针对你的具体情况 ♻️ 可复用设计：产品要能在类似场景中反复使用 📦 模块化存储：每个产品都应该是独立的、可组合的单元 E - Evolution (进化)理念：系统和个体必须不断迭代。 双重进化路径： graph LR subgraph \"个体进化\" P1[产品资产库] --> S1[技能提升] S1 --> O1[产出质量↑] O1 --> P1 end subgraph \"系统进化\" P2[使用反馈] --> S2[优化 Prompt] S2 --> O2[效率提升↑] O2 --> P2 end O1 -.->|反哺| S2 O2 -.->|加速| S1 style S1 fill:#81c784 style S2 fill:#64b5f6 操作方法： 个体进化：通过 P 阶段积累的资产，不断提升你的解决问题能力 每周回顾：我产出了哪些可复用的资产？ 每月盘点：哪些资产被反复使用？（这些是你的核心竞争力） 每季度反思：我的技能树是否在扩展？ 系统进化：定期回顾（Review），优化 AI 的 Prompt（提示词）和工作流 记录哪些 Prompt 效果好，建立 Prompt 库 标准化高频工作流（如：技术文章 → 代码 Snippet） 让系统越来越”懂你” 进化检查清单（每周 15 分钟）： 123456789101112131415## 本周进化回顾### 📦 产出盘点- [ ] 本周产品化了几条信息？- [ ] 哪些产品已经被使用？效果如何？- [ ] 哪些产品可以合并/优化？### 🔧 系统优化- [ ] 哪个 Prompt 特别好用？（收录到 Prompt 库）- [ ] 哪个环节很慢？（如何自动化？）- [ ] 发现了什么新工具/新方法？### 🚀 下周计划- [ ] 重点产品化哪 3 个领域的信息？- [ ] 需要优化哪个工作流？ 实战 Prompt 示例： 12345678910111213141516171819我积累了这些产品化资产，请帮我进化：已有资产清单：1. [资产1：类型、使用频率]2. [资产2：类型、使用频率]3. [资产3：类型、使用频率]...请分析：1. 这些资产反映了我的什么能力矩阵？2. 哪些是高价值资产（反复使用）？3. 哪些资产可以组合成更强大的工具？4. 我的知识体系有哪些空白点？5. 下一步应该重点发展哪个方向？输出：- 技能地图（Mermaid mindmap）- 资产组合建议- 下周学习计划 实际应用场景场景 1：技术人员学习新框架传统方式： 看视频教程 → 记笔记 保存到”编程学习”文件夹 三个月后需要用，完全忘记了 重新学一遍 ❌ E.I.O.S. 方式： Liquid：看完教程后，让 AI 提取核心概念和语法要点 Action：判断是当前项目需要（高优先级）还是未来备用（Resources） Product：让 AI 立即生成： 最小可运行代码示例 常用操作的 Snippet 库 快速查询手册（Cheat Sheet） Evolution：在实际项目中使用，不断优化 Snippet 时间对比： 传统方式：学习 2 小时 + 遗忘 + 重学 2 小时 &#x3D; 4 小时 E.I.O.S.：学习 2 小时 + AI 产品化 15 分钟 &#x3D; 2.25 小时（且有可复用资产） 场景 2：产品经理整理竞品分析传统方式： 截图保存竞品功能 写几段文字描述 存入”竞品分析”文件夹 需要写 PRD 时，找不到当时的关键信息 ❌ E.I.O.S. 方式： Liquid：收集竞品信息后，让 AI 结构化提取： 核心功能列表 UI&#x2F;UX 亮点 用户评价关键词 Action：判断哪些功能可以立即借鉴（Projects），哪些是长期观察（Resources） Product：让 AI 生成： 功能对比矩阵表格 PRD 草稿（针对可借鉴功能） 用户故事（User Story）清单 Evolution：每次竞品分析都积累到对比矩阵中，形成动态的竞品地图 价值提升： 传统方式：信息分散，难以对比 E.I.O.S.：结构化资产库，可快速生成竞品报告 ✅ 场景 3：自由职业者积累业务知识传统方式： 每次接项目都要重新研究 过往经验散落在聊天记录、邮件、文档中 无法形成可复用的知识资产 ❌ E.I.O.S. 方式： Liquid：项目结束后，让 AI 提取： 客户的典型需求 有效的解决方案 遇到的坑和应对方法 Action：分类到对应的业务领域（如”品牌设计”、”网站开发”） Product：让 AI 生成： 标准化报价模板 项目交付 SOP 客户沟通话术库 问题解决方案库 Evolution：每个新项目都丰富这些资产，接单效率越来越高 收益： 传统方式：每次重新摸索 E.I.O.S.：积累可复用的业务操作系统，接单速度 ×3 ✅ 快速上手：3 步启动 E.I.O.S.第 1 步：准备环境（15 分钟） 注册一个 AI 账号（Claude &#x2F; ChatGPT） 在你的笔记工具中创建结构：12345678📥 Inbox（收件箱）📦 Products（产品资产库） ├── 代码 Snippets ├── SOP 文档 ├── 模板库 └── 决策框架📚 Resources（参考资料）🗄️ Archives（归档） 第 2 步：建立你的第一个产品（30 分钟） 选择一篇你最近读过的对工作有用的文章 使用本文的 Liquid Prompt 让 AI 液化内容 使用 Product Prompt 让 AI 生成可执行资产 保存到 Products 对应分类 立即在工作中尝试使用这个产品 第 3 步：建立每日习惯（持续）每天花 15 分钟： 早上：回顾 Inbox，选择 1-2 条信息进行 L.A.P.E. 处理 晚上：记录今天产品化了什么，使用效果如何 关键：不要贪多，从每天处理 1 条信息 开始，养成习惯比数量重要。 常见陷阱与应对❌ 陷阱 1：AI 依赖症表现：什么都让 AI 做，自己不思考。应对：AI 是”放大器”不是”替代品”。你需要提供方向、判断质量、融合经验。 ❌ 陷阱 2：产品化过度表现：为了产品化而产品化，生成一堆用不到的资产。应对：只产品化当前或近期会用的信息。记住：少即是多。 ❌ 陷阱 3：工具焦虑表现：纠结用 Notion 还是 Obsidian，花大量时间折腾工具。应对：E.I.O.S. 的核心是工作流，不是工具。先用最熟悉的工具开始。 ❌ 陷阱 4：Prompt 完美主义表现：花 1 小时优化 Prompt，试图让 AI 一次输出完美结果。应对：接受”够用就行”。Prompt 工程本身也是进化的，不要陷入过度优化。 ❌ 陷阱 5：忽视 Evolution表现：不断产出新资产，但从不回顾和优化。应对：每周 15 分钟回顾。系统不进化，熵增依然会发生。 E.I.O.S. 与 第二大脑 (BASB) 的对比 维度 第二大脑 (BASB) E.I.O.S. 系统 核心工具 笔记软件 (Notion&#x2F;Obsidian) AI Agent (Claude&#x2F;GPT) + 笔记软件 主要动作 复制、粘贴、高亮、手动总结 Prompt 提问、AI 转化、生成代码&#x2F;SOP 分类逻辑 PARA (项目&#x2F;领域&#x2F;资源&#x2F;归档) LAPE (液化&#x2F;行动&#x2F;产出&#x2F;进化) 对人的要求 需要极强的整理习惯和自律 需要极强的 AI 驾驭能力 (Prompt Engineering) 最终产出 井井有条的知识库 可执行的行动方案、SOP、代码块 核心差异总览graph TB subgraph \"第二大脑 BASB\" B1[信息输入] --> B2[手动整理PARA分类] B2 --> B3[渐进式总结加粗/高亮] B3 --> B4[存入知识库] B4 --> B5[需要时查询] B5 -.->|可能忘记| B4 end subgraph \"E.I.O.S. 系统\" E1[信息输入] --> E2[AI 液化自动结构化] E2 --> E3[AI 路由判断价值] E3 --> E4[AI 产品化生成资产] E4 --> E5[立即使用] E5 --> E6[系统进化] E6 -.->|反馈优化| E2 end B5 -->|效率| B_Time[查找成本高] E5 -->|效率| E_Time[即取即用] style B_Time fill:#ff6b6b style E_Time fill:#4ecdc4 style E4 fill:#ffd54f,stroke:#f57c00,stroke-width:3px 何时选择 E.I.O.S.？适合使用 E.I.O.S. 的人： ✅ 你的工作需要快速产出而非深度积累 ✅ 你愿意学习 AI Prompt 工程 ✅ 你重视行动和结果胜过完美的笔记 ✅ 你觉得传统知识管理维护成本太高 ✅ 你希望知识能立即转化为生产力 更适合传统第二大脑的人： 📌 你需要深度思考和联想（学术研究、写书） 📌 你享受手动整理的过程 📌 你重视知识网络的构建 📌 你对 AI 不熟悉或不信任 最佳实践：混合使用 用 E.I.O.S. 处理工作相关、需要快速产出的信息 用 第二大脑 管理个人成长、长期思考的内容 总结：从”知识管理”到”知识生产”如果你觉得传统的”第二大脑”维护起来太累（需要手动整理太多东西），或者你觉得记了笔记却很少用到，E.I.O.S. 就是为你准备的进阶版本。 本质差异： 第二大脑：教你如何成为更好的”图书管理员“（整理、保存、检索） E.I.O.S.：教你如何成为高效的”知识工厂主“（液化、路由、生产、进化） 它本质上是利用 AI 极大地压缩了”CODE”模型中 O (Organize) 和 D (Distill) 的时间，强迫你把精力全部集中在 P (Product) 和 E (Evolution) 上。 关键原则回顾 🌊 信息要”液化”：不要被格式困住，让 AI 统一处理 🎯 分类看”行动”：不按主题，按能否立即行动 📦 知识要”产品化”：输出可执行资产，而非静态笔记 🚀 系统要”进化”：不断优化 Prompt 和工作流 行动建议今天就开始： 打开你的 AI 工具（ChatGPT &#x2F; Claude） 找一篇你最近读的有价值文章 复制本文的 Liquid Prompt，让 AI 帮你液化 复制 Product Prompt，让 AI 生成可执行资产 立即在工作中使用这个资产 记住 E.I.O.S. 的核心理念： “知识的价值不在于你拥有多少，而在于你能用它生产什么。” 不要囤积信息，去创造资产。🚀","categories":[{"name":"方法论","slug":"方法论","permalink":"https://www.silenceboy.com/categories/%E6%96%B9%E6%B3%95%E8%AE%BA/"}],"tags":[{"name":"方法论","slug":"方法论","permalink":"https://www.silenceboy.com/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"}]},{"title":"第二大脑知识管理方法论","slug":"第二大脑知识管理方法论","date":"2026-01-14T07:15:32.000Z","updated":"2026-01-14T07:30:15.500Z","comments":true,"path":"2026/01/14/第二大脑知识管理方法论/","permalink":"https://www.silenceboy.com/2026/01/14/%E7%AC%AC%E4%BA%8C%E5%A4%A7%E8%84%91%E7%9F%A5%E8%AF%86%E7%AE%A1%E7%90%86%E6%96%B9%E6%B3%95%E8%AE%BA/","excerpt":"","text":"“第二大脑”（Building a Second Brain, 简称 BASB） 是近年来在个人知识管理（PKM）领域最流行、最系统化的一套方法论。 它由美国的生产力专家 Tiago Forte 提出，其核心理念可以概括为一句话：“你的大脑是用来产生想法的，而不是用来保存想法的。” 该方法论主要由 核心哲学、CODE 流程 和 PARA 系统 三部分组成。 核心哲学：为什么要建立第二大脑？生物大脑擅长联想、创造和直觉判断，但非常不擅长记忆大量细节。第二大脑就是利用数字工具（笔记软件），构建一个外部的存储和检索系统。 生物大脑（第一大脑）：负责 CPU 的工作（思考、决策、创意）。 数字大脑（第二大脑）：负责硬盘的工作（记忆、保存、索引）。 目标：将记忆外包给计算机，从而减轻焦虑，专注于创造性产出。 核心组织架构：PARA 系统这是第二大脑的“静态”部分，即如何分类和存放文件。Tiago Forte 认为大多数人整理笔记之所以失败，是因为按“主题”分类（如图书馆），而实际上我们应该按 “可行动性”（Actionability）分类。 PARA 将所有信息分为四类，优先级从高到低： P - Projects（项目） 定义：有明确的截止日期和具体目标的任务。 例子：写年度报告、策划一次旅行、装修书房。 特点：这是你当前最关注、最活跃的信息，应该最容易访问。 A - Areas（领域） 定义：没有截止日期，但需要长期维护的责任范围。 例子：健康、财务、职业发展、家庭、汽车保养。 特点：只要你还活着或还在工作，这些领域就一直存在。 R - Resources（资源） 定义：你感兴趣的主题或未来可能用到的素材库。 例子：咖啡制作、网页设计灵感、心理学笔记、食谱。 特点：这是你的知识储备库。 A - Archives（归档） 定义：已完成的项目、不再维护的领域、不再感兴趣的资源。 例子：2025年的项目文件、前任工作的资料。 特点：不要删除，而是存入冷库，以备不时之需，保持前三个文件夹的清爽。 核心原则：信息是流动的。一个”项目”完成后，归档到”归档”；一个”资源”如果你决定对其采取行动，它就变成了”项目”。 PARA 系统可视化graph TD A[新信息] --> B{可行动性判断} B -->|当前活跃有截止日期| P[Projects 项目] B -->|长期维护无截止日期| AR[Areas 领域] B -->|未来可能用感兴趣| R[Resources 资源] P -->|完成/放弃| ARC[Archives 归档] AR -->|不再维护| ARC R -->|不再感兴趣| ARC R -->|决定行动| P style P fill:#ff6b6b style AR fill:#4ecdc4 style R fill:#45b7d1 style ARC fill:#95a5a6 classDef info fill:#f9f9f9,stroke:#333,stroke-width:2px class A info 信息流动示例： 你在阅读时发现一篇关于时间管理的好文章 → 存入 Resources（时间管理） 你决定改进自己的时间管理习惯 → 创建 Project（建立时间管理系统） 项目持续三个月完成 → 归入 Archives（2026年项目） 但时间管理成为你的日常责任 → 在 Areas（个人效能）中持续维护 核心运作流程：CODE 模型这是第二大脑的“动态”部分，即如何处理知识的生命周期。 C - Capture（获取） 不要试图记录所有信息，只记录产生共鸣的信息。 不要在获取时整理，先扔进“收件箱”（Inbox），避免打断心流。 O - Organize（组织） 将收件箱的信息分发到 PARA 系统中。 关键问题：不要问“这属于哪一类？”，而要问**“我在哪个项目中会用到它？”** 以项目为导向进行组织，确保知识能转化为行动。 D - Distill（提炼） 这是 BASB 最独特的地方。不要只存原文，要进行渐进式总结（Progressive Summarization）。 Layer 1：保存原文。 Layer 2：加粗关键句子。 Layer 3：高亮核心观点（在加粗中选优）。 Layer 4：用自己的话写一段”执行摘要”（Executive Summary）在笔记之首。 目的：未来的你看到这篇笔记时，能在几秒钟内抓住重点，而不需要重读全文。 E - Express（表达） 知识管理的最终目的不是”拥有知识”，而是”应用知识”。 通过写作、演示、解决问题，将”中间产物”（Intermediate Packets）组合起来，形成产出。 CODE 循环流程graph LR C[Capture获取] --> O[Organize组织] O --> D[Distill提炼] D --> E[Express表达] E -.->|产生新想法| C C --> C1[收件箱] O --> O1[PARA分类] D --> D1[渐进式总结] E --> E1[创作产出] style C fill:#ffd93d style O fill:#6bcf7f style D fill:#4d96ff style E fill:#ff6b9d 渐进式总结（Progressive Summarization）详解这是第二大脑最具创新性的技术，通过多次阅读逐层提炼： graph TD L1[Layer 1: 原文] --> L2[Layer 2: 加粗关键句] L2 --> L3[Layer 3: 高亮核心观点] L3 --> L4[Layer 4: 执行摘要] L1 -.->|第一次阅读保存原文| Save1[完整保存] L2 -.->|第二次需要时快速扫描| Bold[粗体标记重点] L3 -.->|第三次使用深入理解| High[黄色高亮精华] L4 -.->|准备输出自己的话| Sum[顶部写摘要] style L1 fill:#e8e8e8 style L2 fill:#c8e6c9 style L3 fill:#fff59d style L4 fill:#ff6b6b,color:#fff 实例说明：假设你保存了一篇 10 页的关于”深度工作”的文章： Layer 1：保存全文（稍后阅读） Layer 2（某天需要写文章时）：加粗 15 个关键句子（耗时 5 分钟） Layer 3（准备演讲时）：在加粗中高亮 5 个核心观点（耗时 2 分钟） Layer 4（写演讲稿时）：在笔记顶部写：”深度工作的核心是消除分心，通过时间块和环境设计来保护专注力。”（耗时 1 分钟） 下次再看这篇笔记，你只需读顶部的摘要，就能决定是否需要深入。 关键技术概念在实践第二大脑时，有两个非常实用的微观技巧： 中间产物 (Intermediate Packets) 不要试图一口气完成一个大项目。 把工作拆解成小的模块（如：一个清单、一段草稿、一张图表）。 这些模块既服务于当前项目，也可以被未来的项目复用。 海明威桥 (Hemingway Bridge) 结束一天工作时，不要彻底写完，留一点显而易见的开头给第二天。 在笔记中写下”下一步该做什么”或”当前的思路”，这样第二天可以迅速进入状态，减少启动摩擦。 命名由来：海明威写小说时，会在写到最兴奋的地方突然停笔，第二天能轻松续写。 案例：你正在写一份项目报告，下午 5 点时： ❌ 错误做法：拼命写完”结论”部分才下班，第二天面对空白页不知从何开始。 ✅ 正确做法：在”结论”部分写下：”明天先总结三个关键发现：1）用户留存率提升 25%；2）…”，第二天打开文件就能立即进入状态。 推荐工具与选择根据不同需求，以下是主流的第二大脑工具： 工具 优势 适合人群 PARA支持 Notion 灵活强大，数据库+页面，团队协作好 项目经理、团队协作 ⭐⭐⭐⭐⭐ Obsidian 本地存储，双向链接，Markdown，插件丰富 隐私敏感、技术用户 ⭐⭐⭐⭐ Logseq 大纲式+双链，开源，本地优先 喜欢大纲思维的用户 ⭐⭐⭐⭐ Evernote 稳定成熟，全平台，强大的剪藏功能 传统笔记用户 ⭐⭐⭐ Apple Notes 简单轻便，苹果生态无缝 轻量级用户、苹果用户 ⭐⭐ 选择建议： 如果追求灵活性和美观：Notion 如果重视数据安全和可控性：Obsidian 如果喜欢网状思维和知识联想：Obsidian 或 Logseq 如果想要简单上手快速见效：Notion 或 Evernote 如何开始实践第二大脑很多人失败的原因是一开始就想建立完美系统。正确的做法是从小处开始，逐步迭代。 5 步入门指南第 1 周：搭建基础结构 在你选择的工具中创建四个文件夹：Projects、Areas、Resources、Archives 添加一个 Inbox（收件箱）文件夹，作为临时存放区 第 2-3 周：开始捕获 每当你遇到有价值的信息（文章、想法、图片）时，快速存入 Inbox 不要在捕获时整理，保持快速记录 第 4 周：学习组织 每周抽出 30 分钟，将 Inbox 中的内容分发到 PARA 四个文件夹 关键问题：“我会在哪个项目中用到它？” 第 5-8 周：开始提炼 当你再次打开一篇笔记时，花 2 分钟加粗关键句子（Layer 2） 不要一次性处理所有笔记，只处理你需要用到的 第 9 周及以后：产生输出 尝试从你的笔记库中提取素材，完成一个小项目（写一篇文章、做一次分享） 体会”中间产物”的价值，感受知识复用的快感 关键原则 ⚡ 快速捕获 优于完美整理 🎯 以项目为导向 而非以分类为导向 🔄 渐进式完善 而非一次性做到完美 📤 创造输出 而非仅仅积累 常见误区与建议误区 1：试图记录一切问题：把第二大脑当成档案馆，什么都往里存。建议：只保存产生共鸣（resonate）的内容，或与当前项目直接相关的素材。 误区 2：过度整理而不产出问题：花大量时间美化笔记、调整分类，但从不使用。建议：记住 Tiago Forte 的口号：”笔记的价值在于它帮你完成的工作，而非笔记本身。“ 误区 3：PARA 分类强迫症问题：”这个笔记到底属于 Areas 还是 Resources？”纠结半天。建议：如果不确定，默认放 Resources。PARA 是流动的，随时可以移动。 误区 4：从不归档问题：Projects 文件夹越来越臃肿，几年前的项目还在里面。建议：每月回顾一次，将完成的项目移到 Archives。保持 Projects 轻盈才能聚焦当下。 误区 5：工具选择焦虑问题：反复在 Notion、Obsidian、Roam 之间切换，陷入”工具陷阱”。建议：选一个工具用 3 个月再评估。方法论比工具重要。 第二大脑整体架构最后，让我们用一张图总览第二大脑的完整系统： graph TB subgraph \"信息输入\" A1[阅读] --> Inbox A2[想法] --> Inbox A3[经验] --> Inbox end subgraph \"CODE 流程\" Inbox -->|Capture| C[获取到收件箱] C -->|Organize| O[组织到PARA] O -->|Distill| D[渐进式提炼] D -->|Express| E[创作输出] end subgraph \"PARA 系统\" O --> P[Projects项目] O --> AR[Areas领域] O --> R[Resources资源] P --> ARC[Archives归档] AR --> ARC R --> ARC end subgraph \"价值产出\" E --> OUT1[文章/报告] E --> OUT2[演示/课程] E --> OUT3[决策/方案] end OUT1 -.->|产生新想法| A2 OUT2 -.->|产生新想法| A2 OUT3 -.->|产生新想法| A2 style Inbox fill:#ffd93d style P fill:#ff6b6b style AR fill:#4ecdc4 style R fill:#45b7d1 style ARC fill:#95a5a6 style E fill:#ff6b9d “第二大脑” vs “卡片盒笔记法”很多用户容易混淆这两者，以下是详细对比： 维度 卡片盒笔记法 (Zettelkasten) 第二大脑 (BASB) 核心理念 通过链接发现新知识 以项目为导向快速行动 思维方式 自下而上（从细节到整体） 自上而下（从目标到细节） 组织结构 网状结构，强调卡片间连接 文件夹结构（PARA），强调分类 适用场景 学术研究、写书、深度思考 项目管理、职场工作、快速产出 核心技术 双向链接、永久笔记、索引笔记 PARA分类、渐进式总结、中间产物 时间投入 前期投入高，长期收益 立即可用，快速见效 工具推荐 Obsidian、Logseq、Roam Research Notion、Evernote、Obsidian 理想用户 研究者、作家、知识工作者 项目经理、创作者、职场人士 能否结合？完全可以！很多高级用户采用混合策略： 用 PARA 管理项目和待办（行动层） 用 Zettelkasten 建立知识网络（思考层） 在 Obsidian 中同时实现两者 延伸阅读与资源官方资源： 📘 《Building a Second Brain》— Tiago Forte 著（有中文版《打造第二大脑》） 🌐 Forte Labs 官方博客 — 大量免费文章和案例 🎥 Building a Second Brain 官方课程 相关方法论： 📌 GTD（Getting Things Done） — 任务管理，与 PARA 互补 📌 Zettelkasten（卡片盒笔记法） — 知识联想，与 CODE 互补 📌 PARA + GTD + Zettelkasten — 三者结合的混合方案 社区与讨论： Reddit: r&#x2F;PKMS、r&#x2F;Notion、r&#x2F;ObsidianMD Discord: Building a Second Brain 官方社区 结语第二大脑不是目的，而是手段。 它的价值不在于你积累了多少笔记，而在于： ✅ 你因此完成了多少项目 ✅ 你因此减少了多少焦虑 ✅ 你因此释放了多少创造力 记住 Tiago Forte 的核心理念： “你的大脑是用来产生想法的，而不是用来保存想法的。”","categories":[{"name":"方法论","slug":"方法论","permalink":"https://www.silenceboy.com/categories/%E6%96%B9%E6%B3%95%E8%AE%BA/"}],"tags":[{"name":"方法论","slug":"方法论","permalink":"https://www.silenceboy.com/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"}]},{"title":"深度解析自我改进智能体（Self-Improving Agents）与 Reflexion 架构","slug":"深度解析自我改进智能体（Self-Improving-Agents）与-Reflexion-架构","date":"2025-12-11T07:23:42.000Z","updated":"2025-12-11T07:54:55.690Z","comments":true,"path":"2025/12/11/深度解析自我改进智能体（Self-Improving-Agents）与-Reflexion-架构/","permalink":"https://www.silenceboy.com/2025/12/11/%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%E8%87%AA%E6%88%91%E6%94%B9%E8%BF%9B%E6%99%BA%E8%83%BD%E4%BD%93%EF%BC%88Self-Improving-Agents%EF%BC%89%E4%B8%8E-Reflexion-%E6%9E%B6%E6%9E%84/","excerpt":"","text":"传统的 LLM 就像一个答题速度极快但从不检查的学生——它拿到问题，凭直觉写下答案，然后立即交卷。如果错了，它就错了。 但如果这个学生学会了自我反思呢？如果它写完答案后，自己检查一遍，发现错误并修正，然后再交卷呢？ 这就是自我改进智能体（Self-Improving Agents） 的核心理念。今天，我们将深入探讨这一领域，特别是让 Agent 拥有“自省”能力的 Reflection 机制与 Reflexion 框架。 什么是自我改进智能体？自我改进智能体是指那些不仅能执行任务，还能评估自身表现、并利用反馈来优化后续行动的 AI 系统。 与传统的一次性问答（One-shot QA）不同，自我改进智能体引入了一个闭环： 执行（Action）： 尝试完成任务。 评估（Evaluation）： 检查结果是对是错。 反思（Reflection）： 分析为什么错了，如何改进。 更新（Update&#x2F;Refine）： 利用反思的结果，重新尝试或在下一次任务中做得更好。 这种机制让 AI 从”概率生成器”进化为了具备慢思考能力的推理者。 传统 LLM vs 自我改进智能体让我们通过流程图直观对比两者的差异： graph LR A[传统 LLM] --> B[接收问题] B --> C[生成答案] C --> D[输出结果] D --> E[结束] style A fill:#ffcccc style E fill:#ffcccc graph LR A[自我改进智能体] --> B[接收问题] B --> C[生成答案] C --> D[自我评估] D --> E{结果满意?} E -->|否| F[反思分析] F --> G[优化策略] G --> C E -->|是| H[输出结果] style A fill:#ccffcc style H fill:#ccffcc style F fill:#ffffcc 传统 LLM 是一个单向流水线，而自我改进智能体是一个闭环系统。 自我改进的核心循环graph TB A[执行 Action尝试完成任务] --> B[评估 Evaluation检查结果质量] B --> C[反思 Reflection分析错误原因] C --> D[更新 Update优化策略/记忆] D --> A style A fill:#e1f5ff style B fill:#fff9e1 style C fill:#ffe1f5 style D fill:#e1ffe1 这个循环的关键在于：每次迭代都会累积经验，智能体不会在同一个地方跌倒两次。 Reflection（自我反思）在深入复杂的框架之前，我们需要理解最基础的原子能力：Reflection（反思）。 简单来说，Reflection 就是通过 Prompt Engineering（提示工程），让大模型扮演“批评家”的角色来审视自己生成的“演员”剧本。 它是如何工作的？sequenceDiagram participant User as 用户 participant LLM as 大语言模型 User->>LLM: 生成Prompt: 请写代码解决问题X LLM->>LLM: 生成初始答案 LLM-->>User: 返回答案 v1 User->>LLM: 反思Prompt: 检查上述代码是否有Bug LLM->>LLM: 以\"批评家\"角色审视 LLM-->>User: 指出问题和改进建议 User->>LLM: 修正Prompt: 根据建议修改代码 LLM->>LLM: 生成优化答案 LLM-->>User: 返回答案 v2 (改进版) 通常包含两个步骤的 Prompt： 第一步 - 生成： 1请帮我写一段 Python 代码，实现二分查找算法。 第二步 - 反思： 12345请检查上面生成的代码：1. 是否存在边界条件处理的Bug？2. 逻辑是否完全正确？3. 是否有性能优化空间？4. 如果有错误，请指出并给出修正建议。 实际案例初始生成的代码（可能有问题）： 1234567891011def binary_search(arr, target): left, right = 0, len(arr) # Bug: 应该是 len(arr) - 1 while left &lt; right: mid = (left + right) // 2 if arr[mid] == target: return mid elif arr[mid] &lt; target: left = mid + 1 else: right = mid return -1 Reflection 发现的问题： “right 的初始值应该是 len(arr) - 1 而不是 len(arr)，否则会导致数组越界。同时，循环条件应该是 left &lt;= right，当前写法会遗漏最后一个元素的检查。” 修正后的代码： 1234567891011def binary_search(arr, target): left, right = 0, len(arr) - 1 # 修正 while left &lt;= right: # 修正 mid = (left + right) // 2 if arr[mid] == target: return mid elif arr[mid] &lt; target: left = mid + 1 else: right = mid - 1 # 修正 return -1 为什么它有效？研究发现，LLM 生成正确答案的概率，往往低于“判断答案是否正确”的概率。模型通常具备“识别自己错误”的能力，即使它第一次没能做对。 通过 Reflection，我们强迫模型跳出单纯的“续写文本”模式，进入“逻辑校验”模式。这不仅能减少幻觉，还能显著提升代码生成和复杂推理的准确率。 进阶架构：Reflexion 框架如果说 Reflection 是一种技巧，那么 Reflexion 就是一套完整的、系统化的强化学习（Reinforcement Learning） 替代方案。 Reflexion 是 Shinn 等人在 2023 年提出的一个重要框架（论文：Reflexion: Language Agents with Verbal Reinforcement Learning）。它的核心突破在于：它不需要更新模型的权重（参数），而是通过语言反馈来更新“记忆”。 Reflexion 的三驾马车graph TB subgraph Reflexion框架 Actor[Actor 行动者负责执行任务生成输出] Evaluator[Evaluator 评估者判断输出质量打分] SelfReflection[Self-Reflection 自我反思分析失败原因生成反馈] Memory[(Memory 记忆系统存储反思经验)] end Task[任务输入] --> Actor Actor --> |输出结果| Evaluator Evaluator --> |失败| SelfReflection Evaluator --> |成功| Success[任务完成] SelfReflection --> |语言反馈| Memory Memory --> |历史经验| Actor SelfReflection -.重新尝试.-> Actor style Actor fill:#e3f2fd style Evaluator fill:#fff3e0 style SelfReflection fill:#f3e5f5 style Memory fill:#e8f5e9 style Success fill:#c8e6c9 Reflexion 框架由三个核心模块组成： Actor（行动者）这是干活的模型（比如 GPT-4）。它负责生成文本、代码或执行搜索动作。 输入： 任务描述 + 历史反思记忆 输出： 具体的执行结果（代码、答案、行动序列等） 特点： 可以是任何大型语言模型，无需特殊训练 Evaluator（评估者）这是打分的老师。它负责评估 Actor 的输出质量。 在代码任务中： 评估者可以是单元测试（Unit Tests）或代码执行器 在推理任务中： 它可以是另一个 LLM，用来判断答案是否准确 在游戏任务中： 它可以是游戏引擎返回的成功&#x2F;失败信号 输出： 二元信号（成功&#x2F;失败）或连续分数（0-1） Self-Reflection（自我反思模型）这是 Reflexion 的灵魂。当 Evaluator 判定任务失败时，Self-Reflection 模型会介入。 它不会只给一个冷冰冰的”0分”，而是会生成一段人类可读的语言反馈（Verbal Feedback）。 实际例子： 1234我上次失败是因为没有考虑到边界条件 x=0 的情况，导致程序除零错误。下次我需要在函数开头添加一个 if x == 0: return 0 的语句来处理这个特殊情况。同时，对于所有的除法操作，都应该检查分母是否为零。 反思内容通常包括： 错误识别： 什么地方出错了？ 原因分析： 为什么会出错？ 改进策略： 下次应该怎么做？ 通用经验： 这个错误能推广到什么场景？ 工作流程：从试错中学习flowchart TD Start([开始任务]) --> Trial1[Trial 1: Actor首次尝试] Trial1 --> Eval1{Evaluator评估结果} Eval1 -->|成功| Success([任务完成]) Eval1 -->|失败| Reflect1[Self-Reflection:生成反思1] Reflect1 --> Store1[存入Memory:经验1] Store1 --> Trial2[Trial 2: Actor带着经验1重试] Trial2 --> Eval2{Evaluator评估结果} Eval2 -->|成功| Success Eval2 -->|失败| Reflect2[Self-Reflection:生成反思2] Reflect2 --> Store2[存入Memory:经验1+2] Store2 --> Trial3[Trial 3: Actor带着经验1+2重试] Trial3 --> Eval3{Evaluator评估结果} Eval3 -->|成功| Success Eval3 -->|失败| TrialN[Trial N: 继续迭代...] style Trial1 fill:#e3f2fd style Trial2 fill:#e3f2fd style Trial3 fill:#e3f2fd style Reflect1 fill:#f3e5f5 style Reflect2 fill:#f3e5f5 style Store1 fill:#e8f5e9 style Store2 fill:#e8f5e9 style Success fill:#c8e6c9 Reflexion 的运行流程是一个迭代的循环： Trial（尝试）： Actor 尝试解决问题。 Error（报错）： Evaluator 发现结果不对（例如：代码报错，答案错误）。 Reflect（反思）： Self-Reflection 模型分析错误，生成一段文本摘要，解释”为什么错了”以及”该怎么改”。 Memory（记忆）： 这段反思被存入短期记忆（Context）。 Next Trial（再尝试）： Actor 再次尝试。关键点在于： 这一次，Actor 的 Prompt 里包含了之前的”反思内容”。它不仅仅是重试，而是带着经验重试。 Memory 机制详解Memory（记忆系统）是 Reflexion 区别于简单重试的核心。它维护着智能体的”经验数据库”。 graph TB subgraph Memory系统 direction TB ShortTerm[短期记忆 Short-term Memory当前任务的反思历史存储在Context中] LongTerm[长期记忆 Long-term Memory跨任务的经验积累存储在向量数据库] SlidingWindow[滑动窗口机制限制Context长度保留最相关的经验] end CurrentTask[当前任务] --> ShortTerm ShortTerm --> SlidingWindow SlidingWindow --> ActorPrompt[Actor Prompt构建] ShortTerm -.经验总结.-> LongTerm LongTerm -.相似任务检索.-> ActorPrompt style ShortTerm fill:#fff9e1 style LongTerm fill:#e8f5e9 style SlidingWindow fill:#e1f5ff 短期记忆（Episodic Memory） 作用域： 单个任务内 内容： 当前任务所有失败尝试的反思 格式： 直接拼接在 Prompt 中 示例：123456Previous Attempts:Attempt 1: Failed - 原因是没有处理空列表情况Reflection 1: 需要在函数开头添加 if not arr: return -1Attempt 2: Failed - 原因是循环边界条件错误Reflection 2: right应该初始化为len(arr)-1而不是len(arr) 长期记忆（Long-term Memory） 作用域： 跨任务 内容： 通用的经验和模式 存储： 向量数据库（如 FAISS、Pinecone） 检索： 通过语义相似度匹配 示例： “处理列表问题时，始终要考虑空列表、单元素、重复元素等边界情况” 滑动窗口（Sliding Window）由于 LLM 的 Context 长度有限（如 GPT-4 的 8k tokens），Reflexion 使用滑动窗口机制： 保留最近的 N 次反思 丢弃过早的、可能不相关的经验 确保最关键的经验始终在 Context 中 为什么 Reflexion 比单纯的 Reflection 更强？你可能会问：“这不就是多问几遍吗？” 不完全是。Reflexion 的精髓在于长期记忆与语言强化的结合。 语言即奖励（Language as Reward）：传统的强化学习（RL）使用标量奖励（比如 +1, -1）来调整参数，这非常低效且难以解释。Reflexion 使用语言作为反馈信号。这种反馈包含的信息量极大，能精准指导模型“哪里”出了问题。 跨步记忆（Episodic Memory）：Reflexion 允许智能体在解决一个长任务的过程中，积累多个步骤的反思。它维护了一个“反思缓冲区”，确保智能体不会在同一个坑里跌倒两次。 无需微调（Training-free）：它不需要昂贵的 GPU 资源去重新训练模型。只要有一个足够强的基座模型（Base Model），通过架构设计就能实现性能的飞跃。 实际应用场景Reflexion 框架已经在多个领域展现出强大的能力，以下是几个典型应用： 代码生成与调试（HumanEval）任务： 根据函数描述生成正确的 Python 代码。 传统方法的问题： 一次生成经常有 Bug（语法错误、逻辑错误、边界条件遗漏） 即使重新生成，也可能犯同样的错误 Reflexion 的优势： sequenceDiagram participant T as 测试用例 participant A as Actor participant E as Evaluator participant R as Self-Reflection A->>A: 生成代码 v1 A->>E: 提交代码 E->>T: 运行测试 T-->>E: 3/10 通过 E->>R: 失败，7个用例报错 R->>R: 分析错误日志 R-->>A: 反思：没有处理负数输入 A->>A: 生成代码 v2（加入负数处理） A->>E: 提交代码 E->>T: 运行测试 T-->>E: 9/10 通过 E->>R: 失败，1个边界用例报错 R->>R: 分析错误日志 R-->>A: 反思：空列表需要特殊处理 A->>A: 生成代码 v3（加入空列表检查） A->>E: 提交代码 E->>T: 运行测试 T-->>E: 10/10 通过 ✓ 实验结果： 在 HumanEval 基准测试中，Reflexion 使 GPT-4 的通过率从 68% 提升到 91%。 复杂问答推理（HotPotQA）任务： 回答需要多步推理的复杂问题。 示例问题： “哪位导演的电影获得了奥斯卡最佳影片，同时他的配偶也曾获得奥斯卡最佳女演员？” Reflexion 如何工作： 首次尝试： “斯皮尔伯格”（错误） 反思： “我忽略了’配偶也获奖’这个条件，需要检索导演配偶信息” 二次尝试： 检索配偶信息 → “詹姆斯·卡梅隆”（错误） 反思： “卡梅隆的配偶没有获奖，需要同时满足两个条件” 三次尝试： 交叉检索 → “乔尔·科恩”（正确，配偶Frances McDormand获奥斯卡） 实验结果： 在 HotPotQA 上，Reflexion 使 GPT-3.5 的准确率从 54% 提升到 76%。 决策任务（AlfWorld）任务： 在虚拟环境中完成家务任务（如”把杯子放到冰箱里”）。 挑战： 需要多步探索和规划。 Reflexion 的优势： 记住失败的路径： “上次在卧室找杯子失败了，这次应该去厨房” 优化行动序列： “直接拿杯子再去冰箱，而不是先开冰箱” 实验结果： 成功率从 38% 提升到 83%。 数学问题求解（GSM8K）任务： 小学数学应用题。 Reflexion 应用： 发现计算错误：反思检查每一步的算术 发现理解错误：反思题意是否理解正确 发现逻辑错误：反思解题思路是否合理 示例： 初始答案： 15个苹果（错误） 反思： “我遗漏了题目中’给了朋友3个’这个条件” 修正答案： 12个苹果（正确） 生产环境实际应用 智能客服： 通过反思改进回答质量，减少”答非所问” 代码审查助手： 自动发现和修正代码中的潜在问题 自动化测试生成： 根据失败的测试案例，生成更全面的测试 智能写作助手： 自我检查文章的逻辑性、连贯性和准确性 实验效果对比以下是 Reflexion 论文中的关键数据对比： 任务 基准模型 传统方法 Reflexion 提升幅度 HumanEval（代码生成） GPT-4 68.0% 91.0% +33.8% MBPP（代码生成） GPT-4 72.5% 89.5% +23.4% HotPotQA（多跳推理） GPT-3.5 54.0% 76.0% +40.7% AlfWorld（决策任务） GPT-3.5 38.0% 83.0% +118.4% graph LR subgraph 性能对比 direction TB A[传统LLM单次生成] --> |准确率| B[60-70%] C[带Reflection] --> |准确率| D[70-80%] E[完整Reflexion框架] --> |准确率| F[80-90%+] end style B fill:#ffcccc style D fill:#ffffcc style F fill:#ccffcc 关键发现： 迭代次数越多，效果越好： 大多数任务在 2-3 次迭代后达到最佳性能 复杂任务提升更明显： 需要多步推理的任务（如 AlfWorld）提升幅度最大 成本可控： 虽然需要多次调用 LLM，但总 token 消耗通常在 3-5 倍，相比性能提升是值得的 基座模型越强，效果越好： GPT-4 + Reflexion 的效果显著优于 GPT-3.5 + Reflexion 挑战与局限性虽然 Reflexion 非常强大，但它也面临一些挑战： 成本问题 多次调用 LLM： 每次反思和重试都需要额外的 API 调用 Token 消耗： 长期记忆存储会占用大量 Context 时间延迟： 多次迭代导致响应时间变长 缓解方案： 使用更小的模型做 Self-Reflection（如 GPT-3.5 做反思，GPT-4 做执行） 设置最大迭代次数（如 3-5 次） 使用缓存机制，避免重复的反思 依赖强大的基座模型 反思质量取决于模型的理解能力 弱模型可能产生无效或错误的反思 “垃圾进，垃圾出”的问题依然存在 评估器的准确性 如果 Evaluator 本身有 Bug（如测试用例不完善），会误导反思 在主观任务中（如创意写作），很难定义好的评估标准 无限循环风险 某些困难任务可能永远无法成功 智能体可能在同一个错误上反复循环 需要设计终止条件和”放弃”机制 反思的有效性 并非所有反思都对改进有帮助 模型可能产生”正确但无用”的反思（如”我需要更仔细”） 需要过滤低质量的反思 最佳实践： 123456789101112131415161718192021# 伪代码示例max_iterations = 5success_threshold = 0.9for i in range(max_iterations): result = actor.generate(task, memory) score = evaluator.evaluate(result) if score &gt;= success_threshold: return result # 成功退出 if i == max_iterations - 1: return result # 达到最大次数，返回最佳结果 reflection = self_reflect.reflect(task, result, score) # 过滤低质量反思 if reflection.quality_score &lt; 0.5: continue memory.add(reflection) 总结自我改进智能体，特别是以 Reflexion 为代表的架构，标志着 AI 开发范式的转变：从追求”更强的模型”，转向构建”更强的系统”。 核心洞察mindmap root((自我改进智能体)) 核心机制 执行-评估-反思-更新循环 语言作为反馈信号 记忆系统驱动改进 关键优势 无需模型微调 成本可控 效果显著提升 可解释性强 技术组件 Actor 行动者 Evaluator 评估者 Self-Reflection 反思 Memory 记忆系统 应用领域 代码生成与调试 复杂问答推理 决策规划任务 创意内容生成 发展方向 更高效的记忆机制 更智能的评估器 跨任务知识迁移 多智能体协作 关键 Takeaways Reflection（反思）技术 通过 Prompt Engineering 让模型自我审视 是提升质量的低成本、高效益手段 利用了 LLM”识别错误”能力强于”避免错误”的特性 Reflexion（架构） 将反思结构化为系统级框架 利用语言反馈循环，实现”吃一堑长一智” 通过记忆系统积累经验，避免重复错误 实验效果 代码生成任务提升 23-34% 复杂推理任务提升 40%+ 决策任务提升超过 100% 在多个基准测试中达到 SOTA（当时） 范式转变 从”单次生成”到”迭代优化” 从”参数训练”到”系统设计” 从”黑盒推理”到”可解释循环” 从”孤立任务”到”经验积累” 实践建议 优先在高价值、可评估的任务上应用 设计好评估器是关键（单元测试、标准答案、人类反馈） 控制迭代次数，避免无限循环 记录和分析反思质量，持续优化 Prompt 考虑成本-收益平衡，不是所有任务都需要反思 未来展望Reflexion 开启了自我改进智能体的新时代，未来可能的发展方向包括： 层次化反思： 不仅反思单步错误，还反思整体策略 元学习集成： 结合少样本学习，从更少的试错中学习 多智能体协作： 多个 Reflexion 智能体互相反思和学习 自动化评估器： 通过 AI 自动生成和优化评估标准 跨任务迁移： 将一个领域的反思经验迁移到其他领域 最重要的是： Reflexion 证明了系统设计和认知架构比单纯追求更大的模型参数更重要。这为资源有限的团队提供了一条可行的技术路线——你不需要训练 GPT-5，只需要更聪明地使用 GPT-4。 参考资源核心论文 Reflexion: Language Agents with Verbal Reinforcement Learning 作者：Noah Shinn, Federico Cassano, Ashwin Gopinath, et al. 链接：https://arxiv.org/abs/2303.11366 发表：NeurIPS 2023 相关论文 Self-Refine: Iterative Refinement with Self-Feedback 探索自我反馈的迭代改进机制 https://arxiv.org/abs/2303.17651 ReAct: Synergizing Reasoning and Acting in Language Models 结合推理和行动的智能体框架 https://arxiv.org/abs/2210.03629 Tree of Thoughts: Deliberate Problem Solving with Large Language Models 思维树搜索方法 https://arxiv.org/abs/2305.10601 开源实现 Reflexion GitHub Repo: https://github.com/noahshinn024/reflexion LangChain Reflexion Integration: https://python.langchain.com/docs/use_cases&#x2F;more&#x2F;agents&#x2F;reflexion 相关资源 Lilian Weng’s Blog - LLM Powered Autonomous Agents: 全面介绍 Agent 技术 OpenAI Cookbook - Agents: 实践指南和代码示例","categories":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/tags/AI/"}]},{"title":"深度解析：如何进行 Function Call 微调？它到底难在哪里？","slug":"深度解析：如何进行-Function-Call-微调？它到底难在哪里？","date":"2025-12-10T09:30:39.000Z","updated":"2026-01-21T09:25:23.545Z","comments":true,"path":"2025/12/10/深度解析：如何进行-Function-Call-微调？它到底难在哪里？/","permalink":"https://www.silenceboy.com/2025/12/10/%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C-Function-Call-%E5%BE%AE%E8%B0%83%EF%BC%9F%E5%AE%83%E5%88%B0%E5%BA%95%E9%9A%BE%E5%9C%A8%E5%93%AA%E9%87%8C%EF%BC%9F/","excerpt":"","text":"在 LLM（大语言模型）的应用开发中，Function Call（函数调用） 被视为模型从“聊天机器人”进化为“智能体（Agent）”的关键一步。它让模型不仅能“说话”，还能“连接世界”（如查询数据库、调用 API、控制硬件）。 虽然 GPT-4 的 Function Call 能力非常强大，但在私有化部署、降低成本或特定垂类场景下，我们往往需要对开源模型（如 Llama 3, Qwen, Baichuan 等）进行 Function Call 专项微调。 本文将手把手拆解微调流程，并深度剖析其中的痛点。 第一部分：如何进行 Function Call 微调？微调的核心目标是让模型学会两件事： 识别意图：知道什么时候该调用工具，什么时候该普通对话。 格式化输出：能够准确地按照 API 文档的要求，输出符合格式（通常是 JSON）的参数。 微调 Function Call 的本质，是将“自然语言理解”与“编程语言生成（JSON）”强绑定。我们以目前最通用的 OpenAI 格式 为目标，假设我们要微调 Llama 3 或 Qwen 系列模型。 阶段一：定义数据协议在开始造数据前，必须先定好模型“看”数据的格式。目前业界主流有两种流派： 纯文本流派（Text-based）：把工具描述直接塞进 System Prompt 文本里（最通用，兼容性好，OpenAI格式）。 Special Token 流派：使用 &lt;|tool_start|&gt; 这种特殊标记包裹 JSON（Qwen&#x2F;ChatGLM 常用，定位更精准）。 建议方案：对于大多数微调任务，采用 “ReAct 风格 + OpenAI 格式” 的混合方案效果最稳健。 即结构为：思考(Thought) -&gt; 调用(Function Call) -&gt; 观察(Observation/Output) -&gt; 回答(Response)。 阶段二：构建高质量训练数据集你需要准备四类数据，缺一不可。 正样本：标准的单轮调用这是基础教学，教会模型将自然语言转化为 JSON。 输入 (Prompt):123456789101112131415System: 你是一个助手。你可以使用以下工具：[ &#123; &quot;name&quot;: &quot;search_stock&quot;, &quot;description&quot;: &quot;查询股票价格&quot;, &quot;parameters&quot;: &#123; &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: &#123; &quot;symbol&quot;: &#123;&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;股票代码，如 AAPL&quot;&#125; &#125;, &quot;required&quot;: [&quot;symbol&quot;] &#125; &#125;]User: 帮我查查苹果公司的股价。 输出 (Label&#x2F;Target):12Assistant: &lt;thought&gt;用户想查询苹果股价，我需要调用 search_stock 工具，参数是 AAPL。&lt;/thought&gt;&lt;function_call&gt;&#123;&quot;name&quot;: &quot;search_stock&quot;, &quot;arguments&quot;: &quot;&#123;\\&quot;symbol\\&quot;: \\&quot;AAPL\\&quot;&#125;&quot;&#125;&lt;/function_call&gt; 注：加入 &lt;thought&gt; 思考过程对于提升复杂任务准确率至关重要！ 负样本：拒绝调用（防止乱调）这是最容易被忽略的，没有负样本，模型会变成“API 狂魔”，别人问“你好”，它也尝试调用函数。 输入:12System: (同上，包含 search_stock 工具定义)User: 你好，请问你是谁？ 输出:12Assistant: 我是一个人工智能助手，很高兴为您服务。有什么我可以帮您的吗？(注意：这里绝对不能包含 function_call) 多轮对话与结果反馈教会模型：调用发出去了 -&gt; 拿到结果了 -&gt; 该怎么回复用户。 输入 (包含历史记录):123... (前面的对话历史：用户问股价 -&gt; 模型调工具) ...Tool Output: &#123;&quot;price&quot;: &quot;180.5&quot;, &quot;currency&quot;: &quot;USD&quot;&#125; &lt;-- 这是模拟API返回的结果User: (通常这里是空的，或者系统自动触发下一步) 输出:1Assistant: 苹果公司(AAPL)当前的股价是 180.5 美元。 缺参追问（Slot Filling）教会模型：参数不够时，不要瞎填，要反问用户。 输入:1User: 帮我订一张机票。 输出:1Assistant: 好的，请问您想从哪里出发，去往哪里？大约什么时间？ 阶段三：数据格式化与 Tokenization在将数据喂给模型训练代码（如 LLaMA-Factory, Axolotl, HuggingFace Trainer）之前，你需要处理好 Prompt Template。 关键点：Masking（损失掩码） 在训练时，我们计算 Loss 的策略是 只计算 Assistant 回复部分的 Loss，User 的提问和 System Prompt 不需要计算 Loss。 如果你使用 LLaMA-Factory 等框架，通常需要将数据整理成如下 JSON 格式（alpaca 格式变种）： 1234567[ &#123; &quot;instruction&quot;: &quot;你是一个助手...&quot;, &quot;input&quot;: &quot;User: 查一下北京天气。\\nTools: [...]&quot;, &quot;output&quot;: &quot;&lt;thought&gt;...&lt;/thought&gt;&lt;function_call&gt;&#123;...&#125;&lt;/function_call&gt;&quot; &#125;] 极其重要的细节：EOS Token（结束符）一定要确保在 JSON 闭合后（&#125;），紧跟一个 EOS Token（如 &lt;|end_of_text|&gt;）。否则模型在推理时输出完 JSON 后不会停下来，会继续自言自语，导致解析失败。 阶段四：微调参数配置建议针对 Function Call 任务，这套参数配置经过多次验证比较稳健： 基座模型选择： 建议使用 Instruction Tuned 版本（如 Llama-3-8B-Instruct）作为起点，而不是 Base 模型。因为 Instruct 模型已经具备基本的对话能力，我们只需要做“风格迁移”。 LoRA vs 全量： LoRA 足够了。秩（Rank）建议设为 16 或 32。 Target Modules：必须覆盖 q_proj, k_proj, v_proj, o_proj，最好加上 gate_proj, up_proj, down_proj。因为逻辑推理能力主要在 MLP 层（FFN）。 学习率 (Learning Rate)： 如果是 LoRA，建议 1e-4 到 2e-4。 Function Call 需要模型精确记忆语法，学习率太低会导致学不会格式，太高会破坏原有语言能力。 Batch Size： 尽可能大。因为 Function Call 的样本通常包含很长的 System Prompt（工具定义），需要大 Batch Size 来稳定梯度。 数据配比： 通用对话数据 : Function Call 数据 ≈ 2 : 1 或 3 : 1。 再次强调，千万不要只喂 Function Call 数据，否则模型会变“傻”（Catastrophic Forgetting）。 附：一个简单的 Python 数据构造器（伪代码）为了解决数据难造的问题，通常我们会写一个脚本，让 GPT-4 帮我们生成数据。 12345678910111213141516171819202122232425262728293031import openai# 1. 定义你的工具集tools = [ &#123;&quot;name&quot;: &quot;get_weather&quot;, ...&#125;, &#123;&quot;name&quot;: &quot;calculator&quot;, ...&#125;]# 2. 编写 Prompt 让 GPT-4 扮演数据生成器prompt = f&quot;&quot;&quot;我需要你生成用于微调 LLM Function Call 的训练数据。工具列表如下：&#123;tools&#125;请生成 5 条数据，要求：1. 包含用户指令 (User Query)。2. 包含思维链 (Thought)。3. 包含标准的 JSON 格式调用 (Function Call)。4. 包含 1 条不需要调用工具的负样本。5. 包含 1 条参数缺失需要追问的样本。输出格式请直接返回 JSON List。&quot;&quot;&quot;# 3. 调用 GPT-4 生成并保存response = openai.ChatCompletion.create( model=&quot;gpt-4&quot;, messages=[&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt&#125;])# 4. 后处理：将 GPT-4 生成的数据转换为微调框架需要的格式（如 ShareGPT 或 Alpaca 格式）save_to_jsonl(response) 通过这一步步的拆解，你应该能感觉到：Function Call 微调，70% 的功夫在数据构建（Data Engineering），20% 在 Prompt Engineering（System Prompt 设计），只有 10% 是跑训练代码。 第二部分：Function Call 微调到底难在哪？很多开发者发现，微调后的模型虽然能输出 JSON，但在实际应用中经常“翻车”。Function Call 微调的难点主要体现在以下五个维度： 结构化输出的稳定性这是最基础也最头疼的问题。大模型本质上是概率模型（Next Token Prediction），而 API 接口需要的是确定性的代码（JSON&#x2F;XML）。 难点：模型可能会漏掉一个括号 &#125;，忘记加引号，或者把 int 类型输出成了 string。哪怕错一个字符，JSON Parser 就会报错，导致整个流程中断。 挑战：特别是参数极其复杂（嵌套 JSON）时，开源小参数模型（如 7B）很难稳定维持长文本的结构正确性。 参数提取与推理能力模型不仅要格式对，填进去的内容还得对。 隐含参数提取： 用户说：“帮我订下周二去上海的票。” 模型需要结合当前日期，推理出“下周二”具体的 date 是 202X-XX-XX。 幻觉（Hallucination）： API 定义需要 user_id，但用户没提供。 微调得不好的模型会胡编乱造一个 ID 填进去，而不是反问用户“请提供您的 ID”。这是非常危险的。 触发时机的判断模型需要极其敏锐地判断 “该不该调”。 过敏（False Positive）：用户只是打招呼“Hi”，或者问“你是谁”，模型却强行调用 get_user_info。这会浪费 Token 和 API 资源。 迟钝（False Negative）：用户说“把灯关了”，模型却回答“好的，我已经帮你关灯了”（其实它只是在口嗨，并没有输出调用指令）。 难点：如何在训练数据中构建高质量的负样本（Negative Samples），教模型在不需要工具时保持安静，是微调成功的关键。 多轮对话与状态管理真实场景往往不是一问一答。 场景： 用户：“帮我查查北京天气。” -&gt; 模型调工具 -&gt; 返回结果。 用户：“那上海的呢？” 难点：模型需要理解“那…呢”是指沿用上一步的意图（查天气），但修改参数（地点变为上海）。微调数据如果缺乏这种多轮上下文的样本，模型就会在第二轮变“傻”。 数据构建的复杂性相比于通用对话数据，高质量的 Function Call 数据非常稀缺。 多样性不足：如果你只用 10 个 API 构造数据，模型可能记住了这 10 个 API 的名字。当你换了一个新的 API（即使 Schema 很清晰），模型可能就泛化不过去了。 构造难度大：手写 Function Call 对话极其耗时。目前主流做法是使用 GPT-4 构造合成数据（Data Distillation），但如何保证 GPT-4 生成的数据逻辑严密、没有幻觉，本身又是一个工程难题。 总结与建议Function Call 微调与其说是在调“知识”，不如说是在调“行为范式”。 如果你想克服上述难点，我有以下几点建议： 数据质量大于数量：1000 条覆盖了多轮、负样本、复杂参数提取的高质量数据，远胜于 1万条简单的单轮模板数据。 强制 CoT（思维链）：在训练数据中，强制要求模型在输出 JSON 前，先输出一段 &lt;thought&gt;（思考过程）。例如：“用户想查天气，地点是北京，我需要调用 weather 工具”。让模型先想后写，能显著提高参数提取的准确率。 约束语法：在推理阶段，配合 Grammar-Constrained Decoding（如 GBNF 语法约束），强制模型只能生成合法的 JSON Token，从根源上解决格式错误问题。 Function Call 是大模型落地的“最后一公里”，虽然难走，但一旦调通，模型的实用价值将呈指数级上升。","categories":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/tags/AI/"}]},{"title":"深度解析 AI Agent 设计模式：从 CoT 到多智能体协同","slug":"AI-Agent-Design-Patterns-Comprehensive-Guide","date":"2025-12-05T02:00:00.000Z","updated":"2025-12-05T09:08:13.405Z","comments":true,"path":"2025/12/05/AI-Agent-Design-Patterns-Comprehensive-Guide/","permalink":"https://www.silenceboy.com/2025/12/05/AI-Agent-Design-Patterns-Comprehensive-Guide/","excerpt":"","text":"在 LLM（大语言模型）应用开发的演进过程中，我们正在经历从单纯的 Prompt Engineering（提示词工程） 向 Agent Engineering（智能体工程） 的范式转变。如果说 Prompt Engineering 是在教模型”如何说话”，那么 Agent Engineering 则是在教模型”如何做事”。 作为一名在 AI 领域深耕多年的开发者，我见证了 Agent 从简单的”工具调用”发展为如今复杂的”多智能体协作系统”。选择合适的 Agent 模式（Agentic Patterns）对于构建鲁棒、高效的 AI 应用至关重要。 本文将详细介绍 8 种核心的 Agent 设计模式，从基础到高级，从单体到协作，帮助你构建完整的 Agent 知识体系。 基础模式Chain-of-Thought (CoT) - 思维链Chain-of-Thought 是所有推理模式的基石，也是最简单有效的 Prompt Engineering 技术。它通过引导模型”逐步思考”来提升复杂问题的解决能力。 核心机制通过在 Prompt 中添加 &quot;Let&#39;s think step by step&quot; 或提供少样本推理示例（Few-shot CoT），引导 LLM 将复杂问题分解为中间推理步骤，而非直接给出答案。 graph LR Input[问题] --> Step1[推理步骤 1] Step1 --> Step2[推理步骤 2] Step2 --> Step3[推理步骤 3] Step3 --> Answer[最终答案] 学术来源 论文: Wei et al., 2022, “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models” 核心发现: CoT 在数学、常识推理、符号操作等任务上显著提升准确率（GPT-3 从 17.7% 提升到 58.1%） 特点分析 优点： 零成本: 无需外部工具或额外 API 调用 高度可控: 推理过程完全在模型内部 广泛适用: 适用于几乎所有需要推理的任务 可解释性强: 中间步骤清晰可见 缺点： 无法获取外部信息: 只能基于模型的知识储备 幻觉风险: 推理步骤可能看似合理但实际错误 长文本消耗: 推理步骤会占用 token 适用场景： 数学问题求解 逻辑推理题 常识推理 不需要外部信息的复杂问题 代表框架： 原生 Prompt Engineering（OpenAI, Anthropic, Google） LangChain: LLMChain 配合 CoT prompt 最佳实践12345678910111213141516# Zero-shot CoTprompt = &quot;&quot;&quot;问题：一个书架有 3 层，每层有 8 本书。如果我拿走了 5 本书，还剩多少本？让我们一步步思考：&quot;&quot;&quot;# Few-shot CoTprompt = &quot;&quot;&quot;问题：咖啡厅有 23 个顾客，中午来了 52 个，下午走了 15 个，现在有多少人？思考：开始有 23 人，来了 52 人，所以是 23+52=75 人。然后走了 15 人，75-15=60 人。答案：60 人问题：一个书架有 3 层，每层有 8 本书。如果我拿走了 5 本书，还剩多少本？思考：&quot;&quot;&quot; Tool-Use &#x2F; Function Calling - 工具调用Tool-Use 是最基础的 Agent 模式，让 LLM 能够调用外部工具来完成无法靠自身完成的任务。它是 ReAct 的简化版，通常只进行一次工具调用。 核心机制 LLM 分析用户请求，判断是否需要调用工具 如果需要，选择合适的工具并生成调用参数 执行工具调用，获取结果 基于工具返回结果生成最终答案 graph LR User[用户请求] --> LLM LLM --> Decision{需要工具?} Decision -- Yes --> ToolCall[调用工具] Decision -- No --> DirectAnswer[直接回答] ToolCall --> ToolResult[工具结果] ToolResult --> FinalAnswer[基于结果回答] 学术来源 OpenAI Function Calling (官方文档, 2023) Toolformer (Schick et al., 2023): 自学习使用工具的语言模型 特点分析 优点： 简单直接: 最容易实现的 Agent 模式 低延迟: 通常只需要 2 次 LLM 调用 成本低: Token 消耗少 可靠性高: 错误路径少 缺点： 单步限制: 不适合需要多步推理的复杂任务 无法纠错: 工具调用失败后难以恢复 适用场景： 简单的 API 查询（天气、汇率、股价） 计算器、单位转换 数据库单次查询 翻译、OCR 等单一功能 代表框架： OpenAI: functions &#x2F; tools 参数 Anthropic: tool_use (Claude 3+) Google: Gemini Function Calling LangChain: create_tool_calling_agent 示例代码1234567891011121314151617181920212223242526from openai import OpenAItools = [ &#123; &quot;type&quot;: &quot;function&quot;, &quot;function&quot;: &#123; &quot;name&quot;: &quot;get_weather&quot;, &quot;description&quot;: &quot;获取指定城市的天气&quot;, &quot;parameters&quot;: &#123; &quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: &#123; &quot;city&quot;: &#123;&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;城市名称&quot;&#125; &#125;, &quot;required&quot;: [&quot;city&quot;] &#125; &#125; &#125;]response = client.chat.completions.create( model=&quot;gpt-4&quot;, messages=[&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;北京今天天气如何?&quot;&#125;], tools=tools)# LLM 会返回: &#123;&quot;name&quot;: &quot;get_weather&quot;, &quot;arguments&quot;: &#123;&quot;city&quot;: &quot;北京&quot;&#125;&#125; ReAct (Reasoning + Acting)ReAct 是目前最经典、应用最广泛的 Agent 模式。它将 CoT 的推理能力与 Tool-Use 的行动能力结合，形成”思考-行动-观察”的循环。 核心机制Agent 不再直接给出答案，而是遵循一个循环： Thought（思考）：根据当前情况，思考下一步该做什么 Action（行动）：调用具体的工具（如搜索、计算器、数据库查询） Observation（观察）：获取工具执行的结果 Repeat（重复）：根据观察结果再次思考，直到满足终止条件 graph LR Input --> Thought Thought --> Decision{是否结束?} Decision -- No --> Action Action --> Tool[External Tools] Tool --> Observation Observation --> Thought Decision -- Yes --> Answer[Final Answer] 学术来源 论文: Yao et al., 2022, “ReAct: Synergizing Reasoning and Acting in Language Models” 核心贡献: 将推理轨迹和任务特定行动交织，在 HotpotQA、Fever 等基准上超越 CoT 特点分析 优点： 减少幻觉：通过引入外部真实数据（Observation）来支撑推理 可解释性强：用户可以看到 Agent 的思考路径 灵活性：能够处理需要多步推导的问题 缺点： 上下文消耗：中间步骤会占用大量 Context Window 错误传播：中间某一步错了，后续可能无法挽回 延迟较高：串行执行，每一步都需要 LLM 推理 终止条件： Agent 输出 &quot;Final Answer:&quot; 前缀 达到 max_iterations（通常 5-10 次） 连续失败超过阈值 适用场景： 需要实时数据查询的任务（如”查询昨天的股价并计算涨幅”） 中等复杂度的多步任务 需要组合多个工具的场景 代表框架： LangChain: AgentExecutor (最基础的实现) LlamaIndex: ReActAgent LangGraph: create_react_agent 实际示例12345678910111213141516171819用户: 比较一下特斯拉和比亚迪昨天的股价涨幅Thought 1: 我需要先查询特斯拉昨天的股价Action 1: stock_price(symbol=&quot;TSLA&quot;, date=&quot;2024-01-10&quot;)Observation 1: 开盘 $238.45, 收盘 $242.84Thought 2: 然后查询比亚迪的股价Action 2: stock_price(symbol=&quot;BYDDY&quot;, date=&quot;2024-01-10&quot;)Observation 2: 开盘 $52.30, 收盘 $53.12Thought 3: 现在计算涨幅并比较Action 3: calculate((242.84-238.45)/238.45 * 100)Observation 3: 1.84%Action 4: calculate((53.12-52.30)/52.30 * 100)Observation 4: 1.57%Thought 4: 我已经得到了所有信息，可以给出答案了Final Answer: 特斯拉昨天涨幅为 1.84%，比亚迪为 1.57%，特斯拉涨幅更大。 进阶模式Plan-and-Execute (规划与执行)对于更复杂的任务，ReAct 模式容易迷失在细节中。Plan-and-Execute 模式将过程拆解为”规划师（Planner）”和”执行者（Executor）”两个阶段。 核心机制 Planning：Planner Agent 接收用户指令，生成一个包含多个步骤的计划列表 Execution：Executor Agent 依次执行这些步骤 Replanning (可选)：根据执行结果，动态调整剩余的计划 graph TD User[User Input] --> Planner Planner --> Plan[Step-by-Step Plan] Plan --> Executor Executor --> Tool[Tools] Tool --> Result Result --> Check{是否需要 Replan?} Check -- Yes --> Planner Check -- No --> NextStep[下一步] NextStep --> Executor Executor --> Final[完成] 学术来源 LLM+P (Liu et al., 2023): 使用 LLM 进行规划的框架 AutoGPT (GitHub) &#x2F; BabyAGI (GitHub): 早期的自主 Agent 实现 (2023) 计划表示方式 自然语言列表： 123451. 搜索特斯拉 2023 年财报2. 提取营收和利润数据3. 搜索比亚迪 2023 年财报4. 提取营收和利润数据5. 对比分析并生成报告 结构化 JSON： 123456&#123; &quot;steps&quot;: [ &#123;&quot;id&quot;: 1, &quot;action&quot;: &quot;search&quot;, &quot;params&quot;: &#123;&quot;query&quot;: &quot;Tesla 2023 financial report&quot;&#125;&#125;, &#123;&quot;id&quot;: 2, &quot;action&quot;: &quot;extract&quot;, &quot;params&quot;: &#123;&quot;fields&quot;: [&quot;revenue&quot;, &quot;profit&quot;]&#125;, &quot;depends_on&quot;: [1]&#125; ]&#125; 动态 Replanning 策略 触发条件： 执行失败（工具调用报错） 发现新信息（与原计划假设不符） 用户中途修改需求 调整策略： 局部修正：只调整后续步骤 全局重规划：重新生成整个计划 特点分析 优点： 长程规划能力：适合处理步骤繁多、跨度大的任务 关注点分离：Planner 专注大局，Executor 专注细节，可以使用不同能力的模型（如 GPT-4 用于规划，GPT-3.5 用于执行） 并行潜力：独立的步骤可以并行执行 缺点： 盲目执行：如果初始计划有误且缺乏 Replanning 机制，执行者会”一条道走到黑” 开销：需要更多的 API 调用 计划幻觉：LLM 可能生成不可行的计划 适用场景： 生成长篇报告（调研 → 大纲 → 撰写 → 审阅） 复杂的编码任务（需求分析 → 架构设计 → 编码 → 测试） 数据分析工作流 代表框架： LangGraph: Plan-and-Execute 模板 TaskWeaver (Microsoft, 2024): 面向数据分析的规划型 Agent OpenAI Swarm (2024): 轻量级多 Agent 编排 Reflection &#x2F; Self-Correction (反思与自修正)在人类的工作流中，我们写完东西通常会检查一遍。Reflection 模式赋予了 Agent 这种”自我反思”的能力。 核心机制Agent 在生成结果后，会有一个”批评者（Critique）”角色对其进行评估，如果发现错误或不足，会提出修改建议，Agent 再进行修正。这是一种典型的”System 2”慢思考模式。 graph TD Input --> Generator Generator --> Output Output --> Evaluator[Evaluator / Critique] Evaluator --> Check{质量是否满足?} Check -- No --> Feedback[反馈建议] Feedback --> Generator Check -- Yes --> Final[Final Answer] 学术来源 Reflexion (Shinn et al., 2023): 通过语言反馈进行自我反思 Self-Refine (Madaan et al., 2023): 迭代式自我改进框架 反思机制类型 Self-Critique: Agent 自己评价自己（单模型） 12Generator: [生成代码]Evaluator (同一个 LLM): &quot;这段代码没有处理边界情况...&quot; External Critique: 独立的 Evaluator Agent（双模型） 12Generator (GPT-3.5): [生成内容]Evaluator (GPT-4): &quot;论证不够充分，需要补充数据支持...&quot; Tool-Based Verification: 使用工具验证（如运行代码） 123Generator: [生成 Python 代码]Executor: [运行代码] --&gt; Error: &quot;NameError: &#x27;x&#x27; is not defined&quot;Reflection: &quot;发现未定义的变量 x，需要在函数开头初始化&quot; 迭代终止条件 达到质量阈值（如测试通过、语法正确） 最大迭代次数（通常 3-5 次） 连续两次输出无显著改进（相似度 &gt; 95%） Token 预算耗尽 特点分析 优点： 高质量输出：显著提升代码生成、写作的准确性和质量 自我修复：能纠正一些明显的幻觉或逻辑错误 持续改进：每次迭代都基于上次的不足 缺点： 高延迟：需要多轮对话才能产出结果 成本高：Token 消耗成倍增加（3 轮迭代 ≈ 6x tokens） 可能陷入局部最优：反复修改同一个小问题 适用场景： 代码生成：生成代码 → 运行报错 → 反思错误 → 修正代码 高质量内容创作：起草 → 审阅 → 修改 需要严格正确性的任务（如数学证明、法律文书） 代表框架： LangGraph: 支持构建带有循环和条件判断的反思流 Reflexion: 开源实现 AutoGPT: 内置 self-critique 机制 Tree-of-Thoughts (ToT) - 思维树Tree-of-Thoughts 将推理过程建模为树形结构，探索多条候选路径，并通过评估选择最优路径。它适合需要”试错”和”回溯”的复杂推理任务。 核心机制 生成多个候选思考路径：对于每个问题，生成 N 个可能的下一步思考（branches） 评估每条路径：使用 LLM 或启发式函数评估每条路径的”前景” 选择与扩展：选择得分最高的路径继续扩展 回溯：如果某条路径走不通，回退到上一节点尝试其他分支 graph TD Input --> T1[Thought 1] Input --> T2[Thought 2] Input --> T3[Thought 3] T1 --> Eval1[Evaluate: 7分] T2 --> Eval2[Evaluate: 9分] T3 --> Eval3[Evaluate: 4分] Eval2 --> Best[选择最佳: T2] Best --> T2_1[T2 → Thought 2.1] Best --> T2_2[T2 → Thought 2.2] T2_1 --> Final[继续扩展...] 学术来源 论文: Yao et al., 2023, “Tree of Thoughts: Deliberate Problem Solving with Large Language Models” 官方实现: princeton-nlp&#x2F;tree-of-thought-llm 核心贡献: 在 Game of 24、创意写作、迷你纵横字谜等任务上显著超越 CoT 搜索策略 广度优先搜索 (BFS)：逐层扩展，保证找到最优解 深度优先搜索 (DFS)：快速探索深层，适合有明确目标的任务 束搜索 (Beam Search)：每层只保留 Top-K 候选，平衡质量与效率 特点分析 优点： 探索多条路径：不会因为第一步错误而导致全盘皆输 支持回溯：可以撤销错误决策 适合创意任务：可以生成多个方案供选择 缺点： 成本极高：需要生成和评估大量候选（N^D，N&#x3D;分支数，D&#x3D;深度） 延迟高：需要多次 LLM 调用 实现复杂：需要管理树结构和搜索状态 适用场景： Game of 24：给定 4 个数字，通过加减乘除得到 24 创意写作：探索多个开头、多个情节发展 策略游戏：下棋、走迷宫 数学证明：尝试多种证明路径 代表框架： Princeton NLP: ToT 官方实现 LangChain: ToT 实验性支持 Tree-of-Thought-Prompting: 纯 Prompt 版本 示例：Game of 2412345678910111213Input: 使用 4, 9, 10, 13 得到 24Step 1 - 生成 3 个候选思考:T1: (13 - 9) * (10 - 4) = 4 * 6 = 24 ✓T2: (13 - 4) * 9 / 10 = ?T3: (10 - 4) * 13 / 9 = ?Step 2 - 评估:Eval(T1): 10分 (已得到 24)Eval(T2): 6分 (可能性一般)Eval(T3): 5分 (可能性较低)Step 3 - 选择 T1，验证正确，输出答案 协作模式Hierarchical Agents (层级式智能体)当任务复杂到单个 Agent 无法胜任时，我们需要组织架构。Hierarchical 模式采用了类似公司的”经理-员工”结构。 核心机制 Manager &#x2F; Router：负责理解高层目标，将任务拆解并分发给下层的专家 Agent Workers &#x2F; Sub-Agents：专注于特定领域的任务（如一个负责写 SQL，一个负责做图表，一个负责写文案） graph TD User[User Input] --> Manager[Manager Agent] Manager -- \"Delegation\" --> WorkerA[Worker A: SQL专家] Manager -- \"Delegation\" --> WorkerB[Worker B: 可视化专家] Manager -- \"Delegation\" --> WorkerC[Worker C: 文案专家] WorkerA -- \"Result\" --> Manager WorkerB -- \"Result\" --> Manager WorkerC -- \"Result\" --> Manager Manager --> Synthesize[综合结果] Synthesize --> Final[Final Output] 通信协议 Manager → Worker (任务下发)： 12345&#123; &quot;task&quot;: &quot;查询 2023 年销售数据&quot;, &quot;context&quot;: &#123;&quot;database&quot;: &quot;sales_db&quot;, &quot;year&quot;: 2023&#125;, &quot;expected_output&quot;: &quot;JSON格式的销售统计&quot;&#125; Worker → Manager (结果上报)： 12345&#123; &quot;status&quot;: &quot;success&quot;, &quot;result&quot;: &#123;&quot;total_sales&quot;: 1000000, &quot;top_product&quot;: &quot;iPhone&quot;&#125;, &quot;metadata&quot;: &#123;&quot;query_time&quot;: &quot;0.5s&quot;&#125;&#125; 并行执行与失败处理 并行执行：独立的 Workers 可以同时工作，提升效率 123456# 并行调用results = await asyncio.gather( worker_sql.run(task_sql), worker_viz.run(task_viz), worker_writer.run(task_writer)) 失败处理： 重试机制（最多 3 次） 降级策略（使用备用 Worker） 上报 Manager 重新分配 层级深度 推荐 2-3 层：Manager → Workers → Sub-Workers 过深的问题：信息失真、延迟累积、调试困难 特点分析 优点： 专业化：每个 Sub-Agent 可以挂载不同的 Prompt 和 Tools，更加专注 上下文隔离：Sub-Agent 的繁琐执行过程不需要完全暴露给 Manager，节省上层 Context 可扩展：添加新功能只需增加新的 Worker 缺点： 协调难度：上下级之间的通信和指令传递容易失真 单点故障：Manager 失败会导致整个系统失败 适用场景： 企业级复杂的客服系统（分流到 售前、售后、技术支持） 全栈软件开发（产品经理 → 架构师 → 工程师） 数据分析报告（数据采集 → 数据清洗 → 可视化 → 撰写） 代表框架： LangChain: 传统的 RouterChain Semantic Kernel: 它的 Plugin 架构天然适合这种嵌套调用 AutoGen: 支持 hierarchical chat Multi-Agent Collaboration (多智能体协同)这是目前最前沿的模式。与层级式不同，Multi-Agent 更强调智能体之间的平等交互、讨论甚至辩论。 核心机制多个拥有不同角色（Persona）的 Agent 共享一个环境或对话历史。它们像在一个聊天群组里一样，互相发送消息。 Role-Playing：例如一个扮演”用户”，一个扮演”开发”，一个扮演”测试” Debate：通过不同观点的碰撞来消除偏见 graph TD User --> Environment subgraph \"Multi-Agent Environment\" Environment((Shared Context)) AgentA[Agent A: PM] Environment AgentB[Agent B: Engineer] Environment AgentC[Agent C: QA] AgentA -.消息.-> AgentB AgentB -.消息.-> AgentC AgentC -.消息.-> AgentA end Environment --> Output[最终产出] 通信模式 Broadcast（广播）： 12agent_a.send(message=&quot;需求已确认&quot;, recipients=&quot;all&quot;)# 所有 Agent 都能看到这条消息 Point-to-Point（点对点）： 12agent_pm.send(message=&quot;请实现登录功能&quot;, recipient=agent_engineer)# 只有 agent_engineer 收到 Publish-Subscribe（发布订阅）： 123agent_a.subscribe(topic=&quot;code_review&quot;)agent_b.publish(topic=&quot;code_review&quot;, message=&quot;请审查我的 PR&quot;)# 所有订阅 code_review 的 Agent 都会收到 终止条件 达成共识：所有 Agent 同意当前结果 123Agent A: &quot;我认为这个方案可行&quot;Agent B: &quot;我同意&quot;Agent C: &quot;我也同意&quot; → 终止 最大轮次限制：防止无休止的争论（通常 10-20 轮） 人工介入：无法自动解决时，升级给人类 冲突解决机制 投票：多数 Agent 支持的方案获胜 权重加权：根据 Agent 的专业度赋予不同权重 人工仲裁：复杂冲突由人类决策 特点分析 优点： 涌现能力：多个较弱的模型通过协作可能完成强模型都做不到的任务 解耦：添加新功能只需增加一个新的 Agent 角色 多样性：不同 Agent 带来不同视角，减少偏见 缺点： 死循环：Agent 之间可能陷入无休止的客套或争论 不可控：交互路径难以预测，调试困难 成本极高：每个 Agent 都需要独立的 LLM 调用 适用场景： 模拟社会行为（如斯坦福的小镇实验） 复杂的创意工坊（头脑风暴） 全流程软件开发（如 MetaGPT） 辩论与决策（如政策制定模拟） 代表框架： AutoGen (Microsoft)：目前最流行的多智能体框架，支持灵活的对话流 CrewAI：基于 LangChain，更强调角色的扮演和任务编排 MetaGPT：将 SOP（标准作业程序）编码到 Agent 协作中 ChatDev: 模拟软件公司的 Agent 协作 模式对比与选型指南为了帮助大家更直观地选择，我整理了以下对比图表： 模式 (Pattern) 复杂度 延迟 成本 鲁棒性 可调试性 最佳适用场景 Chain-of-Thought ⭐ ⭐ ⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ 数学推理、逻辑问题、无需外部信息的任务 Tool-Use ⭐ ⭐ ⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 单次API调用、计算器、简单查询 ReAct ⭐⭐ ⭐⭐ ⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ 需要调用工具的实时问答、中等复杂度多步任务 Plan-and-Execute ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ 长流程任务、需要事先规划的复杂操作 Reflection ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ 代码生成、高质量写作、需要严格正确性的任务 Tree-of-Thoughts ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐ 创意任务、策略游戏、需要探索多条路径的问题 Hierarchical ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ 任务领域跨度大、需要专业分工的场景 Multi-Agent ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐ ⭐ 模拟人类团队协作、极高复杂度的开放性问题 选型建议1. 从简原则（由简到繁）1Prompt → CoT → Tool-Use → ReAct → Plan-Execute → Reflection → ToT → Multi-Agent 永远先从最简单的方案开始，只有在遇到瓶颈时才升级到更复杂的模式。 2. 根据瓶颈选择模式 遇到的问题 推荐模式 原因 Agent 推理不够严谨 CoT 引导逐步思考 需要实时数据 Tool-Use 或 ReAct 调用外部工具 Agent 经常”乱跑” Plan-and-Execute 事先规划，约束行为 输出质量不稳定 Reflection 自我检查与改进 第一步就错了导致全盘皆输 ToT 探索多条路径 任务领域跨度大 Hierarchical 专业分工 需要多角度思考 Multi-Agent 不同角色辩论 3. 混合使用在实际生产中，往往需要组合多种模式： ReAct + Reflection: Agent 执行 ReAct 循环，每次 Action 后进行 Self-Critique Plan-Execute + Hierarchical: Planner 生成计划，多个 Worker 并行执行 Multi-Agent + Tool-Use: 每个 Agent 都可以调用工具 ToT + ReAct: 在 ToT 的每个节点内部运行 ReAct 循环 4. 框架选择建议 初学者: LangChain（生态完善、文档丰富） 生产级应用: LangGraph（更灵活、可控性强） 多智能体: AutoGen（最成熟的多智能体框架） 代码生成: Reflexion + LangGraph（支持反思循环） 轻量级: 直接使用 OpenAI &#x2F; Anthropic API + 自定义逻辑 5. 成本与性能权衡 成本敏感: CoT &gt; Tool-Use &gt; ReAct 延迟敏感: Tool-Use &gt; ReAct &gt; Plan-Execute 质量优先: Reflection &gt; ToT &gt; Multi-Agent 可控性优先: Plan-Execute &gt; Hierarchical &gt; ReAct 结语Agent 模式并非非此即彼，在实际的生产级应用中，我们往往会混合使用。例如，在一个 Multi-Agent 系统中，某个单独的 Agent 内部可能运行着 ReAct 循环，并在输出前进行 Reflection。 未来趋势 Compound AI Systems：将多种模式组合成精密系统 Agentic RAG：结合检索增强和 Agent 推理 Human-in-the-Loop：Agent 在关键决策点请求人类确认 可观测性：LangSmith、Weights &amp; Biases 等工具帮助监控和调试 Agent-as-a-Service：云端托管的 Agent 服务（如 OpenAI Assistants API） 学习路径建议 Week 1-2: 掌握 CoT 和 Tool-Use，理解基础推理和工具调用 Week 3-4: 实践 ReAct，构建能调用多个工具的 Agent Week 5-6: 学习 Plan-Execute 和 Reflection，处理复杂任务 Week 7-8: 探索 ToT 和 Hierarchical，理解高级推理和分工协作 Week 9+: 深入 Multi-Agent，构建协同系统 推荐资源 论文集: Awesome LLM Agents Papers 框架: LangChain, LangGraph, AutoGen, CrewAI 课程: DeepLearning.AI 的 “AI Agents in LangGraph” 社区: LangChain Discord, AutoGen GitHub Discussions","categories":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/tags/AI/"},{"name":"Agent","slug":"Agent","permalink":"https://www.silenceboy.com/tags/Agent/"}]},{"title":"RAG系统检索内容缺失问题深度分析与解决方案","slug":"RAG系统检索内容缺失问题深度分析与解决方案","date":"2025-11-24T06:30:00.000Z","updated":"2026-01-22T03:43:31.524Z","comments":true,"path":"2025/11/24/RAG系统检索内容缺失问题深度分析与解决方案/","permalink":"https://www.silenceboy.com/2025/11/24/RAG%E7%B3%BB%E7%BB%9F%E6%A3%80%E7%B4%A2%E5%86%85%E5%AE%B9%E7%BC%BA%E5%A4%B1%E9%97%AE%E9%A2%98%E6%B7%B1%E5%BA%A6%E5%88%86%E6%9E%90%E4%B8%8E%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","excerpt":"在构建和部署RAG（Retrieval-Augmented Generation，检索增强生成）系统的过程中，内容缺失问题是最常见也最影响用户体验的核心挑战之一。当用户提出问题时，系统无法检索到相关内容或检索结果不完整，直接导致生成的答案不准确、不完整甚至完全错误。本文将深入分析RAG检索内容缺失的根本原因，并提供系统化的优化策略。","text":"在构建和部署RAG（Retrieval-Augmented Generation，检索增强生成）系统的过程中，内容缺失问题是最常见也最影响用户体验的核心挑战之一。当用户提出问题时，系统无法检索到相关内容或检索结果不完整，直接导致生成的答案不准确、不完整甚至完全错误。本文将深入分析RAG检索内容缺失的根本原因，并提供系统化的优化策略。 内容缺失问题的表现形式在实际应用中，RAG系统的内容缺失问题通常表现为以下几种典型场景： 完全检索失败系统对于知识库中明确存在的信息无法检索到任何相关内容，导致大模型只能基于其预训练知识回答，或者直接承认”我不知道”。 典型案例： 用户询问：”公司2024年Q3的销售数据是多少？” 知识库中存在相关财报文档 但检索结果为空，系统回答：”抱歉，我没有找到相关信息” 部分信息缺失系统能够检索到相关内容，但关键信息片段缺失，导致答案不完整或存在逻辑断层。 典型案例： 用户询问：”如何配置产品的高级安全功能？” 检索到配置步骤的前3步，但缺少关键的第4-6步 生成的答案不完整，用户无法完成完整配置流程 上下文割裂检索到的多个内容片段之间缺乏必要的连接信息，导致语义不连贯或逻辑关系不清晰。 典型案例： 检索到”概念定义”和”操作步骤”两个片段 但缺少中间的”前置条件”和”注意事项” 导致用户按照步骤操作时遇到未预期的问题 内容缺失的三大根本原因切片策略不合理问题描述： 切片（Chunking）是RAG系统中将长文档分割成小片段的关键步骤。不合理的切片策略是导致内容缺失的首要原因。 常见的切片问题： 固定长度切片导致语义割裂 简单按字符数或token数固定切片（如每512 tokens一个chunk） 可能在句子中间、段落中间甚至词语中间切割 破坏了语义的完整性，导致检索时上下文不完整 123# 不推荐的固定长度切片示例def simple_chunk(text, chunk_size=512): return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)] 切片粒度过大 每个chunk包含过多内容（如2000+ tokens） 导致向量表示过于粗糙，相似度计算不准确 检索时可能因为chunk中包含噪音信息而降低相关性得分 切片粒度过小 每个chunk只包含一两句话（如100-200 tokens） 缺乏足够的上下文信息 即使检索到相关chunk，也无法提供完整的语义背景 忽视文档结构 不考虑文档的章节、段落、列表等结构 在表格、代码块、公式等特殊内容处随意切割 导致结构化信息丢失或混乱 影响： 语义不完整：检索到的片段缺少前因后果 关键信息丢失：重要内容被分割到不同chunk，检索时遗漏 噪音干扰：过大的chunk包含无关内容，降低检索精度 向量召回率低问题描述： 向量召回率是指系统能够从知识库中检索出所有相关内容的能力。低召回率意味着大量相关信息被遗漏。 导致召回率低的因素： 向量表示能力不足 使用的Embedding模型维度较低或质量不佳 模型未针对特定领域进行fine-tuning 对于专业术语、缩写、多语言混合等场景表现不佳 12345678# 示例：选择合适的Embedding模型from sentence_transformers import SentenceTransformer# 通用模型 - 可能不适合专业领域model_general = SentenceTransformer(&#x27;all-MiniLM-L6-v2&#x27;)# 领域优化模型 - 在特定领域表现更好model_domain = SentenceTransformer(&#x27;specialized-model-for-legal&#x27;) 查询与文档的表述差异（Semantic Gap） 用户使用口语化表达：”怎么改密码？” 文档使用正式表述：”密码重置流程” 向量空间距离较大，导致检索失败 检索参数配置不当 Top-K设置过小（如只检索前3个结果） 相似度阈值设置过高（如只返回相似度&gt;0.8的结果） 导致潜在的相关内容被过滤掉 向量索引优化问题 使用近似最近邻算法（ANN）时精度损失过大 索引参数（如HNSW的ef_construction、M参数）设置不合理 为追求速度牺牲了召回精度 影响： 漏检相关内容：知识库中存在的信息无法被检索到 答案不全面：只能基于部分信息生成回答 用户体验差：系统表现出”知识盲区” 知识覆盖不全问题描述： 即使检索系统工作正常，如果知识库本身存在覆盖盲区，也会导致内容缺失问题。 知识覆盖不全的表现： 原始文档质量问题 文档内容不完整、过时或错误 关键信息以图片、附件等形式存在，未被索引 隐式知识未被显式化（如依赖经验判断的决策逻辑） 文档预处理不充分 PDF、Word等格式解析不完整 表格、图表信息丢失或转换错误 OCR识别错误导致文本内容不准确 知识更新不及时 新增文档未及时索引 已更新的文档仍保留旧版本 过期信息未被删除，造成混淆 元数据缺失 缺少文档创建时间、作者、版本等元数据 缺少章节、标题等结构化信息 无法进行基于元数据的过滤和排序 影响： 知识库存在结构性盲区：某些主题完全没有覆盖 信息时效性问题：返回过时或错误信息 无法进行精确定位：缺少元数据辅助检索 系统化的优化解决方案切片策略优化根据文档结构智能切片语义感知切片（Semantic Chunking）： 123456789101112131415161718192021222324252627282930313233def semantic_chunking(document, max_chunk_size=512, overlap=50): &quot;&quot;&quot; 基于语义边界进行智能切片 Args: document: 原始文档文本 max_chunk_size: 最大chunk大小（tokens） overlap: chunk之间的重叠大小（tokens） Returns: List of chunks with semantic coherence &quot;&quot;&quot; from langchain.text_splitter import RecursiveCharacterTextSplitter # 定义分割层级：段落 -&gt; 句子 -&gt; 短语 text_splitter = RecursiveCharacterTextSplitter( chunk_size=max_chunk_size, chunk_overlap=overlap, length_function=len, separators=[ &quot;\\n\\n&quot;, # 段落分隔 &quot;\\n&quot;, # 行分隔 &quot;。&quot;, # 句子分隔（中文） &quot;.&quot;, # 句子分隔（英文） &quot;;&quot;, # 分号 &quot;,&quot;, # 逗号 &quot; &quot;, # 空格 &quot;&quot; # 字符级别（最后手段） ] ) chunks = text_splitter.split_text(document) return chunks 结构化内容特殊处理： 123456789101112131415161718192021222324252627282930313233343536373839def structure_aware_chunking(document): &quot;&quot;&quot; 针对不同内容类型采用不同切片策略 &quot;&quot;&quot; chunks = [] # 1. 识别文档结构 sections = extract_sections(document) # 章节提取 for section in sections: content_type = detect_content_type(section) if content_type == &quot;table&quot;: # 表格完整保留，不切割 chunk = &#123; &quot;content&quot;: section, &quot;type&quot;: &quot;table&quot;, &quot;metadata&quot;: &#123;&quot;format&quot;: &quot;structured&quot;&#125; &#125; chunks.append(chunk) elif content_type == &quot;code&quot;: # 代码块完整保留 chunk = &#123; &quot;content&quot;: section, &quot;type&quot;: &quot;code&quot;, &quot;metadata&quot;: &#123;&quot;language&quot;: detect_language(section)&#125; &#125; chunks.append(chunk) elif content_type == &quot;list&quot;: # 列表项可以分组但保持完整性 chunks.extend(chunk_list_items(section)) else: # 普通文本采用语义切片 chunks.extend(semantic_chunking(section)) return chunks 动态调整切片大小根据内容复杂度和查询场景动态调整： 123456789101112131415161718192021222324def adaptive_chunking(document, complexity_score): &quot;&quot;&quot; 根据内容复杂度动态调整chunk大小 Args: document: 文档内容 complexity_score: 内容复杂度评分 (0-1) Returns: Optimized chunks &quot;&quot;&quot; # 复杂内容需要更大的chunk保持上下文 if complexity_score &gt; 0.7: chunk_size = 800 # 技术文档、法律文本等 elif complexity_score &gt; 0.4: chunk_size = 512 # 标准业务文档 else: chunk_size = 256 # 简单问答、FAQ等 # 重叠比例也根据复杂度调整 overlap_ratio = 0.1 + (complexity_score * 0.1) # 10%-20% overlap = int(chunk_size * overlap_ratio) return semantic_chunking(document, chunk_size, overlap) 增加上下文窗口（Contextual Chunk）为每个chunk添加上下文信息： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def create_contextual_chunks(document): &quot;&quot;&quot; 为每个chunk添加上下文信息 &quot;&quot;&quot; chunks = [] base_chunks = semantic_chunking(document) # 提取文档级元信息 doc_title = extract_title(document) doc_summary = extract_summary(document) for i, chunk in enumerate(base_chunks): # 添加前置上下文（前一个chunk的末尾） prefix_context = &quot;&quot; if i &gt; 0: prefix_context = base_chunks[i-1][-100:] # 前100字符 # 添加后续上下文（下一个chunk的开头） suffix_context = &quot;&quot; if i &lt; len(base_chunks) - 1: suffix_context = base_chunks[i+1][:100] # 后100字符 # 构建增强chunk contextual_chunk = &#123; &quot;core_content&quot;: chunk, &quot;prefix_context&quot;: prefix_context, &quot;suffix_context&quot;: suffix_context, &quot;metadata&quot;: &#123; &quot;doc_title&quot;: doc_title, &quot;doc_summary&quot;: doc_summary, &quot;section&quot;: extract_section_title(chunk), &quot;chunk_index&quot;: i, &quot;total_chunks&quot;: len(base_chunks) &#125; &#125; # 用于向量化的完整内容 contextual_chunk[&quot;full_content&quot;] = ( f&quot;文档：&#123;doc_title&#125;\\n&quot; f&quot;章节：&#123;contextual_chunk[&#x27;metadata&#x27;][&#x27;section&#x27;]&#125;\\n&quot; f&quot;&#123;prefix_context&#125;\\n&quot; f&quot;&#123;chunk&#125;\\n&quot; f&quot;&#123;suffix_context&#125;&quot; ) chunks.append(contextual_chunk) return chunks 提升向量召回率多向量检索策略（Hybrid Search）结合多种检索方法提升召回率： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879from typing import List, Dictimport numpy as npclass HybridRetriever: &quot;&quot;&quot; 混合检索器：结合密集向量、稀疏向量和关键词检索 &quot;&quot;&quot; def __init__(self, dense_model, sparse_model, keyword_index): self.dense_model = dense_model # 密集向量模型（如BERT） self.sparse_model = sparse_model # 稀疏向量模型（如BM25） self.keyword_index = keyword_index # 关键词索引（如Elasticsearch） def retrieve(self, query: str, top_k: int = 10) -&gt; List[Dict]: &quot;&quot;&quot; 混合检索流程 Args: query: 用户查询 top_k: 返回结果数量 Returns: 检索结果列表 &quot;&quot;&quot; # 1. 密集向量检索（语义相似度） dense_results = self.dense_search(query, top_k=top_k*2) # 2. 稀疏向量检索（词汇匹配） sparse_results = self.sparse_search(query, top_k=top_k*2) # 3. 关键词检索（精确匹配） keyword_results = self.keyword_search(query, top_k=top_k*2) # 4. 结果融合与重排序 merged_results = self.merge_and_rerank( dense_results, sparse_results, keyword_results, weights=&#123;&#x27;dense&#x27;: 0.5, &#x27;sparse&#x27;: 0.3, &#x27;keyword&#x27;: 0.2&#125; ) return merged_results[:top_k] def merge_and_rerank(self, dense_results, sparse_results, keyword_results, weights): &quot;&quot;&quot; 结果融合和重排序（Reciprocal Rank Fusion） &quot;&quot;&quot; # 使用RRF算法融合多个检索结果 score_dict = &#123;&#125; k = 60 # RRF参数 # 计算每个结果的融合得分 for rank, result in enumerate(dense_results, 1): doc_id = result[&#x27;id&#x27;] if doc_id not in score_dict: score_dict[doc_id] = &#123;&#x27;doc&#x27;: result, &#x27;score&#x27;: 0&#125; score_dict[doc_id][&#x27;score&#x27;] += weights[&#x27;dense&#x27;] / (k + rank) for rank, result in enumerate(sparse_results, 1): doc_id = result[&#x27;id&#x27;] if doc_id not in score_dict: score_dict[doc_id] = &#123;&#x27;doc&#x27;: result, &#x27;score&#x27;: 0&#125; score_dict[doc_id][&#x27;score&#x27;] += weights[&#x27;sparse&#x27;] / (k + rank) for rank, result in enumerate(keyword_results, 1): doc_id = result[&#x27;id&#x27;] if doc_id not in score_dict: score_dict[doc_id] = &#123;&#x27;doc&#x27;: result, &#x27;score&#x27;: 0&#125; score_dict[doc_id][&#x27;score&#x27;] += weights[&#x27;keyword&#x27;] / (k + rank) # 按融合得分排序 ranked_results = sorted( score_dict.values(), key=lambda x: x[&#x27;score&#x27;], reverse=True ) return [item[&#x27;doc&#x27;] for item in ranked_results] 查询改写与扩展（Query Rewriting）通过改写和扩展查询提升召回率： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081class QueryExpander: &quot;&quot;&quot; 查询扩展器：通过多种策略扩展原始查询 &quot;&quot;&quot; def __init__(self, llm_client, synonym_dict): self.llm_client = llm_client self.synonym_dict = synonym_dict def expand_query(self, original_query: str) -&gt; List[str]: &quot;&quot;&quot; 生成多个查询变体 Returns: 包含原始查询和多个扩展查询的列表 &quot;&quot;&quot; expanded_queries = [original_query] # 1. 同义词扩展 synonym_query = self.add_synonyms(original_query) if synonym_query != original_query: expanded_queries.append(synonym_query) # 2. LLM改写（生成更正式/专业的表达） rewritten_query = self.llm_rewrite(original_query) expanded_queries.append(rewritten_query) # 3. 子查询拆解（针对复杂问题） if self.is_complex_query(original_query): sub_queries = self.decompose_query(original_query) expanded_queries.extend(sub_queries) # 4. 假设性文档生成（HyDE） hypothetical_doc = self.generate_hypothetical_document(original_query) expanded_queries.append(hypothetical_doc) return expanded_queries def llm_rewrite(self, query: str) -&gt; str: &quot;&quot;&quot; 使用LLM改写查询为更专业的表达 &quot;&quot;&quot; prompt = f&quot;&quot;&quot; 将以下用户问题改写为更专业、准确的技术查询： 原始问题：&#123;query&#125; 改写后的查询（只返回改写结果，不要其他说明）： &quot;&quot;&quot; rewritten = self.llm_client.generate(prompt) return rewritten.strip() def generate_hypothetical_document(self, query: str) -&gt; str: &quot;&quot;&quot; HyDE策略：生成假设性文档片段用于检索 &quot;&quot;&quot; prompt = f&quot;&quot;&quot; 假设你要回答这个问题：&#123;query&#125; 请生成一段包含答案的文档片段（2-3句话，包含关键术语）： &quot;&quot;&quot; hypothetical_doc = self.llm_client.generate(prompt) return hypothetical_doc.strip() def decompose_query(self, query: str) -&gt; List[str]: &quot;&quot;&quot; 将复杂查询拆解为多个子查询 &quot;&quot;&quot; prompt = f&quot;&quot;&quot; 将以下复杂问题拆解为2-4个简单的子问题： 原始问题：&#123;query&#125; 子问题列表（每行一个）： &quot;&quot;&quot; response = self.llm_client.generate(prompt) sub_queries = [q.strip() for q in response.split(&#x27;\\n&#x27;) if q.strip()] return sub_queries 使用查询扩展进行检索： 123456789101112131415161718def multi_query_retrieval(query: str, retriever, query_expander, top_k: int = 10): &quot;&quot;&quot; 使用多个查询变体进行检索，提升召回率 &quot;&quot;&quot; # 1. 生成查询变体 expanded_queries = query_expander.expand_query(query) # 2. 对每个查询变体分别检索 all_results = [] for exp_query in expanded_queries: results = retriever.retrieve(exp_query, top_k=top_k) all_results.extend(results) # 3. 去重和重排序 unique_results = deduplicate_results(all_results) reranked_results = rerank_by_relevance(query, unique_results) return reranked_results[:top_k] 精调Embedding模型针对特定领域fine-tune embedding模型： 12345678910111213141516171819202122232425262728293031323334353637383940from sentence_transformers import SentenceTransformer, InputExample, lossesfrom torch.utils.data import DataLoaderdef fine_tune_embedding_model(base_model_name: str, training_data: List[Dict], output_path: str): &quot;&quot;&quot; Fine-tune embedding模型以提升领域适配性 Args: base_model_name: 基础模型名称 training_data: 训练数据，格式为 [&#123;&#x27;query&#x27;: &#x27;...&#x27;, &#x27;positive&#x27;: &#x27;...&#x27;, &#x27;negative&#x27;: &#x27;...&#x27;&#125;] output_path: 模型保存路径 &quot;&quot;&quot; # 1. 加载基础模型 model = SentenceTransformer(base_model_name) # 2. 准备训练样本（三元组：query, positive, negative） train_examples = [] for item in training_data: train_examples.append(InputExample( texts=[item[&#x27;query&#x27;], item[&#x27;positive&#x27;], item[&#x27;negative&#x27;]] )) # 3. 创建DataLoader train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16) # 4. 定义损失函数（MultipleNegativesRankingLoss） train_loss = losses.MultipleNegativesRankingLoss(model) # 5. 训练模型 model.fit( train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=100, output_path=output_path, show_progress_bar=True ) return model 构建训练数据集： 12345678910111213141516171819202122def build_training_dataset(knowledge_base, queries): &quot;&quot;&quot; 从知识库和查询日志构建训练数据 &quot;&quot;&quot; training_data = [] for query in queries: # 获取人工标注的正样本（相关文档） positive_docs = get_relevant_docs(query, knowledge_base) # 负采样：从知识库中随机选择不相关文档 negative_docs = random_sample_negatives(knowledge_base, positive_docs) for pos_doc in positive_docs: for neg_doc in negative_docs: training_data.append(&#123; &#x27;query&#x27;: query, &#x27;positive&#x27;: pos_doc, &#x27;negative&#x27;: neg_doc &#125;) return training_data 完善知识覆盖建立索引质量评估体系123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141class IndexQualityAnalyzer: &quot;&quot;&quot; 索引质量分析器 &quot;&quot;&quot; def __init__(self, vector_db, embedding_model): self.vector_db = vector_db self.embedding_model = embedding_model def analyze_coverage(self, test_queries: List[str]) -&gt; Dict: &quot;&quot;&quot; 分析知识覆盖度 Args: test_queries: 测试查询集（应涵盖业务关键场景） Returns: 覆盖度分析报告 &quot;&quot;&quot; results = &#123; &#x27;total_queries&#x27;: len(test_queries), &#x27;failed_queries&#x27;: [], &#x27;low_confidence_queries&#x27;: [], &#x27;coverage_score&#x27;: 0, &#x27;missing_topics&#x27;: [] &#125; successful_retrievals = 0 for query in test_queries: # 执行检索 retrieved_docs = self.vector_db.search(query, top_k=5) if not retrieved_docs: results[&#x27;failed_queries&#x27;].append(query) elif max([doc[&#x27;score&#x27;] for doc in retrieved_docs]) &lt; 0.6: results[&#x27;low_confidence_queries&#x27;].append(&#123; &#x27;query&#x27;: query, &#x27;max_score&#x27;: max([doc[&#x27;score&#x27;] for doc in retrieved_docs]) &#125;) else: successful_retrievals += 1 # 计算覆盖度得分 results[&#x27;coverage_score&#x27;] = successful_retrievals / len(test_queries) # 分析缺失主题 results[&#x27;missing_topics&#x27;] = self.identify_missing_topics( results[&#x27;failed_queries&#x27;] ) return results def analyze_chunk_quality(self) -&gt; Dict: &quot;&quot;&quot; 分析chunk质量 &quot;&quot;&quot; all_chunks = self.vector_db.get_all_chunks() quality_metrics = &#123; &#x27;avg_chunk_length&#x27;: 0, &#x27;chunks_too_short&#x27;: 0, &#x27;chunks_too_long&#x27;: 0, &#x27;incomplete_chunks&#x27;: [], &#x27;duplicate_chunks&#x27;: [] &#125; chunk_lengths = [] chunk_hashes = &#123;&#125; for chunk in all_chunks: content = chunk[&#x27;content&#x27;] length = len(content) chunk_lengths.append(length) # 检测过短chunk if length &lt; 50: quality_metrics[&#x27;chunks_too_short&#x27;] += 1 # 检测过长chunk if length &gt; 2000: quality_metrics[&#x27;chunks_too_long&#x27;] += 1 # 检测不完整chunk（以标点符号结尾判断） if not content.rstrip().endswith((&#x27;.&#x27;, &#x27;。&#x27;, &#x27;!&#x27;, &#x27;！&#x27;, &#x27;?&#x27;, &#x27;？&#x27;)): quality_metrics[&#x27;incomplete_chunks&#x27;].append(chunk[&#x27;id&#x27;]) # 检测重复chunk content_hash = hash(content) if content_hash in chunk_hashes: quality_metrics[&#x27;duplicate_chunks&#x27;].append(&#123; &#x27;chunk1&#x27;: chunk_hashes[content_hash], &#x27;chunk2&#x27;: chunk[&#x27;id&#x27;] &#125;) else: chunk_hashes[content_hash] = chunk[&#x27;id&#x27;] quality_metrics[&#x27;avg_chunk_length&#x27;] = sum(chunk_lengths) / len(chunk_lengths) return quality_metrics def generate_quality_report(self, test_queries: List[str]) -&gt; str: &quot;&quot;&quot; 生成完整的质量报告 &quot;&quot;&quot; coverage_analysis = self.analyze_coverage(test_queries) chunk_analysis = self.analyze_chunk_quality() report = f&quot;&quot;&quot; ===== RAG索引质量报告 ===== 【覆盖度分析】 - 总查询数: &#123;coverage_analysis[&#x27;total_queries&#x27;]&#125; - 覆盖度得分: &#123;coverage_analysis[&#x27;coverage_score&#x27;]:.2%&#125; - 检索失败查询数: &#123;len(coverage_analysis[&#x27;failed_queries&#x27;])&#125; - 低置信度查询数: &#123;len(coverage_analysis[&#x27;low_confidence_queries&#x27;])&#125; 【缺失主题】 &#123;&#x27;, &#x27;.join(coverage_analysis[&#x27;missing_topics&#x27;])&#125; 【Chunk质量分析】 - 平均chunk长度: &#123;chunk_analysis[&#x27;avg_chunk_length&#x27;]:.0f&#125; 字符 - 过短chunk数: &#123;chunk_analysis[&#x27;chunks_too_short&#x27;]&#125; - 过长chunk数: &#123;chunk_analysis[&#x27;chunks_too_long&#x27;]&#125; - 不完整chunk数: &#123;len(chunk_analysis[&#x27;incomplete_chunks&#x27;])&#125; - 重复chunk数: &#123;len(chunk_analysis[&#x27;duplicate_chunks&#x27;])&#125; 【改进建议】 &quot;&quot;&quot; # 生成改进建议 if coverage_analysis[&#x27;coverage_score&#x27;] &lt; 0.7: report += &quot;\\n⚠️ 覆盖度较低，建议补充知识库内容&quot; if chunk_analysis[&#x27;chunks_too_short&#x27;] &gt; len(chunk_analysis) * 0.1: report += &quot;\\n⚠️ 过短chunk较多，建议调整切片策略&quot; if len(chunk_analysis[&#x27;duplicate_chunks&#x27;]) &gt; 0: report += &quot;\\n⚠️ 存在重复chunk，建议进行去重处理&quot; return report 知识库增量更新机制12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849class IncrementalIndexUpdater: &quot;&quot;&quot; 知识库增量更新管理器 &quot;&quot;&quot; def __init__(self, vector_db, doc_processor, change_detector): self.vector_db = vector_db self.doc_processor = doc_processor self.change_detector = change_detector def detect_changes(self, doc_source_path: str) -&gt; Dict: &quot;&quot;&quot; 检测文档变更 Returns: &#123;&#x27;added&#x27;: [], &#x27;modified&#x27;: [], &#x27;deleted&#x27;: []&#125; &quot;&quot;&quot; return self.change_detector.detect(doc_source_path) def incremental_update(self, changes: Dict): &quot;&quot;&quot; 增量更新索引 &quot;&quot;&quot; # 1. 处理新增文档 for new_doc in changes[&#x27;added&#x27;]: print(f&quot;处理新增文档: &#123;new_doc[&#x27;path&#x27;]&#125;&quot;) chunks = self.doc_processor.process_document(new_doc) self.vector_db.add_chunks(chunks) # 2. 处理修改文档 for modified_doc in changes[&#x27;modified&#x27;]: print(f&quot;处理修改文档: &#123;modified_doc[&#x27;path&#x27;]&#125;&quot;) # 删除旧版本 self.vector_db.delete_by_doc_id(modified_doc[&#x27;id&#x27;]) # 添加新版本 chunks = self.doc_processor.process_document(modified_doc) self.vector_db.add_chunks(chunks) # 3. 处理删除文档 for deleted_doc in changes[&#x27;deleted&#x27;]: print(f&quot;处理删除文档: &#123;deleted_doc[&#x27;path&#x27;]&#125;&quot;) self.vector_db.delete_by_doc_id(deleted_doc[&#x27;id&#x27;]) # 4. 更新元数据 self.vector_db.update_metadata(&#123; &#x27;last_update&#x27;: datetime.now().isoformat(), &#x27;total_documents&#x27;: self.vector_db.count_documents(), &#x27;total_chunks&#x27;: self.vector_db.count_chunks() &#125;) 元数据增强为每个chunk添加丰富的元数据： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051def create_enhanced_chunk_with_metadata(chunk_content: str, document: Dict, chunk_index: int) -&gt; Dict: &quot;&quot;&quot; 创建包含丰富元数据的chunk &quot;&quot;&quot; return &#123; # 核心内容 &#x27;content&#x27;: chunk_content, # 文档元数据 &#x27;doc_metadata&#x27;: &#123; &#x27;doc_id&#x27;: document[&#x27;id&#x27;], &#x27;doc_title&#x27;: document[&#x27;title&#x27;], &#x27;doc_path&#x27;: document[&#x27;path&#x27;], &#x27;doc_type&#x27;: document[&#x27;type&#x27;], # PDF, DOCX, etc. &#x27;doc_author&#x27;: document.get(&#x27;author&#x27;), &#x27;doc_created_date&#x27;: document.get(&#x27;created_date&#x27;), &#x27;doc_modified_date&#x27;: document.get(&#x27;modified_date&#x27;), &#x27;doc_version&#x27;: document.get(&#x27;version&#x27;), &#x27;doc_category&#x27;: document.get(&#x27;category&#x27;), # 产品文档、技术规范、FAQ等 &#x27;doc_tags&#x27;: document.get(&#x27;tags&#x27;, []) &#125;, # Chunk元数据 &#x27;chunk_metadata&#x27;: &#123; &#x27;chunk_id&#x27;: generate_chunk_id(document[&#x27;id&#x27;], chunk_index), &#x27;chunk_index&#x27;: chunk_index, &#x27;chunk_type&#x27;: detect_chunk_type(chunk_content), # text, table, code, list &#x27;section_title&#x27;: extract_section_title(chunk_content), &#x27;section_level&#x27;: extract_section_level(chunk_content), &#x27;page_number&#x27;: extract_page_number(chunk_content, document), &#x27;keywords&#x27;: extract_keywords(chunk_content), &#x27;entities&#x27;: extract_entities(chunk_content), # NER提取的实体 &#125;, # 关系元数据 &#x27;relationship&#x27;: &#123; &#x27;previous_chunk_id&#x27;: get_previous_chunk_id(chunk_index) if chunk_index &gt; 0 else None, &#x27;next_chunk_id&#x27;: get_next_chunk_id(chunk_index), &#x27;related_chunks&#x27;: find_related_chunks(chunk_content), # 语义相关的其他chunks &#x27;parent_section&#x27;: extract_parent_section(chunk_content), &#125;, # 质量元数据 &#x27;quality&#x27;: &#123; &#x27;completeness_score&#x27;: calculate_completeness(chunk_content), &#x27;readability_score&#x27;: calculate_readability(chunk_content), &#x27;information_density&#x27;: calculate_info_density(chunk_content), &#125; &#125; 利用元数据进行精准过滤： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364def metadata_filtered_retrieval(query: str, vector_db, filters: Dict = None, top_k: int = 10) -&gt; List[Dict]: &quot;&quot;&quot; 结合元数据过滤的检索 Args: query: 用户查询 vector_db: 向量数据库 filters: 元数据过滤条件 top_k: 返回结果数 Returns: 过滤后的检索结果 &quot;&quot;&quot; # 1. 向量检索（先召回更多候选） candidates = vector_db.search(query, top_k=top_k*3) # 2. 应用元数据过滤 if filters: filtered_candidates = [] for candidate in candidates: if match_filters(candidate[&#x27;metadata&#x27;], filters): filtered_candidates.append(candidate) candidates = filtered_candidates # 3. 返回top_k结果 return candidates[:top_k]def match_filters(metadata: Dict, filters: Dict) -&gt; bool: &quot;&quot;&quot; 检查元数据是否匹配过滤条件 &quot;&quot;&quot; for key, value in filters.items(): if key == &#x27;doc_type&#x27; and metadata[&#x27;doc_metadata&#x27;][&#x27;doc_type&#x27;] != value: return False if key == &#x27;date_after&#x27; and metadata[&#x27;doc_metadata&#x27;][&#x27;doc_created_date&#x27;] &lt; value: return False if key == &#x27;date_before&#x27; and metadata[&#x27;doc_metadata&#x27;][&#x27;doc_created_date&#x27;] &gt; value: return False if key == &#x27;category&#x27; and metadata[&#x27;doc_metadata&#x27;][&#x27;doc_category&#x27;] != value: return False if key == &#x27;tags&#x27; and not any(tag in metadata[&#x27;doc_metadata&#x27;][&#x27;doc_tags&#x27;] for tag in value): return False return True# 使用示例results = metadata_filtered_retrieval( query=&quot;如何配置SSL证书？&quot;, vector_db=vector_db, filters=&#123; &#x27;doc_type&#x27;: &#x27;PDF&#x27;, &#x27;category&#x27;: &#x27;技术文档&#x27;, &#x27;date_after&#x27;: &#x27;2024-01-01&#x27;, &#x27;tags&#x27;: [&#x27;安全&#x27;, &#x27;配置&#x27;] &#125;, top_k=10) 综合优化实践案例完整的优化Pipeline123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111class OptimizedRAGPipeline: &quot;&quot;&quot; 优化的RAG检索Pipeline &quot;&quot;&quot; def __init__(self): # 初始化各个组件 self.doc_processor = DocumentProcessor() self.query_expander = QueryExpander() self.hybrid_retriever = HybridRetriever() self.reranker = CrossEncoderReranker() self.quality_analyzer = IndexQualityAnalyzer() def process_documents(self, documents: List[Dict]) -&gt; List[Dict]: &quot;&quot;&quot; 文档处理流程 &quot;&quot;&quot; all_chunks = [] for doc in documents: # 1. 解析文档 parsed_content = self.doc_processor.parse(doc) # 2. 智能切片 chunks = self.doc_processor.adaptive_chunking( parsed_content, complexity_score=calculate_complexity(parsed_content) ) # 3. 添加上下文和元数据 enhanced_chunks = [ create_enhanced_chunk_with_metadata(chunk, doc, idx) for idx, chunk in enumerate(chunks) ] all_chunks.extend(enhanced_chunks) return all_chunks def retrieve(self, query: str, top_k: int = 10) -&gt; List[Dict]: &quot;&quot;&quot; 优化的检索流程 &quot;&quot;&quot; # 1. 查询扩展 expanded_queries = self.query_expander.expand_query(query) # 2. 多向量混合检索 all_results = [] for exp_query in expanded_queries: results = self.hybrid_retriever.retrieve(exp_query, top_k=top_k*2) all_results.extend(results) # 3. 去重 unique_results = deduplicate_results(all_results) # 4. 重排序（使用Cross-Encoder） reranked_results = self.reranker.rerank(query, unique_results) # 5. 补充相关chunk（基于元数据关系） final_results = self.supplement_related_chunks( reranked_results[:top_k] ) return final_results def supplement_related_chunks(self, results: List[Dict]) -&gt; List[Dict]: &quot;&quot;&quot; 补充相关chunk，解决上下文割裂问题 &quot;&quot;&quot; supplemented = [] for result in results: supplemented.append(result) # 如果chunk不完整，补充前后chunk if result[&#x27;metadata&#x27;][&#x27;chunk_metadata&#x27;][&#x27;section_level&#x27;] &gt; 1: # 获取前一个chunk prev_chunk_id = result[&#x27;metadata&#x27;][&#x27;relationship&#x27;][&#x27;previous_chunk_id&#x27;] if prev_chunk_id: prev_chunk = self.vector_db.get_by_id(prev_chunk_id) supplemented.insert(-1, prev_chunk) # 获取后一个chunk next_chunk_id = result[&#x27;metadata&#x27;][&#x27;relationship&#x27;][&#x27;next_chunk_id&#x27;] if next_chunk_id: next_chunk = self.vector_db.get_by_id(next_chunk_id) supplemented.append(next_chunk) return supplemented def evaluate_and_improve(self, test_queries: List[str]): &quot;&quot;&quot; 评估和持续改进 &quot;&quot;&quot; # 1. 生成质量报告 report = self.quality_analyzer.generate_quality_report(test_queries) print(report) # 2. 识别问题查询 failed_queries = self.quality_analyzer.get_failed_queries() # 3. 分析失败原因 for query in failed_queries: analysis = self.analyze_failure(query) print(f&quot;查询: &#123;query&#125;&quot;) print(f&quot;失败原因: &#123;analysis[&#x27;reason&#x27;]&#125;&quot;) print(f&quot;建议措施: &#123;analysis[&#x27;recommendation&#x27;]&#125;&quot;) # 4. 自动优化建议 optimization_plan = self.generate_optimization_plan(report) return optimization_plan 监控和持续优化建立监控体系，持续追踪系统表现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class RAGMonitor: &quot;&quot;&quot; RAG系统监控器 &quot;&quot;&quot; def __init__(self, vector_db, logging_db): self.vector_db = vector_db self.logging_db = logging_db def log_retrieval(self, query: str, results: List[Dict], user_feedback: str = None): &quot;&quot;&quot; 记录每次检索 &quot;&quot;&quot; log_entry = &#123; &#x27;timestamp&#x27;: datetime.now().isoformat(), &#x27;query&#x27;: query, &#x27;results_count&#x27;: len(results), &#x27;top_scores&#x27;: [r[&#x27;score&#x27;] for r in results[:3]], &#x27;user_feedback&#x27;: user_feedback, &#x27;latency_ms&#x27;: results[0].get(&#x27;latency_ms&#x27;) &#125; self.logging_db.insert(log_entry) def generate_daily_report(self) -&gt; Dict: &quot;&quot;&quot; 生成每日监控报告 &quot;&quot;&quot; today_logs = self.logging_db.get_today_logs() report = &#123; &#x27;date&#x27;: datetime.now().date().isoformat(), &#x27;total_queries&#x27;: len(today_logs), &#x27;avg_latency&#x27;: np.mean([log[&#x27;latency_ms&#x27;] for log in today_logs]), &#x27;zero_result_rate&#x27;: len([log for log in today_logs if log[&#x27;results_count&#x27;] == 0]) / len(today_logs), &#x27;low_confidence_rate&#x27;: len([log for log in today_logs if max(log[&#x27;top_scores&#x27;]) &lt; 0.6]) / len(today_logs), &#x27;negative_feedback_rate&#x27;: len([log for log in today_logs if log[&#x27;user_feedback&#x27;] == &#x27;negative&#x27;]) / len(today_logs), &#x27;top_failed_queries&#x27;: self.get_top_failed_queries(today_logs) &#125; # 告警 if report[&#x27;zero_result_rate&#x27;] &gt; 0.1: self.send_alert(&quot;零结果率过高&quot;, report) if report[&#x27;negative_feedback_rate&#x27;] &gt; 0.2: self.send_alert(&quot;负面反馈率过高&quot;, report) return report def identify_knowledge_gaps(self, time_window_days: int = 7) -&gt; List[str]: &quot;&quot;&quot; 识别知识盲区 &quot;&quot;&quot; recent_logs = self.logging_db.get_recent_logs(time_window_days) # 找出经常检索失败的查询 failed_queries = [ log[&#x27;query&#x27;] for log in recent_logs if log[&#x27;results_count&#x27;] == 0 or max(log[&#x27;top_scores&#x27;]) &lt; 0.5 ] # 聚类相似查询，识别知识盲区主题 knowledge_gaps = self.cluster_queries(failed_queries) return knowledge_gaps 最佳实践总结切片策略最佳实践 根据内容类型选择策略 技术文档：500-800 tokens，20% overlap FAQ：100-200 tokens，10% overlap 长篇报告：800-1000 tokens，15% overlap 保持语义完整性 优先在段落、章节等自然边界切分 对表格、代码、列表等结构化内容保持完整性 添加上下文窗口（前后各50-100 tokens） 添加元数据 文档标题、章节标题、页码 文档类型、创建日期、作者 前后chunk的引用关系 检索优化最佳实践 采用混合检索 密集向量检索（语义相似度） 稀疏向量检索（关键词匹配） 元数据过滤（时间、类型、标签） 查询优化 同义词扩展 LLM查询改写 复杂查询拆解 HyDE假设性文档生成 两阶段检索+重排序 第一阶段：广召回（top_k × 3） 第二阶段：Cross-Encoder精确重排序 知识库管理最佳实践 建立质量评估体系 定期运行覆盖度测试 监控chunk质量指标 追踪用户反馈和失败查询 持续更新机制 增量更新而非全量重建 版本控制和回滚能力 自动检测文档变更 监控和优化 实时监控检索性能 分析失败查询，识别知识盲区 A&#x2F;B测试不同优化策略的效果 总结RAG系统的内容缺失问题是一个系统性工程，需要从切片策略、检索算法、知识库管理三个维度综合优化： 切片优化：采用语义感知的智能切片，保持内容完整性，添加上下文信息 检索优化：使用混合检索策略，结合查询扩展和重排序，提升召回率和准确率 知识库优化：建立质量评估体系，完善元数据，实现增量更新和持续监控 只有将这三个方面结合起来，构建完整的优化pipeline，才能从根本上解决RAG系统的内容缺失问题，为用户提供准确、完整、可靠的检索结果。 在实际应用中，需要根据具体业务场景和数据特点，不断迭代和调整优化策略，通过A&#x2F;B测试和用户反馈持续改进系统性能。","categories":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/tags/AI/"},{"name":"RAG","slug":"RAG","permalink":"https://www.silenceboy.com/tags/RAG/"}]},{"title":"企业私有知识库RAG系统构建挑战","slug":"企业私有知识库RAG系统构建挑战","date":"2025-11-07T02:43:04.000Z","updated":"2026-01-22T03:43:27.265Z","comments":true,"path":"2025/11/07/企业私有知识库RAG系统构建挑战/","permalink":"https://www.silenceboy.com/2025/11/07/%E4%BC%81%E4%B8%9A%E7%A7%81%E6%9C%89%E7%9F%A5%E8%AF%86%E5%BA%93RAG%E7%B3%BB%E7%BB%9F%E6%9E%84%E5%BB%BA%E6%8C%91%E6%88%98/","excerpt":"","text":"构建企业私有知识库的检索增强生成（RAG）系统面临着多重挑战，这些困难主要集中在源数据的质量和结构、复杂的数据预处理流程以及生产级系统的架构和性能等方面。 一、 源数据（私有文档）的质量与结构挑战企业私有知识库通常包含面向人类阅读的现有文档，其格式和写作风格往往不符合 RAG 系统的优化要求，导致检索器性能不佳。 缺乏结构化格式和元数据： 原始文档可能缺少清晰的章节标题、副标题或元数据，这使得 RAG 模型难以识别和提取相关信息。 例如，没有明确标题的长文档会使确定特定信息的上下文变得困难。高质量的 RAG 需要生成并存储元数据（如文件名、URLs、作者、章节标题、页码等），以实现更快、更高效的数据检索。 语言非正式且不一致： 文档可能包含非正式语言或不一致的术语。 最常见的问题是未定义或法学硕士（LLMs）未知的缩写，这可能会混淆 RAG 模型。 LLMs 接受了海量互联网数据的训练，但缺乏企业内部文档的上下文，因此必须设置上下文、定义缩写并避免使用或定义公司特定的术语。 冗余、冗长和歧义： 原始文档可能过于冗长，包含不必要或重复的信息，使 RAG 模型不堪重负，导致响应不够简洁和相关。 文档中可能包含模棱两可的术语或短语，可能有多种解释，从而导致 RAG 模型误解和不准确的响应。 注入图形和超链接元素： 包含图形和超链接（URLs）的原始文档虽然适合人类使用，但这些元素可能会消耗检索令牌限制，导致后续段落中的关键信息丢失或摘录不完整。 缺乏特定领域的知识或上下文： 文档可能缺乏生成准确响应所需的特定领域知识或上下文，这限制了 RAG 模型生成相关且准确响应的能力。 二、 数据预处理和管道工程挑战将企业非结构化数据转化为可用于 RAG 的高质量向量索引，需要一个复杂且精心策划的数据管道。 文件格式的多样性与复杂性： 企业级数据来源广泛，格式多样，包括但不限于 PDF、Word、Excel、PPT 等。每种格式都有其特定的结构和内容表示方式，给解析工作带来了挑战。 内容复杂性：文档内容可能包含文本、图像、表格、公式等多种元素，这些元素的解析和提取需要不同的技术和方法。 非结构化数据的解析难度： 对于 PDF 和扫描图像 等非结构化数据，信息以视觉化方式呈现，解析难度大，通常需要借助 OCR（光学字符识别）和文档布局检测（DLD）等技术进行预处理和格式信息提取。 表格信息难以解释：RAG 模型可能难以解释表格，因为这需要对二维结构的理解。建议用多级项目符号列表或扁平语法格式化表格信息，以方便模型处理。 分块策略（Chunking）的优化： 将大型文档分解为较小、可管理的部分（分块）是 RAG 工作流中的关键预处理步骤。 糟糕的分块 会导致检索结果不相关、效率低下并降低业务价值。 没有“一刀切”的分块方法。最佳的分块大小和策略（如固定大小分块、段落分块或语义分块）取决于具体的用例和数据性质，需要进行迭代和实验。 数据去重与过滤： 由于数据源可能来自多个共享驱动器，可能会出现重复或近似重复的文档，如果这些冗余块保留在最终索引中，会降低应用程序的性能。 必须进行过滤，以消除与 RAG 目的不相关、太旧、不可靠或包含敏感信息（如个人身份信息 PII）的文档。 三、 生产级系统架构与性能挑战企业 RAG 系统必须具备鲁棒性、可扩展性和高度的准确性，以应对复杂的生产环境和用户查询。 处理复杂的多跳查询： 简单的 RAG 架构（线性、一次性流程）无法解决需要推理、反思和多步骤决策的复杂查询。 真正的企业应用往往涉及多源知识的整合，需要系统能够先从内部文档中识别事实，再通过外部搜索（如 Web 搜索）查找最新信息，最后将二者综合分析，进行多跳推理。这需要构建复杂的 Agentic RAG 流水线来管理状态和控制流。 检索精度和效率： 生产级 RAG 需要采用多阶段检索漏斗（先广召回，再高精度重排）来确保检索结果的质量。 需要在召回率（Recall）（确保找到所有相关信息）和精度（Precision）（确保检索到的文档中相关信息占比高）之间取得平衡。 基础设施和可扩展性： RAG 的核心引擎是向量数据库，它必须针对快速查询进行优化，能够处理数百万个向量，支持分布式索引和分片，以确保低延迟和高吞吐量。 在向量数据库的选择上，需要平衡性能、可扩展性、集成兼容性、成本和基础设施（云原生或自托管）等多个关键因素。 安全性和合规性： 由于处理的是企业私有知识库，数据保护是首要任务。系统必须支持： 静态和传输中的加密。 基于角色的访问控制。 符合如 GDPR 或 HIPAA 等监管标准。 持续评估与迭代： 构建一个持续提供准确、相关响应的 RAG 系统非常困难，需要在开发和生产中持续进行评估，以区分细微的改进和系统崩溃。 传统的 NLP 指标（如 BLEU 和 ROUGE）无法检测到幻觉和上下文利用率，因此需要使用 RAG 三元组（上下文相关性、忠实性和答案相关性）等 RAG 专属指标，并结合人工评估，以确保结果与用户满意度相关。","categories":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/tags/AI/"},{"name":"RAG","slug":"RAG","permalink":"https://www.silenceboy.com/tags/RAG/"}]},{"title":"fidl和arxml互转教程","slug":"fidl和arxml互转教程","date":"2025-09-05T09:56:31.000Z","updated":"2025-09-05T10:12:20.000Z","comments":true,"path":"2025/09/05/fidl和arxml互转教程/","permalink":"https://www.silenceboy.com/2025/09/05/fidl%E5%92%8Carxml%E4%BA%92%E8%BD%AC%E6%95%99%E7%A8%8B/","excerpt":"","text":"fracon 命令行工具可以使用 maven 从源代码构建。本页介绍如何运行本地构建。 请注意，你的公司需要成为 AUTOSAR 组织的成员。如果不是会员，你将无法访问 artop.org，因此无法使用 FARACON 工具。 环境要求：java8 &#x2F; mvn3.x 注意：以下所有操作均在ubuntu20.04上进行。 如何从源码构建命令行工具源码下载源码地址：https://github.com/COVESA/franca_ara_tools 1$ git clone https://github.com/GENIVI/franca_ara_tools.git artop文件下载为了构建franca_ara_tools项目，需要下载artop提供的artop-Update-4.12.1文件。只有加入autosar会员的公司才能注册和登录artop站点：https://www.artop.org。 登录artop站点后，选择Downloads，然后选择All Downloads。 选择SDK 下拉找到artop-Update-4.12.1.zip 注意：目前只能使用4.12.1版本，其他版本无效。 artop-Update-4.12.1.zip解压到制定目录： 1$ unzip -d /home/test/franca/artop-Update-4.12.1 artop-Update-4.12.1.zip franca_ara_tools源码修改进入下载的franca_ara_tools项目目录，然后执行以下操作： 修改releng/org.genivi.faracon.parent/pom.xml文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253+++ b/releng/org.genivi.faracon.parent/pom.xml@@ -401,6 +401,16 @@ &lt;groupId&gt;org.eclipse.jdt&lt;/groupId&gt; &lt;artifactId&gt;org.eclipse.jdt.core&lt;/artifactId&gt; &lt;version&gt;$&#123;jdt-core-version&#125;&lt;/version&gt;+ &lt;exclusions&gt;+ &lt;exclusion&gt;+ &lt;groupId&gt;org.eclipse.platform&lt;/groupId&gt;+ &lt;artifactId&gt;org.eclipse.core.runtime&lt;/artifactId&gt;+ &lt;/exclusion&gt;+ &lt;exclusion&gt;+ &lt;groupId&gt;org.eclipse.platform&lt;/groupId&gt;+ &lt;artifactId&gt;org.eclipse.equinox.common&lt;/artifactId&gt;+ &lt;/exclusion&gt;+ &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.jdt&lt;/groupId&gt;@@ -412,10 +422,36 @@ &lt;artifactId&gt;org.eclipse.jdt.compiler.tool&lt;/artifactId&gt; &lt;version&gt;$&#123;jdt-compiler-tool-version&#125;&lt;/version&gt; &lt;/dependency&gt;+ &lt;dependency&gt;+ &lt;groupId&gt;org.eclipse.platform&lt;/groupId&gt;+ &lt;artifactId&gt;org.eclipse.core.runtime&lt;/artifactId&gt;+ &lt;version&gt;3.12.0&lt;/version&gt;+ &lt;exclusions&gt;+ &lt;exclusion&gt;+ &lt;groupId&gt;org.eclipse.platform&lt;/groupId&gt;+ &lt;artifactId&gt;org.eclipse.equinox.common&lt;/artifactId&gt;+ &lt;/exclusion&gt;+ &lt;/exclusions&gt;+ &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.emf&lt;/groupId&gt; &lt;artifactId&gt;org.eclipse.emf.codegen&lt;/artifactId&gt; &lt;version&gt;$&#123;emf-codegen-version&#125;&lt;/version&gt;+ &lt;exclusions&gt;+ &lt;exclusion&gt;+ &lt;groupId&gt;org.eclipse.platform&lt;/groupId&gt;+ &lt;artifactId&gt;org.eclipse.core.runtime&lt;/artifactId&gt;+ &lt;/exclusion&gt;+ &lt;exclusion&gt;+ &lt;groupId&gt;org.eclipse.platform&lt;/groupId&gt;+ &lt;artifactId&gt;org.eclipse.equinox.common&lt;/artifactId&gt;+ &lt;/exclusion&gt;+ &lt;/exclusions&gt;+ &lt;/dependency&gt;+ &lt;dependency&gt;+ &lt;groupId&gt;org.eclipse.platform&lt;/groupId&gt;+ &lt;artifactId&gt;org.eclipse.equinox.common&lt;/artifactId&gt;+ &lt;version&gt;3.8.0&lt;/version&gt; &lt;/dependency&gt; 修改releng/org.genivi.faracon.target/fara-oxygen-artop.target文件，将location修改为本地artop文件目录：&lt;repository location=&quot;file:/home/test/franca/artop-Update-4.12.1&quot;/&gt; franca_ara_tools源码构建进入franca_ara_tools项目的releng/org.genivi.faracon.parent目录，执行以下命令： 1$ mvn clean install -Pwith-artop -Dtycho.disableP2Mirrors=true 等待构建完成，时间较长： 构建成功之后在franca_ara_tools项目下生成products目录，构建成果物在该目录下。 文件转换构建完成后在products/org.genivi.faracon.cli.product/target/products/org.genivi.faracon.cli.product目录下会生成各个平台的可执行文件，进入目录： 1$ cd products/org.genivi.faracon.cli.product/target/products/org.genivi.faracon.cli.product 由于我在ubuntu20.04下操作，将对应系统文件copy到指定目录： 1$ cp -a franca_ara_tools/products/org.genivi.faracon.cli.product/target/products/org.genivi.faracon.cli.product/linux/gtk/x86_64 $HOME/faracon 为了在任意地方使用faranca命令，设置环境变量：vim ~/.zshrc,添加以下内容： 1export PATH=$PATH:$HOME/faracon 命令行选项faracon-linux-x86_64 --help 将输出以下内容： 12345678910111213141516171819202122232425262728293031Command: Console HelpCommand: Franca ARA Converterusage: faracon [-a &lt;arg&gt;] [-c] [-ca] [-d &lt;arg&gt;] [-e] [-f &lt;arg&gt;] [-i &lt;arg&gt;] [-L &lt;arg&gt;] [-l &lt;arg&gt;] -a,--ara-to-franca &lt;arg&gt; Arxml file that will be converted to .fidl or directory what will be recursively scanned for .arxml files and each one of them will be converted to corresponding fidl ones. -c,--continue-on-errors Do not stop the tool execution when an error occurs. -ca,--check-arxml-files-only Checks the provided ARXML files. -conf,--f2aconfig &lt;arg&gt; Supply configuration file for command line related to Franca-to-arxml generation. -d,--dest &lt;arg&gt; Output directory for the generated files. -e,--warnings-as-errors Treat warnings as errors. -f,--franca-to-ara &lt;arg&gt; Franca file that will be converted to arxml or directory what will be recursively scanned for fidl files and each one of them will be converted to corresponding arxml ones. -i,--include &lt;arg&gt; Additions to classpath. -L,--license &lt;arg&gt; The file path to the license text that will be added to each generated file. -l,--log-level &lt;arg&gt; The log level (quiet or verbose).Command: Console Helpusage: faracon -h -h,--help Print out options of the tool.Command: Version Informationusage: faracon -v -v,--version Show version number of the tool. 转换前注意事项终于可以使用命令行工具了，激动的去试一试，但是发现在执行转换命令操作的时候遇到以下报错： 经过一番排查之后，找到了问题原因，修改$HOME/faracon/faracon-linux-x86_64.ini文件内容，删除--illegal-access=deny参数。 删除前： 删除后： 之后就可以按照以下步骤进行转换了。👦 fidl转arxml转换 /path/to/fidls 中的所有 fidl 文件，将输出存储在 /path/to/output 中，使用详细的日志记录级别，并在发生错误时继续翻译下一个 fidl： 转换fidl使用参数-f 1$ faracon-linux-x86_64 -f /path/to/fidls/ -d /path/to/output/ -l verbose -c 也可指定文件： 1$ faracon-linux-x86_64 -f /path/to/fidls/helloworld.fidl -d /path/to/output/ -l verbose -c arxml转fidl转换 /path/to/arxmls 中的所有 arxml 文件，将输出存储在 /path/to/output 中，使用详细的日志记录级别，并在发生错误时继续翻译下一个 arxml： 转换arxml使用参数-a 1$ faracon-linux-x86_64 -a /path/to/arxmls/ -d /path/to/output/ -l verbose -c 也可指定文件： 1$ faracon-linux-x86_64 -a /path/to/arxmls/helloworld.arxml -d /path/to/output/ -l verbose -c","categories":[{"name":"autosar","slug":"autosar","permalink":"https://www.silenceboy.com/categories/autosar/"}],"tags":[{"name":"fidl","slug":"fidl","permalink":"https://www.silenceboy.com/tags/fidl/"},{"name":"arxml","slug":"arxml","permalink":"https://www.silenceboy.com/tags/arxml/"}]},{"title":"AI编码工具对比分析","slug":"AI编码工具对比分析","date":"2025-09-05T09:16:49.000Z","updated":"2025-09-05T09:38:01.000Z","comments":true,"path":"2025/09/05/AI编码工具对比分析/","permalink":"https://www.silenceboy.com/2025/09/05/AI%E7%BC%96%E7%A0%81%E5%B7%A5%E5%85%B7%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90/","excerpt":"","text":"前言：模型决定下限，Agent决定上限在深入对比分析之前，我想分享一个重要观点：模型决定下限，Agent决定上限。 核心观察如果您使用过不同的AI IDE，可能会发现一个有趣的现象：即使都选择相同的Claude Sonnet 4模型，不同工具生成的代码质量差别往往很大。这种差异的根本原因在于，底层模型只是AI编码工具的基础能力边界，而真正决定实际效果的是各工具的Agent架构和工程实现。具体原因如下： 提示工程的差异 不同工具对相同模型使用不同的提示策略、上下文组织方式和任务分解逻辑 优秀的Agent会根据代码类型、项目结构、开发阶段动态调整提示策略 上下文管理的水平 如何选择、组织和传递代码上下文信息直接影响生成质量 项目级理解、跨文件依赖分析、代码库索引等能力差异巨大 工作流集成的深度 与开发环境、版本控制、测试框架的集成程度 多步骤任务的规划、执行和验证能力 反馈循环的设计 错误检测、自我修正、迭代优化的机制 从用户行为中学习和适应的能力 实践启示基于这一认知，我们在选择和使用AI编码工具时应该： 关注Agent能力而非仅仅模型版本 评估工具的项目理解能力、任务分解能力、上下文管理水平 测试在实际工作场景中的表现，而非单纯的基准测试分数 善用工具组合，发挥协同效应 不同工具在不同场景下各有优势，组合使用往往效果更佳 例如：用Claude Code做架构分析，用Cursor做功能开发，用Copilot做日常补全 持续优化使用策略 学习每个工具的最佳实践和高级功能 根据项目特点和团队需求调整工具配置和使用方式 拥抱Agent时代的编程范式 从”代码补全”思维转向”AI协作”思维 将AI工具视为编程伙伴，而非简单的自动完成工具 执行摘要本报告对当前主流的三款AI编码工具进行了深入分析对比：GitHub Copilot、Cursor和Claude Code。通过功能特性、性能表现、适用场景等多个维度的评估，为开发者和团队针对不同工作内容选择合适的AI编码工具提供决策依据。 关键发现： GitHub Copilot：最适合日常代码补全和多编辑器环境，稳定可靠 Cursor：最适合需要深度AI集成的现代化开发，项目级理解能力强 Claude Code：最适合复杂项目重构和大型代码库分析，上下文理解能力最强 核心建议： 快速开发和日常编码：选择GitHub Copilot 大型项目和团队协作：选择Cursor 复杂重构和架构分析：选择Claude Code 工具概述GitHub Copilot开发商： GitHub &amp; OpenAI发布时间： 2021年核心定位： AI代码补全助手 GitHub Copilot是首批商业化的AI编码工具之一，专注于提供高质量的代码补全和生成功能。基于OpenAI Codex模型训练，能够理解自然语言注释并生成相应代码。 Cursor开发商： Anysphere发布时间： 2023年核心定位： AI增强型集成开发环境 Cursor是基于VS Code构建的AI原生IDE，将AI功能深度集成到开发环境的每个角落，提供从代码补全到项目管理的全方位AI支持。 Claude Code开发商： Anthropic发布时间： 2024年核心定位： 终端AI编程助手 Claude Code是Anthropic推出的命令行AI编程工具，采用代理式架构，具备强大的自主决策能力和超大上下文理解能力。 功能特性对比代码补全能力 特性 GitHub Copilot Cursor Claude Code 补全类型 内联建议、函数生成 多行补全、智能Tab 整体代码生成 上下文理解 当前文件+光标附近 项目级理解 200K tokens超大上下文 预测准确性 高（稳定） 很高（智能） 极高（深度理解） 响应速度 快 快 中等 AI模型支持 工具 支持模型 GitHub Copilot GPT系列，Claude系列，Gemini系列等 Cursor GPT系列，Claude系列，Gemini系列等 Claude Code Claude 系列，通过模型网关，也可接入其他模型 编辑器集成 工具 支持编辑器 集成深度 GitHub Copilot VS Code, JetBrains, Vim, Neovim 插件形式 Cursor 内置（基于VS Code） 原生集成 Claude Code 终端 + VS Code&#x2F;JetBrains插件 命令行为主 详细功能分析GitHub Copilot优势 广泛的编辑器支持 支持VS Code、JetBrains全系列、Vim、Neovim等主流编辑器 提供一致的用户体验，降低学习成本 稳定的代码补全质量 基于大量开源代码训练，代码质量可靠 支持多种编程语言，包括主流和小众语言 深度生态系统与代理能力 与GitHub平台无缝集成（PR、Issue、Actions） 新增“代理模式”（Agentic Coding）：可根据项目上下文建议或执行跨文件变更与命令方案（以 VS Code 集成为主） 适合需要项目级改动规划与执行建议的场景 劣势 上下文理解局限 主要基于当前文件和光标附近代码 对跨文件依赖理解能力有限 难以处理复杂的项目架构 代码质量风险 可能生成包含逻辑错误的代码 不总是遵循最佳实践 需要开发者具备代码审查能力 隐私安全顾虑 代码片段需上传至云端处理 可能涉及知识产权泄露风险 企业级用户需要额外的安全配置 适用场景 日常代码补全需求：适合需要快速代码补全的开发者 多编辑器环境：适合使用多种编辑器的开发团队 GitHub深度用户：已深度使用GitHub生态的团队 预算敏感项目：寻求高性价比AI编码工具的个人开发者 Cursor优势 深度AI集成的IDE体验 AI功能原生集成到IDE的每个角落 提供聊天面板、快捷键触发等多种交互方式 支持AI驱动的代码重构和调试 多模型支持与灵活性 支持GPT系列、Claude系列、Gemini系列等多个模型 用户可根据任务需求切换不同模型 持续集成最新的AI模型 项目级代码理解 能够理解整个代码库结构 支持跨文件依赖分析 适合大型项目的开发和维护 现代化用户界面 基于VS Code构建，保持熟悉的操作体验 提供可视化的差异查看和批量操作 支持VS Code扩展生态 劣势 学习曲线较陡峭 功能丰富，初学者需要时间适应 AI功能的最佳使用方式需要学习 可能存在功能过载的问题 系统资源占用 作为完整IDE，占用较多系统资源 对硬件配置有一定要求 可能影响低配置设备的性能 适用场景 大型项目开发：需要深度理解复杂代码库的项目 AI深度集成需求：希望AI深度参与编程过程的开发者 现代化开发团队：追求高效开发体验的团队 VS Code用户：熟悉VS Code操作的开发者 Claude Code优势 超大上下文窗口 支持200K tokens的上下文窗口 能够一次性处理整个大型代码库 提供更深层次的代码理解能力 强大的自主决策能力 采用代理式架构，能够自主规划任务 支持复杂的多步骤任务分解 具备跨文件依赖分析能力 隐私与执行方式 以本地终端为主要操作界面，但模型推理通常在云端（调用 Anthropic API） 支持通过企业代理&#x2F;私有网关降低代码外发风险（需单独配置） 适合对安全性和审计有要求且可接受受控外发的场景 终端原生体验 无缝融入现有开发工具链 适合习惯命令行操作的开发者 减少界面干扰，专注于代码本身 劣势 学习曲线陡峭 需要熟悉终端操作 命令行界面对初学者不够友好 缺乏图形化的操作指导 缺乏可视化界面 纯命令行工具，无图形界面 不适合习惯GUI操作的开发者 代码审查和协作相对困难 适用场景 复杂项目管理：需要深度代码理解和自动化的项目 终端操作偏好：习惯命令行操作的高级开发者 大型代码库处理：需要处理大型代码库重构的场景 隐私敏感项目：对代码隐私有高要求的企业项目 性能与准确性对比代码生成质量根据社区反馈与公开测评汇总（非统一标准基准，仅作参考，团队应以内部用例复现为准）： 指标 GitHub Copilot Cursor Claude Code 语法正确性 相对稳定 较高 较高 逻辑正确性 中等 中上 较高 最佳实践遵循 中等 中上 较高 上下文相关性 中上 高 很高 响应速度 工具 平均响应时间 网络依赖 GitHub Copilot 100-300ms 高 Cursor 200-500ms 高 Claude Code 500-2000ms 中（仅API调用） 支持语言覆盖 语言类别 GitHub Copilot Cursor Claude Code 主流语言 优秀 优秀 优秀 小众语言 良好 良好 一般 新兴语言 一般 良好 良好 配置文件 良好 良好 优秀 工作场景选择指南基于开发任务类型的选择日常编码和快速开发推荐：GitHub Copilot 适用场景： 编写常见的业务逻辑代码 实现标准的API接口 快速原型开发 学习新的编程语言或框架 优势：响应快速，建议稳定，学习成本低 最佳实践：配合多个编辑器使用，适合快速迭代开发 大型项目开发和重构推荐：Cursor 适用场景： 多文件协同开发 代码重构和架构调整 团队协作开发 需要理解项目整体结构的开发任务 优势：项目级理解，多模型支持，可视化界面 最佳实践：充分利用聊天功能和多模型切换 复杂分析和系统级开发推荐：Claude Code 适用场景： 大型代码库分析和重构 复杂算法实现 系统架构设计 跨多个服务的代码修改 优势：超大上下文，深度理解，自主决策 最佳实践：适合处理复杂的、需要深度思考的编程任务 基于编程语言和技术栈的选择主流编程语言和技术栈前端开发 技术栈 首选工具 原因 备选方案 HTML&#x2F;CSS&#x2F;JavaScript GitHub Copilot 最广泛支持，丰富的代码模式 Cursor React.js Cursor 优秀的组件理解和状态管理 GitHub Copilot Vue.js Cursor 现代前端框架的深度支持 GitHub Copilot Angular GitHub Copilot 企业级框架的成熟支持 Cursor TypeScript Cursor 类型系统的深度理解 GitHub Copilot 后端开发 技术栈 首选工具 原因 备选方案 Python GitHub Copilot 最成熟的Python生态支持 Cursor Django&#x2F;Flask Cursor Web框架的项目级理解 GitHub Copilot Node.js GitHub Copilot JavaScript生态的丰富支持 Cursor Java Spring GitHub Copilot 企业级框架的深度支持 Cursor Go Claude Code 并发模式和微服务架构理解 GitHub Copilot C#&#x2F;.NET GitHub Copilot 微软技术栈的原生支持 Cursor C&#x2F;C++ Claude Code 深度理解内存管理和系统编程 GitHub Copilot 移动开发 技术栈 首选工具 原因 备选方案 React Native GitHub Copilot 跨平台开发的成熟支持 Cursor Flutter&#x2F;Dart GitHub Copilot Google生态的深度集成 Cursor Swift&#x2F;iOS GitHub Copilot 苹果生态的原生支持 Claude Code Kotlin&#x2F;Android GitHub Copilot Android开发的成熟支持 Cursor Xamarin GitHub Copilot 微软跨平台解决方案 Cursor 数据科学与AI 技术栈 首选工具 原因 备选方案 Python数据科学 GitHub Copilot 丰富的科学计算库支持 Cursor R语言 Claude Code 统计分析的深度理解 GitHub Copilot Jupyter Notebook Cursor 交互式开发环境的优秀支持 GitHub Copilot TensorFlow&#x2F;PyTorch Claude Code 深度学习框架的复杂理解 GitHub Copilot SQL&#x2F;数据库 GitHub Copilot 查询优化和数据操作 Claude Code 系统编程与嵌入式 技术栈 首选工具 原因 备选方案 C语言 Claude Code 深度理解底层内存管理和系统调用 GitHub Copilot C++ Claude Code 复杂的面向对象和模板编程理解 GitHub Copilot 嵌入式C&#x2F;C++ Claude Code 理解硬件约束和实时系统要求 GitHub Copilot 汇编语言 Claude Code 底层硬件操作的深度理解 GitHub Copilot Rust Claude Code 内存安全和系统编程的现代理解 GitHub Copilot 系统与运维 技术栈 首选工具 原因 备选方案 Docker&#x2F;容器化 GitHub Copilot 容器化最佳实践 Cursor Kubernetes Claude Code 复杂编排系统的深度理解 Cursor Linux Shell GitHub Copilot 丰富的脚本模式库 Claude Code CI&#x2F;CD管道 Cursor 项目级的持续集成理解 GitHub Copilot 云平台SDK GitHub Copilot AWS&#x2F;Azure&#x2F;GCP的API支持 Cursor 特殊场景开发 场景 首选工具 原因 备选方案 多语言混合项目 Claude Code 超大上下文理解跨语言依赖 Cursor 算法实现 Claude Code 复杂算法逻辑的深度理解 GitHub Copilot 系统集成 Cursor 项目级理解和架构设计 Claude Code 性能优化 Claude Code 系统级性能分析能力 Cursor 代码重构 Claude Code 大规模代码改动的全局理解 Cursor 基于开发阶段的选择项目初期（0-30%进度）推荐：GitHub Copilot + Cursor GitHub Copilot：快速搭建基础代码结构 Cursor：项目架构设计和初始化 策略：先用Copilot快速开发，再用Cursor整理和优化 项目中期（30-70%进度）推荐：Cursor 原因：需要频繁的跨文件操作和功能集成 优势：项目级理解能力强，适合功能开发 策略：充分利用聊天功能解决复杂问题 项目后期（70-100%进度）推荐：Claude Code + GitHub Copilot Claude Code：代码优化和重构 GitHub Copilot：bug修复和细节完善 策略：用Claude Code做整体优化，用Copilot处理细节 基于代码库特征的选择新项目开发 小型项目（&lt;10k行）：GitHub Copilot 中型项目（10k-100k行）：Cursor 大型项目（&gt;100k行）：Claude Code + Cursor 遗留代码维护 文档完善的项目：GitHub Copilot 文档缺失的项目：Claude Code（强大的代码理解能力） 需要重构的项目：Cursor + Claude Code 多语言项目 主流语言组合：GitHub Copilot 包含小众语言：Cursor（多模型支持） 复杂语言交互：Claude Code（深度理解能力） 实际使用场景对比通用开发任务对比前端开发任务 任务类型 GitHub Copilot Cursor Claude Code React组件开发 ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ Vue组件开发 ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ CSS样式编写 ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ JavaScript逻辑 ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ TypeScript开发 ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ 后端开发任务 任务类型 GitHub Copilot Cursor Claude Code RESTful API ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ 数据库操作 ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ 微服务架构 ⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 身份认证 ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 缓存实现 ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ 移动开发任务 任务类型 GitHub Copilot Cursor Claude Code React Native ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ Flutter开发 ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ 原生iOS开发 ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ 原生Android ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ 跨平台集成 ⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ 数据科学任务 任务类型 GitHub Copilot Cursor Claude Code 数据清洗 ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ 机器学习模型 ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 数据可视化 ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ 统计分析 ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ 深度学习 ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 系统编程任务 任务类型 GitHub Copilot Cursor Claude Code 系统调用 ⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ 内存管理 ⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐⭐ 并发编程 ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 网络编程 ⭐⭐⭐⭐ ⭐⭐⭐ ⭐⭐⭐⭐ 性能优化 ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 运维和工具开发 任务类型 GitHub Copilot Cursor Claude Code Docker配置 ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐ 自动化脚本 ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐ CI&#x2F;CD管道 ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐ 监控工具 ⭐⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 日志分析 ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 维护和调试 任务类型 GitHub Copilot Cursor Claude Code Bug修复 ⭐⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 代码审查 ⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 遗留代码理解 ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 架构重构 ⭐⭐ ⭐⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 性能分析 ⭐⭐ ⭐⭐⭐⭐ ⭐⭐⭐⭐⭐ 开发环境适配编辑器偏好 VS Code用户： 首选：Cursor（原生体验） 备选：GitHub Copilot（插件形式） JetBrains用户： 首选：GitHub Copilot（官方支持好） 备选：Claude Code（插件支持） Vim&#x2F;Neovim用户： 首选：GitHub Copilot（插件支持） 备选：Claude Code（终端友好） 多编辑器用户： 首选：GitHub Copilot（广泛支持） 备选：Claude Code（编辑器无关） 操作系统适配 macOS：三个工具都有良好支持 Windows：GitHub Copilot和Cursor支持最好 Linux：Claude Code和GitHub Copilot支持较好 工作流程集成Git工作流 GitHub Flow：GitHub Copilot（原生集成） GitLab Flow：Cursor（通用性好） 复杂分支策略：Claude Code（深度理解） CI&#x2F;CD集成 GitHub Actions：GitHub Copilot Jenkins&#x2F;GitLab CI：Cursor 复杂部署流程：Claude Code 最佳实践建议学习路径建议新手开发者 第一阶段：GitHub Copilot（学习AI辅助编程基础） 第二阶段：Cursor（体验项目级AI支持） 第三阶段：Claude Code（掌握高级AI编程技巧） 有经验开发者 评估阶段：同时试用三个工具，找到最适合的 专精阶段：深度掌握选定工具的高级功能 组合阶段：根据不同场景灵活切换工具 团队采用建议渐进式采用策略 试点阶段：选择1-2个开发者先行试用 评估阶段：收集使用反馈，评估效果 推广阶段：逐步扩大使用范围 标准化阶段：制定团队使用规范 培训和支持 基础培训：AI编程基础概念和最佳实践 工具培训：具体工具的使用方法和技巧 持续支持：建立内部知识分享机制 高级开发场景建议安全性和合规性考虑 代码隐私：对于涉及商业机密的关键代码，优先选择本地运行的AI工具 合规要求：遵循行业安全标准和法规，AI生成的代码需要严格审查 知识产权：避免将专有算法和核心技术通过云端AI工具处理 数据安全：处理敏感数据的代码开发时，选择符合数据保护要求的工具 性能和优化要求 高性能计算：科学计算、图像处理等性能敏感代码，建议使用Claude Code深度分析 内存优化：资源受限环境的开发，需要人工审查AI生成的代码 并发处理：多线程、异步编程等复杂并发场景，利用AI工具的深度理解能力 算法优化：复杂算法实现，选择具备数学和算法理解能力的AI工具 企业级开发协作 微服务架构：使用Cursor理解服务间接口和依赖关系 系统集成：利用Claude Code的大上下文能力理解整个系统架构 版本管理：多团队、多模块的代码版本同步，需要项目级理解能力 API设计：复杂API设计和文档生成，选择理解能力强的AI工具 质量保证策略 单元测试：AI工具生成的测试用例需要覆盖边界条件和异常情况 集成测试：跨模块功能测试，建议使用Cursor理解测试框架 性能测试：AI辅助的性能测试脚本编写和结果分析 代码审查：利用AI工具进行代码质量检查和最佳实践建议 技术债务管理 遗留系统重构：使用Claude Code理解复杂的遗留代码逻辑 架构演进：利用Cursor的项目级理解能力进行架构升级 依赖管理：AI辅助分析和更新项目依赖关系 文档维护：自动生成和更新技术文档 结论与建议总体结论基于深入的功能分析和实际使用场景对比，三款AI编码工具各有明确的优势领域： GitHub Copilot：最适合日常编码和快速开发，稳定可靠，学习成本低 Cursor：最适合大型项目开发和团队协作，项目级理解能力强，功能集成度高 Claude Code：最适合复杂分析和系统级开发，上下文理解能力最强，适合高级开发者 核心选择原则按工作复杂度选择 简单任务：GitHub Copilot（快速高效） 中等复杂度：Cursor（平衡性好） 高复杂度：Claude Code（深度理解） 按项目规模选择 小型项目：GitHub Copilot 中大型项目：Cursor 超大型项目：Claude Code + Cursor组合 按团队特征选择 新手团队：GitHub Copilot（学习成本低） 成熟团队：Cursor（效率提升明显） 专家团队：Claude Code（充分发挥高级能力） 实施建议个人开发者 起步阶段：从GitHub Copilot开始，建立AI编程习惯 进阶阶段：根据项目需要尝试Cursor或Claude Code 成熟阶段：形成个人的工具组合使用策略 团队采用 统一标准：选择一个主要工具作为团队标准 分层使用：不同角色可以使用不同的辅助工具 持续评估：定期评估工具效果，适时调整策略 企业级部署 安全评估：优先考虑数据安全和隐私保护 成本控制：制定合理的使用策略和预算规划 培训体系：建立完善的培训和支持体系 未来展望随着AI技术的快速发展，建议： 保持开放心态：持续关注新工具和新功能 灵活调整策略：根据技术发展适时调整工具选择 投资学习：持续提升AI辅助编程的技能水平 最终建议：没有一个工具能够完美适应所有场景，最佳策略是根据具体的工作内容、项目特点和个人偏好，灵活选择和组合使用这些AI编码工具。","categories":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/tags/AI/"}]},{"title":"SSE与Streamable HTTP：MCP 背后的传输技术","slug":"SSE与Streamable-HTTP：MCP-背后的传输技术","date":"2025-08-27T03:06:46.000Z","updated":"2025-08-27T06:47:45.000Z","comments":true,"path":"2025/08/27/SSE与Streamable-HTTP：MCP-背后的传输技术/","permalink":"https://www.silenceboy.com/2025/08/27/SSE%E4%B8%8EStreamable-HTTP%EF%BC%9AMCP-%E8%83%8C%E5%90%8E%E7%9A%84%E4%BC%A0%E8%BE%93%E6%8A%80%E6%9C%AF/","excerpt":"","text":"MCP 使用的传输协议背后的历史MCP（模型上下文协议）是当今最流行、应用最广泛的人工智能协议之一，它从 2025-03-26 版本开始，用 Streamable HTTP 取代了 HTTP+SSE 传输机制。这标志着该协议架构的重大变革。 现在，在解释这两种传输机制的含义之前，让我们先更好地理解这一变化。 为什么AI协议需要传输机制像 MCP 这样的 AI 协议需要传输机制来促进协议架构中不同组件之间的信息交换。 具体来说，MCP 使用 JSON-RPC 2.0 作为客户端和服务器之间的连接格式。对于 JSON-RPC 消息的传输，它依赖于标准传输机制，如 HTTP+SSE 或 Streamable HTTP（在 stdio 中 - 用于通过本地服务器上的标准输入和标准输出进行通信）。 这些专用的传输层是必需的，因为传统 HTTP 的请求-响应模型对于实时 AI 通信来说效率低下。这是因为纯 HTTP 频繁建立连接会导致高开销和延迟。相比之下，MCP 需要连续、低延迟的数据流——HTTP+SSE 和 Streamable HTTP 正是为此而设计的。 为什么从 SSE 转变到 Streamable HTTPMCP 最初使用 HTTP+SSE 来实现远程场景下的服务器到客户端的流式传输。然而，以下三个主要限制使得这一改变变得合理： 不支持可恢复流：无法从断开处恢复数据流。 需要维护长连接：服务器需要维护长时间的、高可用的连接，消耗资源，尤其是在大 规模部署时。 仅允许通过 SSE 传递服务器消息：客户端必须使用单独的 HTTP POST 请求发送消息， 无法在同一通道内双向通信。 Streamable HTTP 解决了这些问题。它支持无状态通信，甚至支持按需升级到 SSE。这提高了与现代基础设施的兼容性，并确保了更稳定、更高效的通信。同时，这一转变体现了 MCP 在追求以下目标： 提升可伸缩性：通过支持无状态服务器和减少长连接依赖，MCP 能够更好地应对大规模并发请求，为 AI 服务的爆发式增长提供坚实基础。 增强鲁棒性：可恢复流的引入，使得 MCP 在面对不稳定的网络环境时，依然能够保证数据传输的可靠性，减少因网络问题导致的数据丢失或服务中断。 优化资源利用：减少长连接的维护成本，使得服务器资源能够更高效地分配和利用，降低运营成本。 拥抱未来趋势：Streamable HTTP 与现代 Web 技术栈和云原生架构更加契合，为 MCP 未来的发展和与其他技术的融合提供了更广阔的空间。 SSESSE（Server-Sent Events）是一种允许 Web 客户端从服务器接收自动更新的机制。这些更新被称为“事件”，并通过单个长寿命 HTTP 连接发送。 与WebSockets 不同，SSE 是单向的，这意味着数据仅从服务器流向客户端。SSE 的工作原理是服务器通过此开放连接发送事件流，通常格式为text/event-streamMIME类型。 在 MCP 中使用 HTTP+SSE 服务器必须提供两个端点： 客户端用于建立连接并从服务器接收消息的 SSE GET 端点。 客户端向服务器发送 JSON-RPC 消息的常规 HTTP POST 端点。 当客户端连接时，服务器必须发送一个端点事件，其中包含客户端将用于发送消息的 URI。所有客户端 JSON-RPC 消息都将作为 HTTP POST 请求发送到此 URI。 服务器通过打开的 SSE 连接发送流式事件来响应，模拟持久会话。具体来说，服务器消息以 SSE 消息事件的形式传递，其内容在事件数据中以 JSON 格式编码。 对于单个响应，服务器发送消息并关闭流。对于正在进行的通信，连接保持打开状态。 优点和缺点优点： 流式传输大量结果：允许立即发送部分结果，允许立即发送部分结果，避免 MCP 工具处理大量数据或等待外部 API 响应时的延迟。 事件驱动触发器：支持未经请求的服务器事件，通过警报或状态更新通知客户端有关更改。 简单：使用标准 HTTP，无需特殊协议或复杂设置。 缺点： 仅限单向：数据只能在 SSE 通道中从服务器流向客户端。客户端必须使用单独的 HTTP POST 请求来发送消息。 长连接资源使用：维护开放连接会消耗大量服务器资源，尤其是在大规模连接时。 Streamable HTTP在 MCP 的语境中，Streamable HTTP 是一种使用纯 HTTP 在客户端和服务器之间传输流式数据的方法。它为实时通信打开了大门，无需长连接。 虽然它仍然可以使用 SSE 来实现灵活性和向后兼容性，但不再需要该传输方式。这使得 MCP 能够支持无状态服务器，而无需维护高可用性持久连接的开销。 为什么是Streamable HTTP + 可选 SSE 而不是 WebSockets？ 避免不必要的开销：对于简单的 RPC 调用或数据流，WebSocket 的全双工特性可能引入不必要的协议开销和复杂性。Streamable HTTP 在保持流式传输能力的同时，更加轻量。 更好的 HTTP 兼容性：WebSocket 的协议升级机制有时会与现有的 HTTP 基础设施（如代理、负载均衡器）产生兼容性问题，并且浏览器无法直接在 WebSocket 连接上附加 HTTP 头（如 Authorization）。Streamable HTTP 则完全兼容 HTTP，避免了这些问题。 POST 请求的灵活性：WebSocket 的升级握手主要基于 GET 请求，这使得基于 POST 的复杂交互流程实现起来较为繁琐。Streamable HTTP 则对 POST 和 GET 请求都提供了良好的支持。 在 MCP 中使用 Streamable HTTP在 Streamable HTTP 传输中，服务器作为一个独立进程，能够处理多个客户端连接。它使用标准的 HTTP POST 和 GET 请求进行通信。 服务器可以选择使用 SSE 将多条消息流式传输到客户端。这既适用于用于简单请求&#x2F;响应工具的基本 MCP 服务器，也适用于提供更高级功能（例如流式传输和实时服务器到客户端通知）的服务器。 服务器必须公开一个支持 POST 和 GET 方法的 HTTP 端点（称为 “MCP 端点“）。 下图说明了使用 Streamable HTTP 的 MCP 客户端和服务器之间的通信流程： 为了支持恢复断开的连接并重新传递可能丢失的消息，MCP 服务器会为每个流分配 ID。这些 ID 在每个流中充当游标。 优点和缺点优点： 支持无状态服务器：无需始终在线的长连接。 纯 HTTP：可以使用任何标准 HTTP 服务器实现，而无需 SSE。 基础设施友好：与常见的 HTTP 中间件、代理和托管平台兼容。 向后兼容：在以前的 HTTP+SSE 传输基础上逐步构建。 可选流式传输：服务器可在需要时升级为 SSE，以实现流式传输响应。 解决 SSE 局限性：支持可恢复流，无需维护长连接，且允许客户端和服务器在同一 HTTP 端点进行通信（通过 POST 和 GET）。 效率更高：对于大负载或自定义二进制协议，数据传输更高效，没有 SSE 的事件格式 开销。 更强的控制力：应用可以更精细地控制缓冲策略，可能减少内存开销。 缺点： 暂无 SSE 与Streamable HTTP对比 类型 HTTP+SSE Streamable HTTP 通信类型 单向（服务器→客户端） 双向（客户端通过 GET&#x2F;POST ↔ 服务器） HTTP 协议的使用 GET 用于流媒体，POST 用于客户端信息 从一个端点使用标准 HTTP POST 和 GET 状态性 有状态 有状态，但支持无状态服务器 需要长期 HTTP 连接 是 否 要求高可用性 是，用于连接持久性 否，适用于无状态或临时服务器 可扩展性 有限 高 流媒体支持 是（通过文本&#x2F;事件流） 是（通过 SSE 作为可选增强功能） 身份验证支持 是 是 支持可恢复性和重新交付 没有 没有 客户数量 多个 多个 在 MCP 中的使用 自协议版本 2025-03-26 起已弃用 在 2025-03-26 版协议中引入 向后兼容性 - 完全向后兼容基于 SSE 的客户端","categories":[{"name":"mcp","slug":"mcp","permalink":"https://www.silenceboy.com/categories/mcp/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/tags/AI/"},{"name":"mcp","slug":"mcp","permalink":"https://www.silenceboy.com/tags/mcp/"}]},{"title":"命令帮手：tldr 安装与中文配置指南","slug":"命令帮手：tldr-安装与中文配置指南","date":"2025-05-09T05:39:38.000Z","updated":"2025-05-09T05:50:09.000Z","comments":true,"path":"2025/05/09/命令帮手：tldr-安装与中文配置指南/","permalink":"https://www.silenceboy.com/2025/05/09/%E5%91%BD%E4%BB%A4%E5%B8%AE%E6%89%8B%EF%BC%9Atldr-%E5%AE%89%E8%A3%85%E4%B8%8E%E4%B8%AD%E6%96%87%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8D%97/","excerpt":"","text":"在日常开发与运维过程中，你是否会觉得传统的 man 手册过于繁琐？tldr 项目正是为了解决这个问题而生，它为上百个常用命令提供了简明直观的示例，极大地提升了查阅效率。本文将手把手介绍 tldr 在 Windows、Mac、Ubuntu 下的安装流程，基本使用方法，以及如何让 tldr 输出为简体中文。 一、tldr 安装方法1. Windows 系统方法一：用 Scoop 安装（推荐） 安装 Scoop（如果还没装）： 12Set-ExecutionPolicy RemoteSigned -Scope CurrentUserirm get.scoop.sh | iex 安装 tldr： 1scoop install tldr 方法二：用 npm 安装需先装好 Node.js 与 npm，然后在命令行执行： 1npm install -g tldr 方法三：用 pip 安装需先装好python与pip，然后在命令行执行： 1pip3 install tldr 2. macOS 系统推荐使用 Homebrew： 1brew install tldr 或者用 npm： 1npm install -g tldr 或者用 pip： 1pip3 install tldr 3. Ubuntu &#x2F; Debian 系统推荐方式（snap 安装）： 1sudo snap install tldr 或者用 apt（部分老版本仓库没有）： 12sudo apt updatesudo apt install tldr 还可以用 npm： 1npm install -g tldr 或者用 pip： 1pip3 install tldr 二、tldr 的基本使用安装好 tldr 后，在命令行中直接输入： 1tldr 命令名 例如： 1tldr tar 它会显示 tar 常用的简要用法和示例。 常用选项： tldr -u强制更新离线文档缓存。 tldr --list查看有哪些命令有 tldr 页面。 tldr --help查看 tldr 的详细帮助信息。 三、配置 tldr 显示中文页面tldr 官方已支持多语言，目前大部分主流命令已有简体中文文档。只需调整默认语言环境变量即可。 1. 临时使用中文只对当前命令生效： 1LANG=zh tldr ls 或（部分 tldr 客户端支持） 1LANGUAGE=zh tldr ls 2. 永久设置为中文macOS &#x2F; Ubuntu &#x2F; WSL将下方内容添加到 ~/.bashrc 或 ~/.zshrc（具体看你用哪个 shell）： 1export LANG=zh 保存后，执行一次： 1source ~/.bashrc # 或 source ~/.zshrc 这样每次开新终端，tldr 默认输出中文页面。 Windows（CMD 或 PowerShell）CMD 中临时生效： 12set LANG=zhtldr ls 长期生效：在系统环境变量或用户环境变量中添加 LANG，值填 zh。 3. 更新缓存（非常重要）修改语言后，强烈建议刷新缓存让中文页面生效： 1tldr -u 4. 验证效果运行： 1tldr cp 如果出现中文简明用法，表明配置成功。 四、常见问题 部分命令还是英文？可能该命令尚未有中文翻译。同样建议及时更新缓存。 未知命令提示或无法联网？检查网络或采用 tldr --update 手动补全离线缓存。 Windows 环境变量生效问题？尝试重启终端或电脑，确认语言变量设置无误。 五、总结tldr 是极简而实用的命令行速查工具，无论软件开发者还是 Linux&#x2F;Unix&#x2F;Windows 运维者都能从中获益。通过简单的安装和配置，你就能轻松查阅常见命令的简明用法，还能设为汉语输出，降低理解难度，为高效开发助力。 扩展阅读： 官方网站：https://tldr.sh/ GitHub 项目：https://github.com/tldr-pages/tldr","categories":[{"name":"shell","slug":"shell","permalink":"https://www.silenceboy.com/categories/shell/"}],"tags":[{"name":"shell","slug":"shell","permalink":"https://www.silenceboy.com/tags/shell/"}]},{"title":"Python魔法方法介绍","slug":"Python魔法方法介绍","date":"2025-04-25T06:34:11.000Z","updated":"2025-04-25T06:35:45.000Z","comments":true,"path":"2025/04/25/Python魔法方法介绍/","permalink":"https://www.silenceboy.com/2025/04/25/Python%E9%AD%94%E6%B3%95%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"1. 什么是魔法方法魔法方法是指前后都有双下划线的特殊方法，例如 init、new、str 等。这些方法是 Python 内部预定义的，将它们以特定名字命名，是为了配合 Python 的特性（如对象创建、属性访问、运算符重载等）实现自定义行为。 2. 常见魔法方法类别与功能a) 对象的构造与销毁 魔法方法 作用描述 常用场景 __new__ 实例创建，返回一个新对象 单例、元类、自定义创建流程 __init__ 实例初始化，设置属性等 常规对象初始化 __del__ 实例删除前调用（析构函数） 资源释放、日志等 b) 字符串与可打印表现 魔法方法 作用描述 常用场景 __str__ str(obj)、print(obj) 时的表现 用户友好信息 __repr__ repr(obj)、调试器、解释器中直接输入对象时的表现 开发调试、复现对象 c) 运算符重载使实例自定义如何参与各种运算： 魔法方法 覆盖的运算符 例子 __add__ + a + b __sub__ - a - b __mul__ * a * b __truediv__ &#x2F; a &#x2F; b __floordiv__ &#x2F;&#x2F; a &#x2F;&#x2F; b __mod__ % a % b __pow__ ** a ** b __eq__ &#x3D;&#x3D; a &#x3D;&#x3D; b __ne__ !&#x3D; a !&#x3D; b __lt__ &lt; a &lt; b __gt__ &gt; a &gt; b __le__ &lt;&#x3D; a &lt;&#x3D; b 还有很多，比如逻辑运算（__and__, __or__ 等）、反向运算（如 __radd__)等。 d) 集合与映射接口用于容器类的自定义： 魔法方法 作用描述 使用方式 __len__ 求长度 len(obj) __getitem__ 获取指定元素 obj[key] __setitem__ 设置值 obj[key]&#x3D;value __delitem__ 删除项 del obj[key] __contains__ in&#x2F;not in 查询 item in obj __iter__ 返回迭代器 for x in obj __next__ 迭代器的下一个值 next(iterator) e) 上下文管理 魔法方法 作用描述 使用方式 __enter__ 进入上下文、with块前调用 with obj as xxx __exit__ 离开上下文、with块后调用 f) 可调用对象相关 魔法方法 作用描述 示例 __call__ 让实例像函数一样可调用 obj() g) 其他常用魔法方法 __getattr__ &#x2F; __setattr__ &#x2F; __delattr__：属性访问&#x2F;设置&#x2F;删除拦截。 __slots__：限制对象可以有哪些属性，提高内存效率。 3. 魔法方法功能对比表 方法 调用时机&#x2F;作用 返回值须 是否常用 __new__ 创建对象（类-&gt;实例） 实例 or 其子类 部分场景 __init__ 初始化对象 None 常用 __del__ 删除对象时 None 偶尔 __str__ str&#x2F;print str 常用 __repr__ repr()，交互式解释器 str 常用 __add__ a+b 任意 按需 __getitem__ obj[key] 任意 按需 __call__ obj() 任意 有趣&#x2F;高阶 __enter__ with 块开始 任意 按需 __exit__ with 块结束 None&#x2F;Bool 按需 4. 总结 魔法方法本质上让我们可以“像内置类型一样”自定义自己的类行为。 它们不需要显式调用，由 Python 语法环境&#x2F;运算符&#x2F;函数自动调用。 合理地使用魔法方法，可以让自己的类表现得更“Pythonic”，增加灵活性、可读性与可用性。 不是所有魔法方法都需要重写，按需选择。 举例说明： 123456789101112class Vector2D: def __init__(self, x, y): self.x, self.y = x, y def __add__(self, other): return Vector2D(self.x + other.x, self.y + other.y) def __str__(self): return f&quot;(&#123;self.x&#125;, &#123;self.y&#125;)&quot;v1 = Vector2D(1,2)v2 = Vector2D(3,4)print(v1 + v2) # 自动调用 __add__（输出：(4, 6)）print(str(v1)) # 自动调用 __str__（输出：(1, 2)）","categories":[{"name":"python","slug":"python","permalink":"https://www.silenceboy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://www.silenceboy.com/tags/python/"}]},{"title":"取消conda终端默认激活base虚拟环境","slug":"取消conda终端默认激活base虚拟环境","date":"2025-04-09T10:20:02.000Z","updated":"2025-04-09T10:20:52.000Z","comments":true,"path":"2025/04/09/取消conda终端默认激活base虚拟环境/","permalink":"https://www.silenceboy.com/2025/04/09/%E5%8F%96%E6%B6%88conda%E7%BB%88%E7%AB%AF%E9%BB%98%E8%AE%A4%E6%BF%80%E6%B4%BBbase%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/","excerpt":"","text":"默认情况下，安装完conda后，每次打开终端都会自动激活base环境。如果你不习惯这一行为，可以通过修改conda的相关配置，使终端启动时不再默认激活base环境。 ▶️ 临时关闭 auto activate (当前终端有效)如果你只是想一次性临时地关闭，可以运行： 1conda deactivate 但是这种方式仅对当前打开的终端临时生效，打开新的终端窗口时，base还是会自动激活。 ✅ 长期永久关闭默认激活 (推荐)要永久关闭每次终端自动激活base环境，可以运行： 1conda config --set auto_activate_base false 以上命令会修改conda配置文件~/.condarc，添加如下内容： 1auto_activate_base: false 以后你每次打开新的终端窗口时，base将不会再自动激活。 📝 如果你后悔了，如何恢复自动激活base？再次启用自动激活： 1conda config --set auto_activate_base true 🔍 验证你的配置执行以下命令验证： 1conda config --show | grep auto_activate_base 输出： 1auto_activate_base: false 代表成功地关闭了默认激活行为。 ⚠️ 注意： 在关闭默认激活之后，如果想激活base或其他虚拟环境，就需要手动运行： 12conda activate base #激活baseconda activate env_name #激活其他环境 按照以上方法，你便可以自由控制conda是否默认激活base环境了。","categories":[{"name":"python","slug":"python","permalink":"https://www.silenceboy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://www.silenceboy.com/tags/python/"}]},{"title":"python项目批量检查依赖并添加进requirements文件","slug":"python项目批量检查依赖并添加进requirements文件","date":"2025-04-09T01:38:13.000Z","updated":"2025-04-09T09:04:46.000Z","comments":true,"path":"2025/04/09/python项目批量检查依赖并添加进requirements文件/","permalink":"https://www.silenceboy.com/2025/04/09/python%E9%A1%B9%E7%9B%AE%E6%89%B9%E9%87%8F%E6%A3%80%E6%9F%A5%E4%BE%9D%E8%B5%96%E5%B9%B6%E6%B7%BB%E5%8A%A0%E8%BF%9Brequirements%E6%96%87%E4%BB%B6/","excerpt":"","text":"在Python项目中，管理依赖是很重要的工作。下面我介绍几种方法，可以帮助你批量检查项目依赖并更新requirements文件。 1. 使用pipreqs自动生成requirements.txtpipreqs是一个很好的工具，它能分析你的代码并只生成项目实际使用的依赖列表。 12345# 安装pipreqspip install pipreqs# 在项目根目录运行pipreqs . --force # --force选项会覆盖已存在的requirements.txt 这个工具的优点是它只包含代码中实际导入的包，而不是环境中安装的所有包。 2. 使用pip-tools管理依赖pip-tools提供了两个命令：pip-compile和pip-sync，可以更精确地管理依赖。 123456789# 安装pip-toolspip install pip-tools# 创建一个requirements.in文件，列出主要依赖# 然后生成详细的requirements.txtpip-compile requirements.in# 保持环境与requirements.txt同步pip-sync 这种方法的优点是可以区分直接依赖和间接依赖，并且锁定所有包的版本。 3. 检查项目中缺少的依赖可以使用pylint或其他静态分析工具来查找可能缺少的导入： 12345# 安装pylintpip install pylint# 扫描项目pylint --disable=all --enable=no-name-in-module,import-error path/to/your/project 4. 自动化脚本示例你可以创建一个脚本来自动执行这些步骤： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#!/usr/bin/env python3&quot;&quot;&quot;自动检查和更新项目依赖，并添加到requirements.txt文件&quot;&quot;&quot;import osimport subprocessimport sysdef main(): &quot;&quot;&quot;主函数&quot;&quot;&quot; # 确保pip-tools已安装 try: import piptools except ImportError: print(&quot;安装 pip-tools...&quot;) subprocess.check_call([sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;pip-tools&quot;]) # 使用pipreqs分析项目依赖 try: import pipreqs except ImportError: print(&quot;安装 pipreqs...&quot;) subprocess.check_call([sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, &quot;pipreqs&quot;]) # 生成临时requirements文件 print(&quot;分析项目依赖...&quot;) subprocess.check_call([&quot;pipreqs&quot;, &quot;.&quot;, &quot;--savepath&quot;, &quot;requirements.temp.txt&quot;, &quot;--force&quot;]) # 读取现有的requirements.txt (如果存在) existing_reqs = set() if os.path.exists(&quot;requirements.txt&quot;): with open(&quot;requirements.txt&quot;, &quot;r&quot;) as f: for line in f: line = line.strip() if line and not line.startswith(&quot;#&quot;): existing_reqs.add(line.split(&quot;==&quot;)[0]) # 读取新生成的requirements new_reqs = [] with open(&quot;requirements.temp.txt&quot;, &quot;r&quot;) as f: for line in f: line = line.strip() if line and not line.startswith(&quot;#&quot;): new_reqs.append(line) # 合并并分类 main_deps = [] for req in new_reqs: pkg_name = req.split(&quot;==&quot;)[0] if pkg_name not in existing_reqs: main_deps.append(f&quot;&#123;req&#125; # 新添加的依赖&quot;) else: main_deps.append(req) # 写入最终的requirements.txt with open(&quot;requirements.txt&quot;, &quot;w&quot;) as f: f.write(&quot;# 主要依赖 - 项目直接使用\\n&quot;) for dep in sorted(main_deps): f.write(f&quot;&#123;dep&#125;\\n&quot;) f.write(&quot;\\n# 根据实际需要添加开发和测试依赖\\n&quot;) # 清理临时文件 os.remove(&quot;requirements.temp.txt&quot;) print(&quot;完成! requirements.txt 已更新。&quot;)if __name__ == &quot;__main__&quot;: main() 将这个脚本保存为update_requirements.py，然后运行： 1python update_requirements.py 5. 整合进Makefile如果你的项目使用Makefile，可以添加一个目标来更新依赖： 1234567.PHONY: update-depsupdate-deps: @echo &quot;更新项目依赖...&quot; pip install pipreqs pipreqs . --force @echo &quot;依赖已更新到requirements.txt&quot; 6. 对于复杂项目的建议对于有多个环境的复杂项目： 使用pyproject.toml配合setuptools或poetry管理依赖 区分开发依赖和运行时依赖 使用虚拟环境确保依赖隔离 1234# 使用poetrypoetry init # 创建pyproject.tomlpoetry add package1 package2 # 添加依赖poetry export -f requirements.txt --output requirements.txt # 导出requirements.txt 7. 检查未使用的依赖可以使用pip-extra-reqs工具检查未使用的依赖： 12pip install pip-extra-reqspip-extra-reqs src/ 通过以上步骤，你可以有效地管理项目依赖，确保requirements.txt文件包含所有必要的依赖，同时避免添加不必要的包。无论项目规模大小，这些方法都能帮助你保持依赖的清晰和最新。","categories":[{"name":"python","slug":"python","permalink":"https://www.silenceboy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://www.silenceboy.com/tags/python/"}]},{"title":"本地管理多个python版本","slug":"本地管理多个python版本","date":"2025-03-26T12:13:54.000Z","updated":"2025-04-09T09:04:36.000Z","comments":true,"path":"2025/03/26/本地管理多个python版本/","permalink":"https://www.silenceboy.com/2025/03/26/%E6%9C%AC%E5%9C%B0%E7%AE%A1%E7%90%86%E5%A4%9A%E4%B8%AApython%E7%89%88%E6%9C%AC/","excerpt":"","text":"在本地管理多个 Python 版本是开发中的常见需求，以下是几种主流且高效的方法，适用于不同操作系统： 一、使用 pyenv（推荐给 macOS&#x2F;Linux 用户）原理：通过修改环境变量动态切换 Python 版本，不依赖系统自带的 Python。 1. 安装 pyenv1234567# 使用安装脚本curl https://pyenv.run | bash# 将以下内容添加到 ~/.bashrc 或 ~/.zshrcexport PATH=&quot;$HOME/.pyenv/bin:$PATH&quot;eval &quot;$(pyenv init -)&quot;eval &quot;$(pyenv virtualenv-init -)&quot; 2. 常用命令1234567891011121314151617# 查看可安装版本pyenv install --list# 安装指定版本（如 Python 3.9.6）pyenv install 3.9.6# 列出已安装版本pyenv versions# 设置全局默认版本pyenv global 3.9.6# 设置当前目录的本地版本（优先级更高）pyenv local 3.8.12# 卸载版本pyenv uninstall 3.7.0 二、使用 conda（跨平台，适合科学计算场景）原理：通过虚拟环境管理 Python 版本和依赖。 1. 安装 Miniconda&#x2F;Anaconda从官网下载安装包：https://docs.conda.io/en/latest/miniconda.html 2. 创建不同 Python 版本的环境1234567891011121314# 创建名为 py38 的环境，指定 Python 3.8conda create -n py38 python=3.8# 激活环境conda activate py38# 查看所有环境conda env list# 退出环境conda deactivate# 删除环境conda env remove -n py38 三、使用 Docker（适合隔离开发环境）通过容器化技术隔离不同项目环境。 示例 Dockerfile12345FROM python:3.7-slimWORKDIR /appCOPY requirements.txt .RUN pip install -r requirements.txtCMD [&quot;python&quot;, &quot;app.py&quot;] 使用不同镜像版本12# 运行 Python 3.9 容器docker run -it --rm python:3.9-alpine python --version 四、Windows 用户的替代方案 pyenv-win（类似 Unix 的 pyenv）： 123# 安装命令Invoke-WebRequest -UseBasicParsing -Uri &quot;https://raw.githubusercontent.com/pyenv-win/pyenv-win/master/pyenv-win/install-pyenv-win.ps1&quot; -OutFile &quot;./install-pyenv-win.ps1&quot;&amp; &quot;./install-pyenv-win.ps1&quot; 手动安装多个版本： 从官网下载不同版本的安装包（如 python-3.8.exe 和 python-3.10.exe） 安装时勾选 “Add to PATH”，但通过修改可执行文件名区分版本：12# 将 Python 3.8 的可执行文件重命名mv /path/to/python3.8/python.exe /path/to/python3.8/python38.exe 五、通用技巧：虚拟环境 + 版本指定即使使用系统 Python，也可通过 venv 或 virtualenv 隔离环境： 123456# 使用特定 Python 版本创建虚拟环境/path/to/python3.9 -m venv myenv# 激活环境source myenv/bin/activate # Linux/macOSmyenv\\Scripts\\activate.bat # Windows 版本管理工具对比 工具 适用系统 特点 pyenv macOS&#x2F;Linux 轻量级，纯命令行操作 conda 跨平台 集成包管理，适合科学计算 Docker 跨平台 完全环境隔离，但需学习容器技术 手动管理 所有系统 灵活性高，但维护成本较高 选择工具时，建议优先使用 pyenv（Unix）或 conda（跨平台）以简化操作。","categories":[{"name":"python","slug":"python","permalink":"https://www.silenceboy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://www.silenceboy.com/tags/python/"}]},{"title":"pip自动写入requirements的终极方案","slug":"pip自动写入requirements的终极方案","date":"2025-03-21T04:50:53.000Z","updated":"2025-03-21T04:52:40.000Z","comments":true,"path":"2025/03/21/pip自动写入requirements的终极方案/","permalink":"https://www.silenceboy.com/2025/03/21/pip%E8%87%AA%E5%8A%A8%E5%86%99%E5%85%A5requirements%E7%9A%84%E7%BB%88%E6%9E%81%E6%96%B9%E6%A1%88/","excerpt":"","text":"在Python开发中，手动维护requirements.txt文件容易遗漏依赖项。本文将介绍三种自动化解决方案，让依赖管理更高效。 方案一：智能Shell别名（原生pip增强）实现原理： 通过Shell函数封装pip命令，在执行安装后自动更新requirements文件 配置方法（在.bashrc&#x2F;.zshrc中添加）： 1234567pip() &#123; if [ &quot;$1&quot; = &quot;install&quot; ]; then command pip &quot;$@&quot; &amp;&amp; pip freeze --exclude-editable | grep -v &#x27;^#&#x27; &gt; requirements.txt else command pip &quot;$@&quot; fi&#125; 使用示例： 12345678# 安装包并自动记录pip install requests==2.26.0# 安装多个包（支持所有pip参数）pip install django~=3.2.0 celery[redis]# 开发模式安装（不会记录到requirements）pip install -e . 方案特点： ✅ 零依赖，纯Shell实现 🛡️ 排除-e安装的本地包 🔍 自动过滤注释行 ⚠️ 注意：会覆盖原有requirements文件 方案二：pip-autosave工具（专业级自动记录）安装使用： 1pip install pip-autosave 使用场景： 12345678# 基础用法（自动生成requirements.txt）pip install requests --save# 指定保存文件pip install pandas --save requirements-dev.txt# 批量安装并记录pip install -r base-requirements.txt --save 核心功能： 📦 增量更新模式（保留已有依赖） 🎯 智能版本锁定（记录精确版本号） 🔄 支持多环境文件（dev&#x2F;prod） 📊 生成依赖关系树可视化：1pip show pandas --save --tree 方案三：现代项目管理工具集成1. Pipenv工作流： 123456# 安装并自动更新Pipfilepipenv install requestspipenv install --dev pytest# 生成标准requirements文件pipenv requirements &gt; requirements.txt 2. Poetry配置： 12345678910# pyproject.toml 配置示例[tool.poetry.dependencies]python = &quot;^3.8&quot;requests = &#123; version = &quot;*&quot;, optional = true &#125;[tool.poetry.dev-dependencies]pytest = &quot;^6.2.5&quot;# 导出requirements.txtpoetry export -f requirements.txt --output requirements.txt 3. Hatch环境管理： 12345# 创建带自动依赖跟踪的环境hatch env create myenv# 在环境中安装依赖hatch run myenv pip install numpy 版本控制最佳实践 差异化版本记录： 12345# 生产依赖pip freeze --exclude-editable | grep -v &#x27;pkg-resources==0.0.0&#x27; &gt; requirements.txt # 开发依赖pip freeze --exclude-editable | grep -E &#x27;(pytest|coverage)&#x27; &gt; requirements-dev.txt 依赖树可视化检查： 1pipdeptree --exclude pip,pip-autosave,setuptools,wheel 安全更新策略： 12345# 查看过时依赖pip list --outdated --format=columns # 批量更新命令pip install $(pip list --outdated | awk &#x27;NR&gt;2 &#123;print $1&#125;&#x27;) --upgrade 不同方案的适用场景对比 方案 适用场景 优势 局限性 Shell别名 快速原型开发 无需安装新工具 功能有限，可能覆盖文件 pip-autosave 企业级项目 精细控制，支持多环境 需要额外安装 Pipenv&#x2F;Poetry 长期维护的大型项目 完整依赖解析，支持锁定文件 学习成本较高 Hatch 多环境复杂配置 集成测试和构建流程 生态系统较新 常见问题解决方案Q：如何处理不同操作系统依赖？ 12# 使用平台标记pip install pywin32 --save; sys_platform == &#x27;win32&#x27; Q：如何避免开发工具污染生产依赖？ 123456# 使用分层requirements文件.├── requirements│ ├── base.txt│ ├── dev.txt│ └── prod.txt Q：依赖冲突自动解决示例： 123# 使用版本范围语法Django&gt;=3.2,&lt;4.0requests&gt;=2.25.1,!=2.28.0","categories":[{"name":"python","slug":"python","permalink":"https://www.silenceboy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://www.silenceboy.com/tags/python/"}]},{"title":"Python虚拟环境创建、激活、管理与最佳实践","slug":"Python虚拟环境创建、激活、管理与最佳实践","date":"2025-03-21T04:47:36.000Z","updated":"2025-03-21T04:49:00.000Z","comments":true,"path":"2025/03/21/Python虚拟环境创建、激活、管理与最佳实践/","permalink":"https://www.silenceboy.com/2025/03/21/Python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E5%88%9B%E5%BB%BA%E3%80%81%E6%BF%80%E6%B4%BB%E3%80%81%E7%AE%A1%E7%90%86%E4%B8%8E%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/","excerpt":"","text":"引言：为什么需要虚拟环境？在Python开发中，不同项目往往依赖不同版本的第三方库。全局安装的包可能导致版本冲突，例如项目A需要Django 3.2，而项目B需要Django 4.0。虚拟环境通过为每个项目创建隔离的Python运行环境，完美解决这一难题。 一、创建虚拟环境1. 使用内置venv模块12# 适用于Python 3.3+版本python -m venv myenv # 创建名为myenv的虚拟环境 2. 指定Python解释器版本1python3.8 -m venv py38_env # 使用特定Python版本创建 3. 目录结构解析12345myenv/├── bin/ # Unix激活脚本├── Scripts/ # Windows激活脚本├── Lib/ # 安装的第三方库└── pyvenv.cfg # 环境配置文件 二、激活虚拟环境Windows系统12myenv\\Scripts\\activate# 命令提示符显示 (myenv) C:\\&gt; macOS&#x2F;Linux系统12source myenv/bin/activate# 终端提示符显示 (myenv) $ 验证激活状态： 12which python # Unixwhere python # Windows 三、管理项目依赖安装依赖包1pip install django==3.2.12 导出依赖清单1pip freeze &gt; requirements.txt 批量安装依赖1pip install -r requirements.txt 四、退出虚拟环境12deactivate# 命令提示符恢复默认状态 五、进阶技巧与工具1. 虚拟环境管理工具对比 工具 特点 适用场景 venv Python内置，轻量级 简单项目 virtualenv 支持Python 2&#x2F;3 兼容旧项目 pipenv 整合pip+虚拟环境 复杂依赖管理 poetry 依赖解析+打包一体化 专业项目开发 2. 快速复制环境12345# 在原环境执行pip list --format=freeze &gt; requirements.txt# 在新环境执行pip install -r requirements.txt 3. 环境配置加速12python -m pip install pip --upgrade # 升级pippip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple # 国内镜像源 六、最佳实践 项目隔离原则：每个独立项目创建专属虚拟环境 版本控制：将requirements.txt加入Git仓库，忽略虚拟环境目录1234# .gitignoremyenv/venv/*.env/ 定期维护：12pip list --outdated # 检查过期包pip-autoremove # 清理无用依赖 结语掌握虚拟环境是Python开发者的必备技能。通过venv创建隔离环境，配合requirements.txt管理依赖，能有效避免”在我机器上能运行”的经典问题。建议立即在您的下一个Python项目中实践这些技巧，体验更干净的开发环境！ 提示：删除虚拟环境只需删除对应目录即可，但请确保已执行deactivate退出环境。","categories":[{"name":"python","slug":"python","permalink":"https://www.silenceboy.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://www.silenceboy.com/tags/python/"}]},{"title":"ASIL等级是什么","slug":"ASIL等级是什么","date":"2024-10-22T06:45:40.000Z","updated":"2024-10-22T06:46:23.000Z","comments":true,"path":"2024/10/22/ASIL等级是什么/","permalink":"https://www.silenceboy.com/2024/10/22/ASIL%E7%AD%89%E7%BA%A7%E6%98%AF%E4%BB%80%E4%B9%88/","excerpt":"","text":"ASIL（Automotive Safety Integrity Level，汽车安全完整性等级）是ISO 26262标准中定义的一个概念，用于评估和分类汽车电子系统中潜在故障对安全的影响。ASIL等级从A到D，共分为四个级别，其中ASIL D表示最高的安全要求，ASIL A表示最低的安全要求。 ASIL等级的定义ASIL等级基于三个主要因素来确定： 严重性（Severity, S）：故障导致的潜在伤害的严重程度。 暴露率（Exposure, E）：驾驶员或乘客暴露于潜在故障的频率。 可控性（Controllability, C）：驾驶员或乘客在故障发生时控制车辆的能力。 ASIL等级的划分根据上述三个因素的组合，ASIL等级可以分为以下四个级别： ASIL A：最低的安全要求。适用于故障对安全影响较小、暴露率低且可控性高的情况。 ASIL B：中等安全要求。适用于故障对安全影响中等、暴露率中等且可控性一般的情况。 ASIL C：较高的安全要求。适用于故障对安全影响较大、暴露率较高且可控性较低的情况。 ASIL D：最高的安全要求。适用于故障对安全影响非常大、暴露率非常高且几乎不可控的情况。 ASIL等级的确定ASIL等级的确定通常通过一个风险评估矩阵来完成，该矩阵综合考虑严重性、暴露率和可控性三个因素。以下是一个简化的示例矩阵： 严重性&#x2F;暴露率&#x2F;可控性 E1（低） E2（中低） E3（中高） E4（高） S1（低） QM QM ASIL A ASIL A S2（中） QM ASIL A ASIL B ASIL B S3（高） ASIL A ASIL B ASIL C ASIL D S4（极高） ASIL B ASIL C ASIL D ASIL D 注：QM（Quality Management）表示不需要特别的安全措施，只需按照质量管理标准进行处理。 ASIL等级的应用在汽车电子系统的设计和开发过程中，确定ASIL等级是确保系统安全性的关键步骤。不同的ASIL等级对应不同的开发流程和验证要求： ASIL A：基本的安全措施和验证。 ASIL B：需要更严格的设计和测试流程。 ASIL C：需要高级的安全分析和验证技术。 ASIL D：需要最高级别的安全措施、冗余设计和全面的验证。 总结ASIL等级是评估和管理汽车电子系统安全性的关键工具。通过确定系统的ASIL等级，开发人员可以采取适当的设计和验证措施，以确保系统在各种可能的故障情况下都能保持安全性。ISO 26262标准提供了详细的指南和方法，帮助开发人员在整个开发生命周期中实现和维护所需的安全完整性等级。","categories":[{"name":"ASIL","slug":"ASIL","permalink":"https://www.silenceboy.com/categories/ASIL/"}],"tags":[{"name":"ASIL","slug":"ASIL","permalink":"https://www.silenceboy.com/tags/ASIL/"}]},{"title":"SOC中除了R核和A核还有哪些处理器核心","slug":"SOC中除了R核和A核还有哪些处理器核心","date":"2024-10-21T06:25:57.000Z","updated":"2024-10-21T06:26:19.000Z","comments":true,"path":"2024/10/21/SOC中除了R核和A核还有哪些处理器核心/","permalink":"https://www.silenceboy.com/2024/10/21/SOC%E4%B8%AD%E9%99%A4%E4%BA%86R%E6%A0%B8%E5%92%8CA%E6%A0%B8%E8%BF%98%E6%9C%89%E5%93%AA%E4%BA%9B%E5%A4%84%E7%90%86%E5%99%A8%E6%A0%B8%E5%BF%83/","excerpt":"","text":"除了R核（实时核心）和A核（应用核心），在系统级芯片（SoC）设计中，还有其他类型的处理器核心，它们各自有特定的用途和特点。以下是一些常见的核心类型： M核（Microcontroller Core）M核通常指的是微控制器核心，主要用于低功耗、低成本的嵌入式系统。以下是M核的一些特点和应用： 低功耗：设计目标是尽量减少功耗，适合电池供电的设备。 简单架构：通常具有较简单的指令集和架构，易于编程和调试。 集成外设：通常集成了丰富的外设接口，如ADC、DAC、UART、I2C、SPI等。 实时性：虽然不如R核那样严格，但也能处理一些实时任务。 应用场景 家用电器 传感器节点 简单控制系统 物联网设备 NPU（Neural Processing Unit）NPU是神经网络处理单元，专门用于加速深度学习和人工智能任务。以下是NPU的一些特点和应用： 高效计算：专门优化用于矩阵运算和卷积操作，适合深度学习模型的推理和训练。 并行处理：具有高度并行的计算能力，能够同时处理大量数据。 低功耗：相对于通用处理器，NPU在执行AI任务时具有更高的能效比。 应用场景 图像和视频处理 语音识别 自然语言处理 自动驾驶 DSP（Digital Signal Processor）DSP是数字信号处理器，专门用于处理数字信号，如音频、视频和通信信号。以下是DSP的一些特点和应用： 高效信号处理：专门优化用于快速傅里叶变换（FFT）、滤波和其他信号处理算法。 实时处理：能够实时处理输入信号，适合实时音频和视频处理。 低延迟：设计目标是尽量减少处理延迟，确保信号处理的实时性。 应用场景 音频处理（如回声消除、降噪） 视频处理（如编码、解码） 通信系统（如调制、解调） 雷达和声纳 GPU（Graphics Processing Unit）GPU是图形处理单元，主要用于图形渲染和计算加速。以下是GPU的一些特点和应用： 高并行性：具有大量并行处理单元，能够同时处理大量数据。 图形渲染：专门用于处理图形渲染任务，如3D图形、游戏图形等。 通用计算：近年来，GPU也被广泛用于通用计算（GPGPU），如科学计算、机器学习等。 应用场景 游戏和娱乐 图形设计和渲染 科学计算 深度学习 VPU（Vision Processing Unit）VPU是视觉处理单元，专门用于处理计算机视觉任务。以下是VPU的一些特点和应用： 优化视觉任务：专门优化用于图像和视频处理任务，如对象检测、图像识别等。 高效能效比：在处理视觉任务时具有高效的能效比。 实时处理：能够实时处理图像和视频数据，适合实时应用。 应用场景 计算机视觉 增强现实（AR）和虚拟现实（VR） 自动驾驶 安全监控 总结在现代SoC设计中，除了R核和A核，还有多种类型的处理器核心，如M核、NPU、DSP、GPU和VPU等。每种核心都有其特定的用途和优势，通过组合使用这些核心，SoC能够在单一芯片上实现多种功能，满足不同应用场景的需求。","categories":[{"name":"SoC","slug":"SoC","permalink":"https://www.silenceboy.com/categories/SoC/"}],"tags":[{"name":"SoC","slug":"SoC","permalink":"https://www.silenceboy.com/tags/SoC/"}]},{"title":"SOC的R核、A核有什么区别","slug":"SOC的R核、A核有什么区别","date":"2024-10-21T06:22:57.000Z","updated":"2024-10-21T06:23:53.000Z","comments":true,"path":"2024/10/21/SOC的R核、A核有什么区别/","permalink":"https://www.silenceboy.com/2024/10/21/SOC%E7%9A%84R%E6%A0%B8%E3%80%81A%E6%A0%B8%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB/","excerpt":"","text":"在系统级芯片（SoC, System on Chip）设计中，R核和A核是指不同类型的处理器核心，它们各自具有不同的用途和特点。 R核（Real-time Core）R核通常指的是实时处理器核心，主要用于处理实时任务。以下是R核的一些特点和应用： 实时性：R核设计用于处理需要严格时间约束的任务，确保任务在规定时间内完成。 确定性：R核通常具有确定的响应时间，适合用于实时操作系统（RTOS）。 低延迟：R核的设计目标是尽量减少处理延迟，以确保快速响应外部事件。 低功耗：R核通常功耗较低，适合用于电池供电的嵌入式设备。 应用场景 工业自动化控制 汽车电子（如引擎控制单元） 医疗设备 物联网设备 A核（Application Core）A核通常指的是应用处理器核心，主要用于运行复杂的应用程序和操作系统。以下是A核的一些特点和应用： 高性能：A核设计用于处理复杂计算任务，具有较高的处理能力。 多任务处理：A核通常支持多任务处理，能够运行完整的操作系统（如Linux、Android）。 丰富的外设支持：A核通常集成了丰富的外设接口，如USB、HDMI、以太网等。 较高功耗：由于需要处理复杂任务，A核的功耗通常较高。 应用场景 智能手机和平板电脑 个人计算机 多媒体设备（如智能电视和机顶盒） 高级嵌入式系统（如无人机和机器人） 例子：ARM Cortex 系列在ARM的Cortex系列处理器中，R核和A核的区别非常明显： Cortex-R 系列：这是ARM的实时处理器系列，设计用于高可靠性和实时性应用，如Cortex-R5和Cortex-R8。 Cortex-A 系列：这是ARM的应用处理器系列，设计用于高性能和复杂应用，如Cortex-A53、Cortex-A72等。 SoC中的组合在许多SoC设计中，R核和A核常常被组合使用，以便同时满足实时处理和高性能计算的需求。例如： 智能手机SoC：通常包含多个Cortex-A系列核心用于运行操作系统和应用程序，同时可能包含一个或多个Cortex-R系列核心用于处理实时任务，如基带处理。 汽车电子SoC：可能包含Cortex-R系列核心用于引擎控制和安全系统，同时包含Cortex-A系列核心用于信息娱乐系统。 总结 R核（Real-time Core）：用于实时任务，强调低延迟和确定性，适合工业控制、汽车电子等领域。 A核（Application Core）：用于运行复杂应用程序和操作系统，强调高性能和多任务处理，适合智能手机、平板电脑等设备。 通过结合使用R核和A核，SoC能够在单一芯片上实现多种功能，满足不同应用场景的需求。","categories":[{"name":"SoC","slug":"SoC","permalink":"https://www.silenceboy.com/categories/SoC/"}],"tags":[{"name":"SoC","slug":"SoC","permalink":"https://www.silenceboy.com/tags/SoC/"}]},{"title":"docker容器默认用户非root，且不知道密码，无法执行sudo操作，该怎么解决？","slug":"docker容器默认用户非root，且不知道密码，无法执行sudo操作，该怎么解决？","date":"2024-06-28T07:22:18.000Z","updated":"2024-06-28T07:23:01.000Z","comments":true,"path":"2024/06/28/docker容器默认用户非root，且不知道密码，无法执行sudo操作，该怎么解决？/","permalink":"https://www.silenceboy.com/2024/06/28/docker%E5%AE%B9%E5%99%A8%E9%BB%98%E8%AE%A4%E7%94%A8%E6%88%B7%E9%9D%9Eroot%EF%BC%8C%E4%B8%94%E4%B8%8D%E7%9F%A5%E9%81%93%E5%AF%86%E7%A0%81%EF%BC%8C%E6%97%A0%E6%B3%95%E6%89%A7%E8%A1%8Csudo%E6%93%8D%E4%BD%9C%EF%BC%8C%E8%AF%A5%E6%80%8E%E4%B9%88%E8%A7%A3%E5%86%B3%EF%BC%9F/","excerpt":"","text":"如果你在 Docker 容器中默认用户是 dev，但不知道密码，无法执行 sudo 操作，你可以通过以下几种方法来解决这个问题： 方法一：以 root 用户重新启动容器你可以以 root 用户重新启动容器，然后更改 dev 用户的密码或者直接执行你需要的操作。 1docker exec -u root -it &lt;container_id_or_name&gt; /bin/bash 方法二：修改 Dockerfile 重新构建镜像如果你有 Dockerfile，可以修改 Dockerfile 来设置 dev 用户的密码或者直接使用 root 用户。 例如，修改 Dockerfile 来设置 dev 用户的密码： 123456FROM your_base_image# 设置 dev 用户的密码RUN echo &#x27;dev:your_new_password&#x27; | chpasswd# 其他 Dockerfile 指令 然后重新构建镜像并运行容器： 12docker build -t your_image_name .docker run -it your_image_name 方法三：创建新的镜像并修改用户配置如果你没有 Dockerfile，但可以访问容器的镜像，你可以创建一个新的镜像并修改用户配置。 首先，启动一个新的容器并进入： 1docker run -it --name temp_container your_image_name /bin/bash 在容器中修改 dev 用户的密码： 1echo &#x27;dev:your_new_password&#x27; | chpasswd 退出容器并提交更改： 12exitdocker commit temp_container your_new_image_name 使用新的镜像启动容器： 1docker run -it your_new_image_name 方法四：直接进入容器并修改密码如果容器已经在运行，你可以直接进入容器并修改 dev 用户的密码。 首先，以 root 用户进入容器： 1docker exec -u root -it &lt;container_id_or_name&gt; /bin/bash 在容器中修改 dev 用户的密码： 1echo &#x27;dev:your_new_password&#x27; | chpasswd 方法五：使用 docker commit 修改现有容器如果你已经有一个运行中的容器，可以使用 docker commit 来创建一个新的镜像，并在新镜像中修改 dev 用户的密码。 首先，以 root 用户进入容器： 1docker exec -u root -it &lt;container_id_or_name&gt; /bin/bash 在容器中修改 dev 用户的密码： 1echo &#x27;dev:your_new_password&#x27; | chpasswd 退出容器并提交更改： 12exitdocker commit &lt;container_id_or_name&gt; your_new_image_name 使用新的镜像启动容器： 1docker run -it your_new_image_name 通过这些方法，你可以解决无法执行 sudo 操作的问题，并根据需要修改 dev 用户的密码或直接使用 root 用户。","categories":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/tags/docker/"}]},{"title":"代码扫描工具：sonar、fireline、coverity、fortify、blackduck对比","slug":"代码扫描工具：sonar、fireline、coverity、fortify、blackduck对比","date":"2024-06-24T08:02:01.000Z","updated":"2024-06-24T08:10:17.000Z","comments":true,"path":"2024/06/24/代码扫描工具：sonar、fireline、coverity、fortify、blackduck对比/","permalink":"https://www.silenceboy.com/2024/06/24/%E4%BB%A3%E7%A0%81%E6%89%AB%E6%8F%8F%E5%B7%A5%E5%85%B7%EF%BC%9Asonar%E3%80%81fireline%E3%80%81coverity%E3%80%81fortify%E3%80%81blackduck%E5%AF%B9%E6%AF%94/","excerpt":"","text":"代码扫描工具在软件开发中扮演着重要的角色，它们帮助开发者识别和修复代码中的潜在问题。以下是对 SonarQube、Fireline、Coverity、Fortify 和 Black Duck 的介绍及其区别： 1. SonarQube简介: SonarQube 是一个开源的代码质量管理工具。 支持多种编程语言，如 Java、C#、JavaScript、TypeScript、Python 等。 功能: 静态代码分析：检测代码中的漏洞、错误和代码异味。 代码度量：提供代码覆盖率、复杂度等度量指标。 集成：与 CI&#x2F;CD 工具（如 Jenkins、GitLab CI）集成，自动化代码质量检查。 优点: 开源且社区活跃。 支持多种插件，扩展功能强大。 直观的用户界面，易于使用。 2. Fireline简介: Fireline 是一个专注于代码安全的静态分析工具。 功能: 检测代码中的安全漏洞，如 SQL 注入、跨站脚本攻击（XSS）等。 提供详细的漏洞报告和修复建议。 优点: 专注于安全漏洞检测，提供深度分析。 适用于需要高安全性的应用程序开发。 3. Coverity简介: Coverity 是 Synopsys 提供的静态代码分析工具。 支持多种编程语言，如 C、C++、Java、C#、JavaScript 等。 功能: 静态代码分析：检测代码中的缺陷和安全漏洞。 自动化分析：与 CI&#x2F;CD 工具集成，实现自动化代码扫描。 报告和修复建议：提供详细的缺陷报告和修复建议。 优点: 高准确性和低误报率。 强大的企业级支持和服务。 支持广泛的编程语言和复杂代码库。 4. Fortify简介: Fortify 是 Micro Focus 提供的应用安全测试工具。 包含静态应用安全测试（SAST）和动态应用安全测试（DAST）。 功能: 静态代码分析：检测代码中的安全漏洞。 动态分析：在运行时检测应用程序的安全漏洞。 安全审计和合规性检查。 优点: 综合的安全测试解决方案，覆盖静态和动态分析。 强大的企业级支持和服务。 提供详细的漏洞修复指导。 5. Black Duck简介: Black Duck 是 Synopsys 提供的开源安全和管理工具。 专注于开源组件的安全和合规性。 功能: 开源组件扫描：检测项目中使用的开源组件。 漏洞检测：识别开源组件中的已知漏洞。 许可证合规性：检查开源组件的许可证合规性。 优点: 专注于开源组件的管理和安全。 提供详细的漏洞和许可证合规报告。 与其他 Synopsys 工具集成，提供全面的安全解决方案。 区别总结 SonarQube：主要关注代码质量和代码异味，提供全面的代码度量和质量报告。 Fireline：专注于代码安全漏洞的检测，适用于需要高安全性的应用。 Coverity：提供高准确性的静态代码分析，适合复杂和大型代码库。 Fortify：综合的应用安全测试工具，覆盖静态和动态分析，适合全面的安全测试需求。 Black Duck：专注于开源组件的安全和合规性管理，适用于使用大量开源组件的项目。 每种工具都有其特定的优势和应用场景，选择合适的工具应根据项目的具体需求和环境来决定。","categories":[{"name":"代码扫描","slug":"代码扫描","permalink":"https://www.silenceboy.com/categories/%E4%BB%A3%E7%A0%81%E6%89%AB%E6%8F%8F/"}],"tags":[{"name":"代码扫描","slug":"代码扫描","permalink":"https://www.silenceboy.com/tags/%E4%BB%A3%E7%A0%81%E6%89%AB%E6%8F%8F/"}]},{"title":"shell脚本里的#*@是什么意思","slug":"shell脚本里的-是什么意思","date":"2023-09-27T09:22:45.000Z","updated":"2023-12-05T01:44:02.000Z","comments":true,"path":"2023/09/27/shell脚本里的-是什么意思/","permalink":"https://www.silenceboy.com/2023/09/27/shell%E8%84%9A%E6%9C%AC%E9%87%8C%E7%9A%84-%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D/","excerpt":"","text":"在Shell脚本中，$&#123;variable#pattern&#125; 是一种字符串处理方式，其中 # 后跟着一个模式（pattern）。这个语法的作用是从字符串变量 variable 的开头删除匹配 pattern 的最短子串，并返回删除后的结果。$&#123;variable#*@&#125; 的含义是： ● variable 是一个字符串变量，通常是一个包含文本的字符串。 ● # 表示从字符串开头开始匹配。 ● *@ 是一个通配符模式，它匹配字符串中的任意字符序列，直到第一个 @ 字符。 所以，$&#123;variable#*@&#125; 的作用是从变量 variable 的开头删除匹配 *@ 模式的最短子串，并返回删除后的结果。通常，这种操作用于处理文本或字符串，以过滤掉或提取感兴趣的部分。例如，如果 variable 的值是 &quot;user@example.com&quot;，那么 $&#123;variable#*@&#125; 的结果将是 &quot;example.com&quot;，因为它删除了字符串中第一个 &quot;@&quot; 符号及其之前的部分。","categories":[{"name":"shell","slug":"shell","permalink":"https://www.silenceboy.com/categories/shell/"}],"tags":[{"name":"shell","slug":"shell","permalink":"https://www.silenceboy.com/tags/shell/"}]},{"title":"用go开发sse接口","slug":"用go开发sse接口","date":"2023-07-11T02:50:24.000Z","updated":"2023-07-21T05:36:57.000Z","comments":true,"path":"2023/07/11/用go开发sse接口/","permalink":"https://www.silenceboy.com/2023/07/11/%E7%94%A8go%E5%BC%80%E5%8F%91sse%E6%8E%A5%E5%8F%A3/","excerpt":"","text":"开发环境： 系统： ubuntu20.04 go版本： 1.19 编辑器： goland sse 服务器发送事件(Server-Sent Events)，服务端向客户端单向传递消息。在一些只需要接受服务端数据的需求中可以取代websocket技术，使用起来也很简单。 目前go已经有一些支持sse的库，我这里选用了：eventsource.v1 1$ go get gopkg.in/antage/eventsource.v1 广播模式SSE广播模式SSE指不设置事件名，或者说是不设置通道，所有客户端接收同样的数据。 具体代码实现如下：点击查看源码 服务端 1234567891011121314151617181920212223242526272829303132package mainimport ( &quot;fmt&quot; &quot;gopkg.in/antage/eventsource.v1&quot; &quot;log&quot; &quot;net/http&quot; &quot;time&quot;)// 广播模式SSE，不设置事件名称（也可理解为通道）func main() &#123; es := eventsource.New(nil, nil) defer es.Close() http.Handle(&quot;/&quot;, http.FileServer(http.Dir(&quot;./public&quot;))) http.Handle(&quot;/events&quot;, es) go func() &#123; for &#123; // 只设置发送数据，不添加事件名 es.SendEventMessage(fmt.Sprintf(&quot;send data: %s&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;)), &quot;&quot;, &quot;&quot;) log.Printf(&quot;客户端连接数: %d&quot;, es.ConsumersCount()) time.Sleep(2 * time.Second) &#125; &#125;() log.Println(&quot;Open URL http://localhost:8080/ in your browser.&quot;) err := http.ListenAndServe(&quot;:8080&quot;, nil) if err != nil &#123; log.Fatal(err) &#125;&#125; 前端 123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;SSE test&lt;/title&gt; &lt;script type=&quot;text/javascript&quot;&gt; const es = new EventSource(&quot;http://localhost:8080/events&quot;); es.onmessage = function (e) &#123; document.getElementById(&quot;test&quot;) .insertAdjacentHTML(&quot;beforeend&quot;, &quot;&lt;li&gt;&quot; + e.data + &quot;&lt;/li&gt;&quot;); &#125; es.onerror = function (e) &#123; // readyState说明 // 0：浏览器与服务端尚未建立连接或连接已被关闭 // 1：浏览器与服务端已成功连接，浏览器正在处理接收到的事件及数据 // 2：浏览器与服务端建立连接失败，客户端不再继续建立与服务端之间的连接 console.log(&quot;readyState = &quot; + e.currentTarget.readyState); &#125; &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;SSE test&lt;/h1&gt;&lt;div&gt; &lt;ul id=&quot;test&quot;&gt; &lt;/ul&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 启动服务端代码： 1$ go run main 打开浏览器访问前端代码： http://localhost:8080 服务端输出： 前端输出： 点对点模式SSE点击查看源代码 实现方式和广播模式差不多，只需做简单修改： 服务端代码只需添加事件名称： 12// 设置事件名称为：test-eventes.SendEventMessage(fmt.Sprintf(&quot;send data: %s&quot;, time.Now().Format(&quot;2006-01-02 15:04:05&quot;)), &quot;test-event&quot;, &quot;&quot;) 前端代码修改接收方式： 1234es.addEventListener(&quot;test-event&quot;, (e) =&gt; &#123; document.getElementById(&quot;test&quot;) .insertAdjacentHTML(&quot;beforeend&quot;, &quot;&lt;li&gt;&quot; + e.data + &quot;&lt;/li&gt;&quot;);&#125;); 支持跨域的SSE点击查看源代码 现在项目开发基本上都是前后端分离，这样就会存在跨域问题，SSE解决跨域的方式只需要在new方法内增加允许跨域请求头： 12345678es := eventsource.New( eventsource.DefaultSettings(), func(req *http.Request) [][]byte &#123; return [][]byte&#123; []byte(&quot;X-Accel-Buffering: no&quot;), []byte(&quot;Access-Control-Allow-Origin: *&quot;), &#125; &#125;) 前端创建sse连接时也可添加允许跨域参数： 1const es = new EventSource(&quot;http://localhost:8080/events&quot;, &#123; withCredentials: true &#125;); 解决火狐浏览器断开不会自动重连问题点击查看源代码 在Chrome浏览器中sse断开后会自动重连，但firefox浏览器中断开后不会重连，解决办法是，前端js通过判断连接状态主动进行重连请求，通过判断readyState的值进行重新调用初始化操作 readyState说明： 0：浏览器与服务端尚未建立连接或连接已被关闭 1：浏览器与服务端已成功连接，浏览器正在处理接收到的事件及数据 2：浏览器与服务端建立连接失败，客户端不再继续建立与服务端之间的连接 前端代码可修改为如下： 12345678910111213141516171819202122232425262728293031323334353637&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;SSE test&lt;/title&gt; &lt;script type=&quot;text/javascript&quot;&gt; let es = null; // 解决火狐浏览器断开不会自动重连问题 function initES() &#123; if (es == null || es.readyState == 2) &#123; es = new EventSource(&quot;http://localhost:8080/events&quot;, &#123;withCredentials: true&#125;); es.addEventListener(&quot;test-event&quot;, (e) =&gt; &#123; document.getElementById(&quot;test&quot;) .insertAdjacentHTML(&quot;beforeend&quot;, &quot;&lt;li&gt;&quot; + e.data + &quot;&lt;/li&gt;&quot;); &#125;); es.onerror = function (e) &#123; // readyState说明 // 0：浏览器与服务端尚未建立连接或连接已被关闭 // 1：浏览器与服务端已成功连接，浏览器正在处理接收到的事件及数据 // 2：浏览器与服务端建立连接失败，客户端不再继续建立与服务端之间的连接 console.log(&quot;readyState = &quot; + e.currentTarget.readyState); if (es.readyState == 2) &#123; setTimeout(initES, 5000) &#125; &#125; &#125; &#125; initES() &lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;SSE test&lt;/h1&gt;&lt;div&gt; &lt;ul id=&quot;test&quot;&gt; &lt;/ul&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;","categories":[{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/categories/go/"}],"tags":[{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/tags/go/"}]},{"title":"grpc系列课程（五）：grpc调试工具","slug":"grpc系列课程（五）：grpc调试工具","date":"2023-06-26T02:38:01.000Z","updated":"2023-07-21T05:36:57.000Z","comments":true,"path":"2023/06/26/grpc系列课程（五）：grpc调试工具/","permalink":"https://www.silenceboy.com/2023/06/26/grpc%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9Agrpc%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7/","excerpt":"","text":"开发环境： 系统： ubuntu20.04 go版本： 1.19 编辑器： goland 平时我们编写http的api接口时能够很方便的通过postman工具进行接口调试，那么grpc接口是否也有类似postman的工具可以调试呐？当然可以，github上有一款工具grpcui，专门用来在浏览器中进行grpc接口调试。 安装grpcui工具1$ go install github.com/fullstorydev/grpcui/cmd/grpcui@latest 项目测试我们以demo_1测试代码为例进行调试。首先在项目中安装依赖： 1$ go get github.com/fullstorydev/grpcui 运行项目1$ go run server/main.go 启动grpcui另起一个终端，执行一下命令，注意这里的端口号要和grpc服务端口号保持一致。 1$ grpcui -plaintext 127.0.0.1:8080 如果你在执行以上命令的时候出现一下报错： 1Failed to compute set of methods to expose: server does not support the reflection API 需要在server/main.go文件中添加如下代码，增加反射： 1reflection.Register(s) 此时再运行以上启动grpcui命令，可看到一下输出： 12gRPC Web UI available at http://127.0.0.1:41619/Opening in existing browser session. 浏览器调试浏览器打开http://127.0.0.1:41619/ ,可以看到grpcui调试页面： 点击Invoke按钮，可以看到Responsetab页有对于的相应数据：","categories":[{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/categories/go/"}],"tags":[{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/tags/go/"}]},{"title":"grpc系列课程（四）：双向流式rpc","slug":"grpc系列课程（四）：双向流式rpc","date":"2023-06-18T07:13:21.000Z","updated":"2023-07-21T05:36:57.000Z","comments":true,"path":"2023/06/18/grpc系列课程（四）：双向流式rpc/","permalink":"https://www.silenceboy.com/2023/06/18/grpc%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E5%8F%8C%E5%90%91%E6%B5%81%E5%BC%8Frpc/","excerpt":"","text":"开发环境： 系统： ubuntu20.04 go版本： 1.19 编辑器： goland grpc：远程过程调用，使用场景很多，也是比较流行的技术之一。使用go开发grpc服务，除了必须的go语言开发环境之外，还需要安装grpc相关命令。 grpc环境配置protoc安装1234$ sudo apt install -y protobuf-compiler$ protoc --versionlibprotoc 3.6.1 如果是其他系统电脑，安装protoc可参考文档：Protocol Buffer Compiler Installation protocol编译插件安装12$ go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28$ go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2 安装完成后可以在bin目录下看到相关指令： 123$ ls $GOPATH/binprotoc-gen-go protoc-gen-go-grpc 项目开发项目源码地址 项目目录结构123456789101112├── demo_4│ ├── client│ │ └── main.go│ ├── go.mod│ ├── go.sum│ ├── helloworld│ │ ├── helloworld_grpc.pb.go│ │ ├── helloworld.pb.go│ │ └── helloworld.proto│ ├── README.md│ └── server│ └── main.go 项目创建12$ mkdir demo_4 &amp;&amp; cd demo_4$ go mod init 安装grpc依赖1$ go get -u google.golang.org/grpc 编写proto文件流式rpc使用stream关键字定义 123456789101112131415161718# helloworld/helloworld.protosyntax = &quot;proto3&quot;;option go_package = &quot;/helloworld&quot;;package helloworld;service Hello &#123; rpc SayHello (stream HelloRequest) returns (stream HelloReply) &#123;&#125;&#125;message HelloRequest &#123; string name = 1;&#125;message HelloReply &#123; string message = 1;&#125; 生成go代码123$ protoc --go_out=. --go_opt=paths=source_relative \\ --go-grpc_out=. --go-grpc_opt=paths=source_relative \\ helloworld/helloworld.proto 命令执行成功之后会在helloworld目录下生成两个文件： helloworld_grpc.pb.go和helloworld.pb.go，注意： 不要手动编辑这两个文件。 编写服务端代码flag用法可参考官方文档： https://pkg.go.dev/flag 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859// server/main.gopackage mainimport ( &quot;flag&quot; &quot;fmt&quot; &quot;github.com/silenceboychen/gostudy/demo_4/helloworld&quot; &quot;google.golang.org/grpc&quot; &quot;google.golang.org/grpc/reflection&quot; &quot;io&quot; &quot;log&quot; &quot;net&quot; &quot;strconv&quot;)var ( port = flag.Int(&quot;port&quot;, 8080, &quot;The server port&quot;))type server struct &#123; helloworld.UnimplementedHelloServer&#125;func (s *server) SayHello(stream helloworld.Hello_SayHelloServer) error &#123; n := 0 for &#123; res, err := stream.Recv() if err == io.EOF &#123; return nil &#125; if err != nil &#123; return err &#125; err = stream.Send(&amp;helloworld.HelloReply&#123; Message: &quot;server stream: &quot; + res.GetName() + &quot;_&quot; + strconv.Itoa(n), &#125;) if err != nil &#123; return err &#125; n++ log.Printf(&quot;client stream: %s&quot;, res.GetName()) &#125;&#125;func main() &#123; flag.Parse() lis, err := net.Listen(&quot;tcp&quot;, fmt.Sprintf(&quot;:%d&quot;, *port)) if err != nil &#123; log.Fatalf(&quot;failed to listen: %v&quot;, err) &#125; s := grpc.NewServer() reflection.Register(s) helloworld.RegisterHelloServer(s, &amp;server&#123;&#125;) log.Printf(&quot;server listening at %v&quot;, lis.Addr()) if err := s.Serve(lis); err != nil &#123; log.Fatalf(&quot;failed to serve: %v&quot;, err) &#125;&#125; 编写客户端代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// client/main.gopackage mainimport ( &quot;context&quot; &quot;flag&quot; &quot;github.com/silenceboychen/gostudy/demo_4/helloworld&quot; &quot;google.golang.org/grpc&quot; &quot;google.golang.org/grpc/credentials/insecure&quot; &quot;io&quot; &quot;log&quot; &quot;strconv&quot; &quot;time&quot;)var ( addr = flag.String(&quot;addr&quot;, &quot;localhost:8080&quot;, &quot;the address to connect to&quot;) name = flag.String(&quot;name&quot;, &quot;world&quot;, &quot;Name to greet&quot;))func main() &#123; flag.Parse() conn, err := grpc.Dial(*addr, grpc.WithTransportCredentials(insecure.NewCredentials())) if err != nil &#123; log.Fatalf(&quot;did not connect: %v&quot;, err) &#125; defer conn.Close() c := helloworld.NewHelloClient(conn) _, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() stream, err := c.SayHello(context.Background()) if err != nil &#123; log.Fatalf(&quot;stream err: %v&quot;, err) &#125; for n := 0; n &lt; 5; n++ &#123; err := stream.Send(&amp;helloworld.HelloRequest&#123;Name: *name + &quot;_&quot; + strconv.Itoa(n)&#125;) if err != nil &#123; log.Fatalf(&quot;client stream err: %v&quot;, err) &#125; res, err := stream.Recv() if err == io.EOF &#123; break &#125; if err != nil &#123; log.Fatalf(&quot;server stream err: %v&quot;, err) &#125; // 打印返回值 log.Println(res.GetMessage()) &#125; err = stream.CloseSend() if err != nil &#123; log.Fatalf(&quot;close stream err: %v&quot;, err) &#125;&#125; 项目运行开启两个终端，分别运行服务端代码和客户端代码，服务端代码要先运行。 服务端 12345678$ go run server/main.go2023/06/16 20:51:39 server listening at [::]:80802023/06/16 20:51:42 client stream: world_02023/06/16 20:51:42 client stream: world_12023/06/16 20:51:42 client stream: world_22023/06/16 20:51:42 client stream: world_32023/06/16 20:51:42 client stream: world_4 客户端 1234567$ go run client/main.go2023/06/16 20:51:42 server stream: world_0_02023/06/16 20:51:42 server stream: world_1_12023/06/16 20:51:42 server stream: world_2_22023/06/16 20:51:42 server stream: world_3_32023/06/16 20:51:42 server stream: world_4_4 下一篇将向大家介绍grpc调试工具。","categories":[{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/categories/go/"}],"tags":[{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/tags/go/"}]},{"title":"grpc系列课程（三）：客户端流式rpc","slug":"grpc系列课程（三）：客户端流式rpc","date":"2023-06-18T07:12:48.000Z","updated":"2023-07-21T05:36:57.000Z","comments":true,"path":"2023/06/18/grpc系列课程（三）：客户端流式rpc/","permalink":"https://www.silenceboy.com/2023/06/18/grpc%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%B5%81%E5%BC%8Frpc/","excerpt":"","text":"开发环境： 系统： ubuntu20.04 go版本： 1.19 编辑器： goland grpc：远程过程调用，使用场景很多，也是比较流行的技术之一。使用go开发grpc服务，除了必须的go语言开发环境之外，还需要安装grpc相关命令。 grpc环境配置protoc安装1234$ sudo apt install -y protobuf-compiler$ protoc --versionlibprotoc 3.6.1 如果是其他系统电脑，安装protoc可参考文档：Protocol Buffer Compiler Installation protocol编译插件安装12$ go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28$ go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2 安装完成后可以在bin目录下看到相关指令： 123$ ls $GOPATH/binprotoc-gen-go protoc-gen-go-grpc 项目开发项目源码地址 项目目录结构123456789101112├── demo_3│ ├── client│ │ └── main.go│ ├── go.mod│ ├── go.sum│ ├── helloworld│ │ ├── helloworld_grpc.pb.go│ │ ├── helloworld.pb.go│ │ └── helloworld.proto│ ├── README.md│ └── server│ └── main.go 项目创建12$ mkdir demo_3 &amp;&amp; cd demo_3$ go mod init 安装grpc依赖1$ go get -u google.golang.org/grpc 编写proto文件流式rpc使用stream关键字定义 123456789101112131415161718# helloworld/helloworld.protosyntax = &quot;proto3&quot;;option go_package = &quot;/helloworld&quot;;package helloworld;service Hello &#123; rpc SayHello (stream HelloRequest) returns (HelloReply) &#123;&#125;&#125;message HelloRequest &#123; string name = 1;&#125;message HelloReply &#123; string message = 1;&#125; 生成go代码123$ protoc --go_out=. --go_opt=paths=source_relative \\ --go-grpc_out=. --go-grpc_opt=paths=source_relative \\ helloworld/helloworld.proto 命令执行成功之后会在helloworld目录下生成两个文件： helloworld_grpc.pb.go和helloworld.pb.go，注意： 不要手动编辑这两个文件。 编写服务端代码flag用法可参考官方文档： https://pkg.go.dev/flag 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// server/main.gopackage mainimport ( &quot;flag&quot; &quot;fmt&quot; &quot;github.com/silenceboychen/gostudy/demo_3/helloworld&quot; &quot;google.golang.org/grpc&quot; &quot;google.golang.org/grpc/reflection&quot; &quot;io&quot; &quot;log&quot; &quot;net&quot;)var ( port = flag.Int(&quot;port&quot;, 8080, &quot;The server port&quot;))type server struct &#123; helloworld.UnimplementedHelloServer&#125;func (s *server) SayHello(stream helloworld.Hello_SayHelloServer) error &#123; for &#123; res, err := stream.Recv() if err == io.EOF &#123; return stream.SendAndClose(&amp;helloworld.HelloReply&#123;Message: &quot;over&quot;&#125;) &#125; if err != nil &#123; return err &#125; log.Println(res.Name) &#125;&#125;func main() &#123; flag.Parse() lis, err := net.Listen(&quot;tcp&quot;, fmt.Sprintf(&quot;:%d&quot;, *port)) if err != nil &#123; log.Fatalf(&quot;failed to listen: %v&quot;, err) &#125; s := grpc.NewServer() reflection.Register(s) helloworld.RegisterHelloServer(s, &amp;server&#123;&#125;) log.Printf(&quot;server listening at %v&quot;, lis.Addr()) if err := s.Serve(lis); err != nil &#123; log.Fatalf(&quot;failed to serve: %v&quot;, err) &#125;&#125; 编写客户端代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950// client/main.gopackage mainimport ( &quot;context&quot; &quot;flag&quot; &quot;github.com/silenceboychen/gostudy/demo_3/helloworld&quot; &quot;google.golang.org/grpc&quot; &quot;google.golang.org/grpc/credentials/insecure&quot; &quot;log&quot; &quot;strconv&quot; &quot;time&quot;)var ( addr = flag.String(&quot;addr&quot;, &quot;localhost:8080&quot;, &quot;the address to connect to&quot;) name = flag.String(&quot;name&quot;, &quot;world&quot;, &quot;Name to greet&quot;))func main() &#123; flag.Parse() conn, err := grpc.Dial(*addr, grpc.WithTransportCredentials(insecure.NewCredentials())) if err != nil &#123; log.Fatalf(&quot;did not connect: %v&quot;, err) &#125; defer conn.Close() c := helloworld.NewHelloClient(conn) ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() stream, err := c.SayHello(ctx) if err != nil &#123; log.Fatalf(&quot;Upload list err: %v&quot;, err) &#125; for n := 0; n &lt; 5; n++ &#123; //向流中发送消息 err := stream.Send(&amp;helloworld.HelloRequest&#123;Name: &quot;stream client rpc &quot; + strconv.Itoa(n)&#125;) if err != nil &#123; log.Fatalf(&quot;stream request err: %v&quot;, err) &#125; &#125; //关闭流并获取返回的消息 res, err := stream.CloseAndRecv() if err != nil &#123; log.Fatalf(&quot;SayHello get response err: %v&quot;, err) &#125; log.Println(res)&#125; 项目运行开启两个终端，分别运行服务端代码和客户端代码，服务端代码要先运行。 服务端 12345678$ go run server/main.go2023/06/16 20:24:15 server listening at [::]:80802023/06/16 20:24:22 stream client rpc 02023/06/16 20:24:22 stream client rpc 12023/06/16 20:24:22 stream client rpc 22023/06/16 20:24:22 stream client rpc 32023/06/16 20:24:22 stream client rpc 4 客户端 123$ go run client/main.go2023/06/16 20:24:22 message:&quot;over&quot; 下一篇将向大家介绍双向流式rpc。","categories":[{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/categories/go/"}],"tags":[{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/tags/go/"}]},{"title":"grpc系列课程（二）：服务端流式rpc","slug":"grpc系列课程（二）：服务端流式rpc","date":"2023-06-18T07:11:52.000Z","updated":"2023-07-21T05:36:57.000Z","comments":true,"path":"2023/06/18/grpc系列课程（二）：服务端流式rpc/","permalink":"https://www.silenceboy.com/2023/06/18/grpc%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%B5%81%E5%BC%8Frpc/","excerpt":"","text":"开发环境： 系统： ubuntu20.04 go版本： 1.19 编辑器： goland grpc：远程过程调用，使用场景很多，也是比较流行的技术之一。使用go开发grpc服务，除了必须的go语言开发环境之外，还需要安装grpc相关命令。 grpc环境配置protoc安装1234$ sudo apt install -y protobuf-compiler$ protoc --versionlibprotoc 3.6.1 如果是其他系统电脑，安装protoc可参考文档：Protocol Buffer Compiler Installation protocol编译插件安装12$ go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28$ go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2 安装完成后可以在bin目录下看到相关指令： 123$ ls $GOPATH/binprotoc-gen-go protoc-gen-go-grpc 项目开发项目源码地址 项目目录结构123456789101112├── demo_2│ ├── client│ │ └── main.go│ ├── go.mod│ ├── go.sum│ ├── helloworld│ │ ├── helloworld_grpc.pb.go│ │ ├── helloworld.pb.go│ │ └── helloworld.proto│ ├── README.md│ └── server│ └── main.go 项目创建12$ mkdir demo_2 &amp;&amp; cd demo_2$ go mod init 安装grpc依赖1$ go get -u google.golang.org/grpc 编写proto文件流式rpc使用stream关键字定义 123456789101112131415161718# helloworld/helloworld.protosyntax = &quot;proto3&quot;;option go_package = &quot;/helloworld&quot;;package helloworld;service Hello &#123; rpc SayHello (HelloRequest) returns (stream HelloReply) &#123;&#125;&#125;message HelloRequest &#123; string name = 1;&#125;message HelloReply &#123; string message = 1;&#125; 生成go代码123$ protoc --go_out=. --go_opt=paths=source_relative \\ --go-grpc_out=. --go-grpc_opt=paths=source_relative \\ helloworld/helloworld.proto 命令执行成功之后会在helloworld目录下生成两个文件： helloworld_grpc.pb.go和helloworld.pb.go，注意： 不要手动编辑这两个文件。 编写服务端代码flag用法可参考官方文档： https://pkg.go.dev/flag 1234567891011121314151617181920212223242526272829303132333435363738394041424344// server/main.gopackage mainimport ( &quot;flag&quot; &quot;fmt&quot; &quot;github.com/silenceboychen/gostudy/demo_2/helloworld&quot; &quot;google.golang.org/grpc&quot; &quot;google.golang.org/grpc/reflection&quot; &quot;log&quot; &quot;net&quot;)var ( port = flag.Int(&quot;port&quot;, 8080, &quot;The server port&quot;))type server struct &#123; helloworld.UnimplementedHelloServer&#125;func (s *server) SayHello(in *helloworld.HelloRequest, stream helloworld.Hello_SayHelloServer) error &#123; log.Printf(&quot;Received: %v&quot;, in.GetName()) for i := 0; i &lt; 5; i++ &#123; stream.Send(&amp;helloworld.HelloReply&#123;Message: fmt.Sprintf(&quot;hello %s---%d&quot;, in.Name, i)&#125;) &#125; return nil&#125;func main() &#123; flag.Parse() lis, err := net.Listen(&quot;tcp&quot;, fmt.Sprintf(&quot;:%d&quot;, *port)) if err != nil &#123; log.Fatalf(&quot;failed to listen: %v&quot;, err) &#125; s := grpc.NewServer() reflection.Register(s) helloworld.RegisterHelloServer(s, &amp;server&#123;&#125;) log.Printf(&quot;server listening at %v&quot;, lis.Addr()) if err := s.Serve(lis); err != nil &#123; log.Fatalf(&quot;failed to serve: %v&quot;, err) &#125;&#125; 编写客户端代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// client/main.gopackage mainimport ( &quot;context&quot; &quot;flag&quot; &quot;github.com/silenceboychen/gostudy/demo_2/helloworld&quot; &quot;google.golang.org/grpc&quot; &quot;google.golang.org/grpc/credentials/insecure&quot; &quot;io&quot; &quot;log&quot; &quot;time&quot;)var ( addr = flag.String(&quot;addr&quot;, &quot;localhost:8080&quot;, &quot;the address to connect to&quot;) name = flag.String(&quot;name&quot;, &quot;world&quot;, &quot;Name to greet&quot;))func main() &#123; flag.Parse() conn, err := grpc.Dial(*addr, grpc.WithTransportCredentials(insecure.NewCredentials())) if err != nil &#123; log.Fatalf(&quot;did not connect: %v&quot;, err) &#125; defer conn.Close() c := helloworld.NewHelloClient(conn) ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() stream, err := c.SayHello(ctx, &amp;helloworld.HelloRequest&#123;Name: *name&#125;) if err != nil &#123; log.Fatalf(&quot;could not call: %v&quot;, err) &#125; for &#123; res, err := stream.Recv() if err == io.EOF &#123; break &#125; if err != nil &#123; log.Printf(&quot;stream error: %v&quot;, err) &#125; log.Printf(&quot;%s&quot;, res.Message) &#125;&#125; 项目运行开启两个终端，分别运行服务端代码和客户端代码，服务端代码要先运行。 服务端 1234$ go run server/main.go2023/06/16 18:56:57 server listening at [::]:80802023/06/16 18:57:02 Received: world 客户端 1234567$ go run client/main.go2023/06/16 20:11:55 hello world---02023/06/16 20:11:55 hello world---12023/06/16 20:11:55 hello world---22023/06/16 20:11:55 hello world---32023/06/16 20:11:55 hello world---4 下一篇将向大家介绍客户端流式rpc。","categories":[{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/categories/go/"}],"tags":[{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/tags/go/"}]},{"title":"grpc系列课程（一）：单项rpc","slug":"grpc系列课程（一）：单项rpc","date":"2023-06-18T07:09:39.000Z","updated":"2023-07-21T05:36:57.000Z","comments":true,"path":"2023/06/18/grpc系列课程（一）：单项rpc/","permalink":"https://www.silenceboy.com/2023/06/18/grpc%E7%B3%BB%E5%88%97%E8%AF%BE%E7%A8%8B%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%8D%95%E9%A1%B9rpc/","excerpt":"","text":"开发环境： 系统： ubuntu20.04 go版本： 1.19 编辑器： goland grpc：远程过程调用，使用场景很多，也是比较流行的技术之一。使用go开发grpc服务，除了必须的go语言开发环境之外，还需要安装grpc相关命令。 grpc环境配置protoc安装1234$ sudo apt install -y protobuf-compiler$ protoc --versionlibprotoc 3.6.1 如果是其他系统电脑，安装protoc可参考文档：Protocol Buffer Compiler Installation protocol编译插件安装12$ go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.28$ go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@v1.2 安装完成后可以在bin目录下看到相关指令： 123$ ls $GOPATH/binprotoc-gen-go protoc-gen-go-grpc 项目开发项目源码地址 项目目录结构123456789101112├── demo_1│ ├── client│ │ └── main.go│ ├── go.mod│ ├── go.sum│ ├── helloworld│ │ ├── helloworld_grpc.pb.go│ │ ├── helloworld.pb.go│ │ └── helloworld.proto│ ├── README.md│ └── server│ └── main.go 项目创建12$ mkdir demo_1 &amp;&amp; cd demo_1$ go mod init 安装grpc依赖1$ go get -u google.golang.org/grpc 编写proto文件123456789101112131415161718# helloworld/helloworld.protosyntax = &quot;proto3&quot;;option go_package = &quot;/helloworld&quot;;package helloworld;service Hello &#123; rpc SayHello (HelloRequest) returns (HelloReply) &#123;&#125;&#125;message HelloRequest &#123; string name = 1;&#125;message HelloReply &#123; string message = 1;&#125; 生成go代码123$ protoc --go_out=. --go_opt=paths=source_relative \\ --go-grpc_out=. --go-grpc_opt=paths=source_relative \\ helloworld/helloworld.proto 命令执行成功之后会在helloworld目录下生成两个文件： helloworld_grpc.pb.go和helloworld.pb.go，注意： 不要手动编辑这两个文件。 编写服务端代码flag用法可参考官方文档： https://pkg.go.dev/flag 123456789101112131415161718192021222324252627282930313233343536373839404142// server/main.gopackage mainimport ( &quot;context&quot; &quot;flag&quot; &quot;fmt&quot; &quot;github.com/silenceboychen/gostudy/demo_1/helloworld&quot; &quot;google.golang.org/grpc&quot; &quot;google.golang.org/grpc/reflection&quot; &quot;log&quot; &quot;net&quot;)var ( port = flag.Int(&quot;port&quot;, 8080, &quot;The server port&quot;))type server struct &#123; helloworld.UnimplementedHelloServer&#125;func (s *server) SayHello(ctx context.Context, in *helloworld.HelloRequest) (*helloworld.HelloReply, error) &#123; log.Printf(&quot;Received: %v&quot;, in.GetName()) return &amp;helloworld.HelloReply&#123;Message: &quot;Hello &quot; + in.GetName()&#125;, nil&#125;func main() &#123; flag.Parse() lis, err := net.Listen(&quot;tcp&quot;, fmt.Sprintf(&quot;:%d&quot;, *port)) if err != nil &#123; log.Fatalf(&quot;failed to listen: %v&quot;, err) &#125; s := grpc.NewServer() reflection.Register(s) helloworld.RegisterHelloServer(s, &amp;server&#123;&#125;) log.Printf(&quot;server listening at %v&quot;, lis.Addr()) if err := s.Serve(lis); err != nil &#123; log.Fatalf(&quot;failed to serve: %v&quot;, err) &#125;&#125; 编写客户端代码12345678910111213141516171819202122232425262728293031323334353637// client/main.gopackage mainimport ( &quot;context&quot; &quot;flag&quot; &quot;github.com/silenceboychen/gostudy/demo_1/helloworld&quot; &quot;google.golang.org/grpc&quot; &quot;google.golang.org/grpc/credentials/insecure&quot; &quot;log&quot; &quot;time&quot;)var ( addr = flag.String(&quot;addr&quot;, &quot;localhost:8080&quot;, &quot;the address to connect to&quot;) name = flag.String(&quot;name&quot;, &quot;world&quot;, &quot;Name to greet&quot;))func main() &#123; flag.Parse() conn, err := grpc.Dial(*addr, grpc.WithTransportCredentials(insecure.NewCredentials())) if err != nil &#123; log.Fatalf(&quot;did not connect: %v&quot;, err) &#125; defer conn.Close() c := helloworld.NewHelloClient(conn) ctx, cancel := context.WithTimeout(context.Background(), time.Second) defer cancel() r, err := c.SayHello(ctx, &amp;helloworld.HelloRequest&#123;Name: *name&#125;) if err != nil &#123; log.Fatalf(&quot;could not greet: %v&quot;, err) &#125; log.Printf(&quot;Greeting: %s&quot;, r.GetMessage())&#125; 项目运行开启两个终端，分别运行服务端代码和客户端代码，服务端代码要先运行。 服务端 1234$ go run server/main.go2023/06/16 18:56:57 server listening at [::]:80802023/06/16 18:57:02 Received: world 客户端 123$ go run client/main.go2023/06/16 18:57:02 Greeting: Hello world 以上便是go实现单项rpc的所有内容，如果一切顺利，恭喜你，以及成功入门grpc，下一篇将向大家介绍服务端流式rpc。","categories":[{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/categories/go/"}],"tags":[{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/tags/go/"}]},{"title":"ubuntu20.04禁用搜狗输入法繁简体快捷键设置","slug":"ubuntu20-04禁用搜狗输入法繁简体快捷键设置","date":"2023-06-14T03:00:52.000Z","updated":"2025-08-27T06:44:25.000Z","comments":true,"path":"2023/06/14/ubuntu20-04禁用搜狗输入法繁简体快捷键设置/","permalink":"https://www.silenceboy.com/2023/06/14/ubuntu20-04%E7%A6%81%E7%94%A8%E6%90%9C%E7%8B%97%E8%BE%93%E5%85%A5%E6%B3%95%E7%B9%81%E7%AE%80%E4%BD%93%E5%BF%AB%E6%8D%B7%E9%94%AE%E8%AE%BE%E7%BD%AE/","excerpt":"","text":"最近在ubuntu20.04系统上安装了搜狗输入法之后，发现vscode，idea，golang等编辑器的全局搜索快捷键Ctrl+Shift+F无法使用了，经排查发现是因为搜狗输入法占用了该快捷键，是搜狗输入法的繁简体切换快捷键。打开搜狗输入法的设置页面可以在高级选项里看到。 但是在设置里取消简繁切换快捷键的勾选保存之后发现无效果，快捷键依然被占用。 正确的解决方案是： 修改搜狗输入法配置文件 1$ vim ~/.config/sogoupinyin/conf/env.ini 找到ShortCutFanJian配置，将值改为0；默认1，等于开启；改为0表示关闭 打开系统输入法配置 附加组件中找到简繁转换，点击配置 将默认快捷键设置为空, 保存即可。 如果发现未生效，需要重启电脑。","categories":[{"name":"linux","slug":"linux","permalink":"https://www.silenceboy.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://www.silenceboy.com/tags/linux/"}]},{"title":"gomobile开发安卓应用环境搭建完整流程","slug":"gomobile开发安卓应用环境搭建完整流程","date":"2023-06-08T05:33:05.000Z","updated":"2023-06-08T05:34:01.000Z","comments":true,"path":"2023/06/08/gomobile开发安卓应用环境搭建完整流程/","permalink":"https://www.silenceboy.com/2023/06/08/gomobile%E5%BC%80%E5%8F%91%E5%AE%89%E5%8D%93%E5%BA%94%E7%94%A8%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E5%AE%8C%E6%95%B4%E6%B5%81%E7%A8%8B/","excerpt":"","text":"go环境搭建不在这里赘述。 以下内容的执行环境为： 系统：ubuntu20.04 go版本：v1.19 项目创建执行以下命令创建一个go开发安卓应用的测试目录： 12$ mkdir $GOPATH/src/goapp &amp;&amp; cd $GOPATH/src/goapp$ go mod init 在该目录下执行以下命令获取官方提供的示例项目： 1$ go get -d golang.org/x/mobile/example/basic 安装gomibile12$ go install golang.org/x/mobile/cmd/gomobile@latest$ gomobile init 然后执行以下命令打包安卓应用： 1$ gomobile build -target=android -androidapi 19 golang.org/x/mobile/example/basic 此时会发现以下相关错误： gomobile: could not locate Android SDK: stat /home/test/Android/Sdk: no such file or directory; Android SDK was not found at /home/test/Android/Sdk gomobile: no usable NDK in /home/test/Android/Sdk: open /home/test/Android/Sdk/ndk: no such file or directory, open /home/test/Android/Sdk/ndk-bundle/meta/platforms.json: no such file or directory 这是因为本地没有配置安卓开发环境导致的。 安卓开发环境搭建Android Studio安装访问谷歌中国开发者网站下载 Android Studio 编辑器：https://developer.android.google.cn/studio 下载完成后执行以下操作： 12345678910111213141516# 将安装包移到/opt目录下，需要管理员权限$ sudo mv android-studio-2022.2.1.20-linux.tar.gz /opt# 进入/opt目录$ cd /opt# 解压文件，需要管理员权限$ sudo tar -xzvf android-studio-2022.2.1.20-linux.tar.gz# 运行Android Studio$ ./android-studio/bin/studio.sh==========================================================# 如果想在任意位置打开android studio，可配置软连接$ sudo ln -s /opt/android-studio/bin/studio.sh /usr/bin/android-studio# 配置完成以后在任意位置执行android-studio即可打开应用$ android-studio 第一次打开android-studio需要进行一些配置，一直选择下一步设置即可，其中有两个地方需要注意： 选择自定义安装 插件安装，可以全选 插件安装完成之后点击Finish即可打开应用。 安装NDKAndroid studio安装完成后并没有万事大吉，默认并没安装NDK，需要自己手工再安装。 点击ok会自动下载选择的插件。 编译安卓应用此时继续回到之前的项目目录，执行安卓构建命令 1$ gomobile build -target=android -androidapi 19 golang.org/x/mobile/example/basic 这一次没有出现报错，并且目录下多了一个basic.apk文件，该文件即为打包成功的安卓应用，可以安装一个安卓模拟器进行测试了。 安装安卓模拟器模拟器我选用了Anbox 1$ sudo snap install --devmode --edge anbox 安装完成之后执行以下命令启动安卓模拟器： 1$ anbox.appmgr 我比较顺利没有遇到报错，如果遇到模拟器启动报错，可以参考文章：https://juejin.cn/post/7152407243974148127 解决 打开后的界面如下： 测试应用安装安卓应用还需要adb命令： 1$ sudo apt install android-tools-adb -y 然后在最开始的项目目录下执行以下命令安装应用，此时安卓模拟器必须是打开的状态： 123$ asb install ./basic.apk# 或者$ adb install /home/test/go/src/goapp/basic.apk 安装成功后即可在模拟器中看到该应用 单击打开，运行效果如下： 此文章主要目的是为了帮助你了解如何使用golang开发安卓应用的流程，流程打通之后，可以结合自己的想法，做一些自己的应用。","categories":[{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/categories/go/"}],"tags":[{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/tags/go/"}]},{"title":"go语言1.18 go:linkname must refer to declared function or variable解决办法","slug":"go语言1-18-go-linkname-must-refer-to-declared-function-or-variable解决办法","date":"2022-07-12T06:13:58.000Z","updated":"2022-07-12T06:14:58.000Z","comments":true,"path":"2022/07/12/go语言1-18-go-linkname-must-refer-to-declared-function-or-variable解决办法/","permalink":"https://www.silenceboy.com/2022/07/12/go%E8%AF%AD%E8%A8%801-18-go-linkname-must-refer-to-declared-function-or-variable%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/","excerpt":"","text":"在macos环境中，go1.18刚刚部署后，会报错如下： 12345678910111213golang.org/x/sys/unix# golang.org/x/sys/unixvendor/golang.org/x/sys/unix/syscall_darwin.1_13.go:29:3: //go:linkname must refer to declared function or variablevendor/golang.org/x/sys/unix/zsyscall_darwin_amd64.1_13.go:27:3: //go:linkname must refer to declared function or variablevendor/golang.org/x/sys/unix/zsyscall_darwin_amd64.1_13.go:40:3: //go:linkname must refer to declared function or variablevendor/golang.org/x/sys/unix/zsyscall_darwin_amd64.go:28:3: //go:linkname must refer to declared function or variablevendor/golang.org/x/sys/unix/zsyscall_darwin_amd64.go:43:3: //go:linkname must refer to declared function or variablevendor/golang.org/x/sys/unix/zsyscall_darwin_amd64.go:59:3: //go:linkname must refer to declared function or variablevendor/golang.org/x/sys/unix/zsyscall_darwin_amd64.go:75:3: //go:linkname must refer to declared function or variablevendor/golang.org/x/sys/unix/zsyscall_darwin_amd64.go:90:3: //go:linkname must refer to declared function or variablevendor/golang.org/x/sys/unix/zsyscall_darwin_amd64.go:105:3: //go:linkname must refer to declared function or variablevendor/golang.org/x/sys/unix/zsyscall_darwin_amd64.go:121:3: //go:linkname must refer to declared function or variablevendor/golang.org/x/sys/unix/zsyscall_darwin_amd64.go:121:3: too many errors 解决办法如下： 1.运行如下命令： 1go get -u golang.org/x/sys 2.运行： 1go mod vendor","categories":[{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/categories/go/"}],"tags":[{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/tags/go/"}]},{"title":"macbook如何卸载天空卫士","slug":"macbook如何卸载天空卫士","date":"2022-05-20T06:07:27.000Z","updated":"2022-05-20T06:19:34.000Z","comments":true,"path":"2022/05/20/macbook如何卸载天空卫士/","permalink":"https://www.silenceboy.com/2022/05/20/macbook%E5%A6%82%E4%BD%95%E5%8D%B8%E8%BD%BD%E5%A4%A9%E7%A9%BA%E5%8D%AB%E5%A3%AB/","excerpt":"","text":"首先找到安装天空卫士的目录：/Library/Application Support/SkyGuard 电脑关机，长按开机键直到进入recovery模式, 点自己的用户，输入密码，点下一步，然后到左上方找到终端打开 diskutil list 查看目前的磁盘, 找到标记有synthesized的磁盘，并找到对应的数据盘（有data关键字），这里假设为：/dev/disk3s3 执行diskutil mount /dev/disk3s3, 可能会报 this is an encrypted and locked APFS Volume的错，根据提示执行：diskutil apfs unlockVolume /dev/disk3s3，然后输入密码。再次执行diskutil mount /dev/disk3s3即可。 进入/Volumes/Macintosh HD/Library/Application Support目录，可以看到有SkyGuard文件。 执行rm -rf SkyGuard删除天空位置文件目录 reboot重启电脑，发现天空卫士已成功卸载","categories":[{"name":"mac","slug":"mac","permalink":"https://www.silenceboy.com/categories/mac/"}],"tags":[{"name":"mac","slug":"mac","permalink":"https://www.silenceboy.com/tags/mac/"},{"name":"天空卫士","slug":"天空卫士","permalink":"https://www.silenceboy.com/tags/%E5%A4%A9%E7%A9%BA%E5%8D%AB%E5%A3%AB/"}]},{"title":"mac卸载jdk","slug":"mac卸载jdk","date":"2022-04-29T09:25:19.000Z","updated":"2022-04-29T09:27:28.000Z","comments":true,"path":"2022/04/29/mac卸载jdk/","permalink":"https://www.silenceboy.com/2022/04/29/mac%E5%8D%B8%E8%BD%BDjdk/","excerpt":"","text":"删除运行路径和运行环境等 123sudo rm -fr /Library/Internet\\ Plug-Ins/JavaAppletPlugin.pluginsudo rm -fr /Library/PreferencesPanes/JavaControlPanel.prefPanesudo rm -fr ~/Library/Application\\ Support/Java 删除当前版本的jdk 1sudo rm -rf /Library/Java/JavaVirtualMachines/jdk1.8.0_301.jdk 检查是否卸载成功 1java -version","categories":[{"name":"mac","slug":"mac","permalink":"https://www.silenceboy.com/categories/mac/"}],"tags":[{"name":"mac","slug":"mac","permalink":"https://www.silenceboy.com/tags/mac/"},{"name":"jdk","slug":"jdk","permalink":"https://www.silenceboy.com/tags/jdk/"}]},{"title":"docker跨平台打包问题does not match the detected host platform","slug":"docker跨平台打包问题does-not-match-the-detected-host-platform","date":"2022-04-29T03:44:40.000Z","updated":"2022-04-29T03:56:38.000Z","comments":true,"path":"2022/04/29/docker跨平台打包问题does-not-match-the-detected-host-platform/","permalink":"https://www.silenceboy.com/2022/04/29/docker%E8%B7%A8%E5%B9%B3%E5%8F%B0%E6%89%93%E5%8C%85%E9%97%AE%E9%A2%98does-not-match-the-detected-host-platform/","excerpt":"","text":"问题最近在mac M1上构建的docker镜像，发布到ubuntu20系统的服务器上后，一直运行失败。输出以下报错信息： 1The requested image&#x27;s platform (linux/arm64) does not match the detected host platform (linux/amd64) and no specific platform was requested 在使用docker logs 查看docker日志的时候提示： 1standard_init_linux.go:228: exec user process caused: exec format error​ 解决方案 mac M1上设置”experimental”: true 实现跨平台打包 docker buildx build –platform linux&#x2F;amd64 -t name .","categories":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/tags/docker/"},{"name":"mac","slug":"mac","permalink":"https://www.silenceboy.com/tags/mac/"},{"name":"ubuntu","slug":"ubuntu","permalink":"https://www.silenceboy.com/tags/ubuntu/"}]},{"title":"mac电脑M1芯片如何安装低版本node环境","slug":"mac电脑M1芯片如何安装低版本node环境","date":"2021-10-22T09:15:17.000Z","updated":"2021-10-22T09:25:13.000Z","comments":true,"path":"2021/10/22/mac电脑M1芯片如何安装低版本node环境/","permalink":"https://www.silenceboy.com/2021/10/22/mac%E7%94%B5%E8%84%91M1%E8%8A%AF%E7%89%87%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85%E4%BD%8E%E7%89%88%E6%9C%ACnode%E7%8E%AF%E5%A2%83/","excerpt":"","text":"在mac M1上安装v14 及以下的老版本 Node会出现闪退问题，究其原因还是因为低版本的 node 并不是基于 arm64 架构的，所以不适配 M1 芯片。在这里教大家两个方法，就能成功安装上低版本 Node。 方法一在终端中，输入： 1arch -x86_64 zsh 通过这个命令可以让 shell 运行在Rosetta2下。之后你可以通过 nvm install v14 来安装低版本 Node。在此之后，您可以不用在 Rosetta2 中就可以使用安装的可执行文件，也就是说，您可以将 Node v15与其他节点版本互换使用。 方法二方法二就是通过 Rosetta2 来启动终端，这样通过 Rosetta2 转译到 x86 架构中执行安装，也一样可以安装成功。 在 finder 中，点击应用程序，并在实用工具中找到终端 (Terminal) 右键终端，点击获取信息 勾选 Open using Rosetta 重启终端，并执行 nvm install v14 命令","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"},{"name":"mac","slug":"mac","permalink":"https://www.silenceboy.com/tags/mac/"},{"name":"M1","slug":"M1","permalink":"https://www.silenceboy.com/tags/M1/"}]},{"title":"Arch linux i3wm运行企业微信和微信","slug":"Arch-linux-i3wm运行企业微信和微信","date":"2021-05-26T14:05:02.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2021/05/26/Arch-linux-i3wm运行企业微信和微信/","permalink":"https://www.silenceboy.com/2021/05/26/Arch-linux-i3wm%E8%BF%90%E8%A1%8C%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E5%92%8C%E5%BE%AE%E4%BF%A1/","excerpt":"","text":"当你运行在i3wm环境下时，运行通过deepin-wine安装的企业微信和微信时，打开软件时会出现闪退的现象，启动不了软件。或直接在命令行里执行命令也启动不了程序，会出现以下提示： 12345X Error of failed request: BadWindow (invalid Window parameter) Major opcode of failed request: 20 (X_GetProperty) Resource id in failed request: 0x0 Serial number of failed request: 10 Current serial number in output stream: 10 解决方案 这个问题其实和 KDE 无关, 应该是 deepin 在打包 deepin-wine 的过程中有意或者无意加入了 GNOME 依赖。 执行 /usr/lib/gnome-settings-daemon/gsd-xsettings 即可.或者后台运行： 1nohup /usr/lib/gnome-settings-daemon/gsd-xsettings &gt; /dev/null 2&gt;&amp;1 &amp; 如果 GNOME 的版本较低(比如Debian 9), 没有单独的 gsd-xsettings 可执行文件, 则执行 gnome-settings-daemon. 然后切换到对应目录 cd /opt/deepinwine/apps/Deepin-WXWork 或者 /opt/deepinwine/apps/Deepin-WeChat运行 ./run.sh即可启动软件。 由于每次都执行上边的命令很繁琐，可以将其加入i3的启动项，每次开机制动设置即可。","categories":[{"name":"linux","slug":"linux","permalink":"https://www.silenceboy.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://www.silenceboy.com/tags/linux/"},{"name":"i3wm","slug":"i3wm","permalink":"https://www.silenceboy.com/tags/i3wm/"},{"name":"i3","slug":"i3","permalink":"https://www.silenceboy.com/tags/i3/"}]},{"title":"MySQL TEXT数据类型的最大长度","slug":"MySQL-TEXT数据类型的最大长度","date":"2020-12-22T04:33:17.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2020/12/22/MySQL-TEXT数据类型的最大长度/","permalink":"https://www.silenceboy.com/2020/12/22/MySQL-TEXT%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%9C%80%E5%A4%A7%E9%95%BF%E5%BA%A6/","excerpt":"","text":"类型 长度 TINYTEXT 256 bytes TEXT 65,535 bytes ~64kb MEDIUMTEXT 16,777,215 bytes ~16MB LONGTEXT 4,294,967,295 bytes ~4GB","categories":[{"name":"mysql","slug":"mysql","permalink":"https://www.silenceboy.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://www.silenceboy.com/tags/mysql/"}]},{"title":"ubuntu18.04安装jdk1.8","slug":"ubuntu18-04安装jdk1-8","date":"2020-11-28T06:30:31.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2020/11/28/ubuntu18-04安装jdk1-8/","permalink":"https://www.silenceboy.com/2020/11/28/ubuntu18-04%E5%AE%89%E8%A3%85jdk1-8/","excerpt":"","text":"下载jdk安装包https://www.oracle.com/java/technologies/javase/javase-jdk8-downloads.html 解压1tar -zxvf jdk-8u171-linux-x64.tar.gz 移动到自己想放的位置12##将文件从下载目录 挪到/usr/local下sudo mv jdk1.8.0_171 /usr/local/jdk1.8 设置环境变量 设置全局生效 修改全局配置文件，作用与所有用户： vim /etc/profile 1234 export JAVA_HOME=/usr/local/jdk1.8export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=.:$&#123;JAVA_HOME&#125;/bin:$PATH 设置当前用户生效 修改当前用户配置文件，只作用于当前用户：vim ~/.bashrc 1234 export JAVA_HOME=/usr/local/jdk1.8export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=.:$&#123;JAVA_HOME&#125;/bin:$PATH 使修改的配置立刻生效1234##对应方法一：source /etc/profile ##对应方法二：source ~/.bashrc 检查是否安装成功1java -version","categories":[{"name":"linux","slug":"linux","permalink":"https://www.silenceboy.com/categories/linux/"}],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://www.silenceboy.com/tags/ubuntu/"},{"name":"jdk","slug":"jdk","permalink":"https://www.silenceboy.com/tags/jdk/"},{"name":"java","slug":"java","permalink":"https://www.silenceboy.com/tags/java/"}]},{"title":"alsamixer控制音量","slug":"alsamixer控制音量","date":"2020-11-28T05:51:40.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2020/11/28/alsamixer控制音量/","permalink":"https://www.silenceboy.com/2020/11/28/alsamixer%E6%8E%A7%E5%88%B6%E9%9F%B3%E9%87%8F/","excerpt":"","text":"解除各声道的静音目前版本的 ALSA 安装后，所有声道默认是静音的，必须手动解除。 使用 alsamixer 的 ncurses 界面，配置十分简单： 1$ alsamixer 此外，还可以在命令行下使用 amixer： 1$ amixer sset Master unmute 在 alsamixer 中，下方标有 MM 的声道是静音的，而标有 00 的通道已经启用。 使用 ← 和 → 方向键，选中 Master 和 PCM 声道。按下 m 键解除静音。使用 ↑ 方向键增加音量，直到增益值为0。该值显示在左上方 Item: 字段后。过高的增益值会导致声音失真。 要启用麦克风，切换至 Capture 选项卡，按下 F4，按下 空格 启用其中一个声道即可。 按下 Esc 键退出 alsamixer。 alsamixer 终端交互式设置音量123F6 选择网卡F2 显示系统信息，可以看到系统中已有网卡信息Esc 后退 123M 静音状态切换Q,W,E 增大 左,右,通道 的音量Z,X,C 减小 左,右,通道 的音量 amixer 命令行控制系统声音1cat /proc/asound/cards # 查看系统声卡 输出如下： 120 [PCH ]: HDA-Intel - HDA Intel PCH HDA Intel PCH at 0xe1340000 irq 130 设置声音 1amixer -c 1 -q set Master 2dB+ unmute 12-c 制定声卡id, 默认为0-q 安静模式，不输出结果","categories":[{"name":"linux","slug":"linux","permalink":"https://www.silenceboy.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://www.silenceboy.com/tags/linux/"}]},{"title":"使用npm安装依赖时报错：gyp: No Xcode or CLT version detected!","slug":"使用npm安装依赖时报错：gyp-No-Xcode-or-CLT-version-detected","date":"2020-05-05T08:47:52.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2020/05/05/使用npm安装依赖时报错：gyp-No-Xcode-or-CLT-version-detected/","permalink":"https://www.silenceboy.com/2020/05/05/%E4%BD%BF%E7%94%A8npm%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E6%97%B6%E6%8A%A5%E9%94%99%EF%BC%9Agyp-No-Xcode-or-CLT-version-detected/","excerpt":"","text":"最近在macOS中使用npm安装模块，出现如下错误： 1234567891011121314151617181920212223npm WARN deprecated fsevents@1.2.12: fsevents 1 will break on node v14+. Upgrade to fsevents 2 with massive improvements.&gt; fsevents@1.2.12 install /Users/chenhao/outsourcing/egg-car/node_modules/fsevents&gt; node-gyp rebuildNo receipt for &#x27;com.apple.pkg.CLTools_Executables&#x27; found at &#x27;/&#x27;.No receipt for &#x27;com.apple.pkg.DeveloperToolsCLILeo&#x27; found at &#x27;/&#x27;.No receipt for &#x27;com.apple.pkg.DeveloperToolsCLI&#x27; found at &#x27;/&#x27;.gyp: No Xcode or CLT version detected!gyp ERR! configure errorgyp ERR! stack Error: `gyp` failed with exit code: 1gyp ERR! stack at ChildProcess.onCpExit (/Users/chenhao/.nvm/versions/node/v10.16.0/lib/node_modules/npm/node_modules/node-gyp/lib/configure.js:351:16)gyp ERR! stack at ChildProcess.emit (events.js:198:13)gyp ERR! stack at Process.ChildProcess._handle.onexit (internal/child_process.js:248:12)gyp ERR! System Darwin 19.4.0gyp ERR! command &quot;/Users/chenhao/.nvm/versions/node/v10.16.0/bin/node&quot; &quot;/Users/chenhao/.nvm/versions/node/v10.16.0/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js&quot; &quot;rebuild&quot;gyp ERR! cwd /Users/chenhao/outsourcing/egg-car/node_modules/fseventsgyp ERR! node -v v10.16.0gyp ERR! node-gyp -v v5.0.5gyp ERR! not ok 网上查找解决方案，都是通过执行xcode-select --install命令修复，但是执行该命令时会出现如下提示： 1xcode-select: error: command line tools are already installed, use &quot;Software Update&quot; to install updates 最终的解决办法是先卸载之前安装的xcode-select，并重新安装： 12$ sudo rm -rf $(xcode-select -print-path)$ xcode-select --install","categories":[{"name":"npm","slug":"npm","permalink":"https://www.silenceboy.com/categories/npm/"}],"tags":[{"name":"npm","slug":"npm","permalink":"https://www.silenceboy.com/tags/npm/"}]},{"title":"zsh在scp时不能使用通配符的原因和解决方案","slug":"zsh在scp时不能使用通配符的原因和解决方案","date":"2020-03-06T07:14:33.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2020/03/06/zsh在scp时不能使用通配符的原因和解决方案/","permalink":"https://www.silenceboy.com/2020/03/06/zsh%E5%9C%A8scp%E6%97%B6%E4%B8%8D%E8%83%BD%E4%BD%BF%E7%94%A8%E9%80%9A%E9%85%8D%E7%AC%A6%E7%9A%84%E5%8E%9F%E5%9B%A0%E5%92%8C%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","excerpt":"","text":"问题scp是经常使用的一个本地与远程服务器相互拷贝数据的命令，zsh是我最喜欢的shell，但是在zsh下使用scp来拷贝远程服务器的文件时，却出现这样的错误。 12$ scp -r test-server:/etc/nginx/conf.d/* .zsh: no matches found: test-server:/etc/nginx/conf.d/* 同样地命令，在bash下确实可以执行的，这个原因是什么呢？ 由于zsh不会按照远程地址上的文件去扩展参数，当你使用test-server:/etc/nginx/conf.d/*，因为本地当前目录中，是不存在test-server:/etc/nginx/conf.d/*，所以匹配失败。默认情况下，bash 在匹配失败时就使用原来的内容，zsh 则报告一个no matches的错误。 解决方案在zsh中执行setopt nonomatch，告诉它不要报告no matches的错误，而是当匹配失败时直接使用原来的内容。 实际上，不管是 bash 还是 zsh，不管设置了什么选项，只要把test-server:/etc/nginx/conf.d/*加上引号，如&quot;test-server:/etc/nginx/conf.d/*&quot;，就可解决问题。 当然根本的解决办法还是告诉zsh不要报告no matches错误。 执行下面的命令可以一劳永逸： 123$ echo &quot;setopt nonomatch&quot; &gt;&gt; ~/.zshrc或$ echo &quot;set -o nonomatch&quot; &gt;&gt; ~/.zshrc","categories":[{"name":"shell","slug":"shell","permalink":"https://www.silenceboy.com/categories/shell/"}],"tags":[{"name":"scp","slug":"scp","permalink":"https://www.silenceboy.com/tags/scp/"},{"name":"zsh","slug":"zsh","permalink":"https://www.silenceboy.com/tags/zsh/"}]},{"title":"git clone 出现ssh: connect to host github.com port 22: Connection timed out解决方案","slug":"git-clone-出现ssh-connect-to-host-github-com-port-22-Connection-timed-out解决方案","date":"2020-03-04T05:07:40.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2020/03/04/git-clone-出现ssh-connect-to-host-github-com-port-22-Connection-timed-out解决方案/","permalink":"https://www.silenceboy.com/2020/03/04/git-clone-%E5%87%BA%E7%8E%B0ssh-connect-to-host-github-com-port-22-Connection-timed-out%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","excerpt":"","text":"物理机服务器，安装git之后，想从github上clone自己的项目运行，ssh key已经配置过, 但在执行git clone命令时出现了如下报错： 1234ssh: connect to host github.com port 22: Connection timed outfatal: 无法读取远程仓库。请确认您有正确的访问权限并且仓库存在。 解决方案在系统~/.ssh目录下执行touch config命令新建config文件，并修改文件权限： 1sudo chmod 600 config 然后在config文件中添加如下内容： 123456Host github.comUser email@qq.com // 替换成自己的github登录邮箱Hostname ssh.github.comPreferredAuthentications publickeyIdentityFile ~/.ssh/id_rsaPort 443 然后设置： 12git config --global user.name &quot;XXX&quot;git config --global user.email XXX@xx.com 此时再去执行git clone命令，一切正常。","categories":[{"name":"git","slug":"git","permalink":"https://www.silenceboy.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://www.silenceboy.com/tags/git/"}]},{"title":"mysql5.7 ibtmp1文件过大","slug":"mysql5-7-ibtmp1文件过大","date":"2019-12-26T08:29:14.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/12/26/mysql5-7-ibtmp1文件过大/","permalink":"https://www.silenceboy.com/2019/12/26/mysql5-7-ibtmp1%E6%96%87%E4%BB%B6%E8%BF%87%E5%A4%A7/","excerpt":"","text":"服务器上的磁盘被占满，通过一下命令查找服务器上的大文件： 1$ sudo find / -type f -size +800M -print0 | xargs -0 du -h | sort -nr 经过排查后发现，/var/lib/mysql目录下面有一个ibtmp1的文件特别大，有64G 。 ibtmp1是个什么东西呢？查看官方文档后发现这是非压缩的innodb临时表的独立表空间。通过innodb_temp_data_file_path参数指定文件的路径，文件名和大小，默认配置为ibtmp1:12M:autoextend，也就是说在支持大文件的系统这个文件大小是可以无限增长的。 解决办法： 修改my.cnf配置文件： 1innodb_temp_data_file_path = ibtmp1:12M:autoextend:max:5G 设置innodb_fast_shutdown参数 1SET GLOBAL innodb_fast_shutdown = 0; #InnoDB does a slow shutdown, a full purge and a change buffer merge before shutting down 关闭mysql服务 1systemctl stop mysqld.service 删除ibtmp1文件 启动mysql服务 1systemctl start mysqld.service","categories":[{"name":"mysql","slug":"mysql","permalink":"https://www.silenceboy.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://www.silenceboy.com/tags/mysql/"}]},{"title":"pm2日志拆分","slug":"pm2日志拆分","date":"2019-12-20T03:41:48.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/12/20/pm2日志拆分/","permalink":"https://www.silenceboy.com/2019/12/20/pm2%E6%97%A5%E5%BF%97%E6%8B%86%E5%88%86/","excerpt":"","text":"pm2默认会将日志文件写入家目录下的 .pm2/logs 目录中，但是pm2的日志文件不能自动分割，这会导致一个文件不断变大，不但影响性能，查看这些日志也会带来麻烦。 pm2的日志切割模块pm2-logrotate 安装pm2-logrotate： 1$ pm2 install pm2-logrotate 设置切割规则 1234567891011121314151. 设置文件大小为100M，大于等于开始切割pm2 set pm2-logrotate:max_size 100M2.设置文件切割的监控间，监控间隔比较大，可能会使切割出的文件大小和max_size有出入pm2 set pm2-logrotate:workerInterval 13. 设置文件最多多少个，超过则删除pm2 set pm2-logrotate:retain 104. 设置文件是否压缩$ pm2 set pm2-logrotate:compress false5. 设置文件命名格式$ pm2 set pm2-logrotate:dateFormat YYYY-MM-DD_HH-mm-ss 更新pm2 执行一下命令使pm2配置生效 1$ pm2 update","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"},{"name":"pm2","slug":"pm2","permalink":"https://www.silenceboy.com/tags/pm2/"}]},{"title":"ssh使用ProxyCommand连接阿里云内网服务器","slug":"ssh使用ProxyCommand连接阿里云内网服务器","date":"2019-10-21T05:44:57.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/10/21/ssh使用ProxyCommand连接阿里云内网服务器/","permalink":"https://www.silenceboy.com/2019/10/21/ssh%E4%BD%BF%E7%94%A8ProxyCommand%E8%BF%9E%E6%8E%A5%E9%98%BF%E9%87%8C%E4%BA%91%E5%86%85%E7%BD%91%E6%9C%8D%E5%8A%A1%E5%99%A8/","excerpt":"","text":"在没有发现proxyCommand命令的好处之前，本地连接想要访问内网服务器，需要先ssh连接开放外网ip并且与我们要访问的目标主机在同一个内网环境的esc服务器，然后将该服务器作为跳板机，在该服务器上ssh连接内网服务器。该操作非常麻烦。使用proxyCommand能够很方便的解决该问题。 proxyCommand配置修改~/.ssh/config： 123456789Host tiaobanHostname 跳板机的ipPort 跳板机的端口(如果是非22的需要填写)User root(如果非root,换成跳板机的用户)Host targetHostname 目标机的IPPort 跳板机的端口(如果是非22的需要填写)User root(如果非root,换成跳板机的用户)ProxyCommand ssh -q -x -W %h:%p tiaoban 这儿的%h表示要连接的目标机,也就是Hostname指定的ip或者主机名,%p表示要连接到目标机的端口.这儿可以直接写死固定值,但是使用%h和%p可以保证在Hostname和Port变化的情况下ProxyCommand这行不用跟着变化. 然后我们直接ssh target,可以看到直接就连接上了.","categories":[{"name":"ssh","slug":"ssh","permalink":"https://www.silenceboy.com/categories/ssh/"}],"tags":[{"name":"ssh","slug":"ssh","permalink":"https://www.silenceboy.com/tags/ssh/"}]},{"title":"linux 添加只读用户","slug":"linux-添加只读用户","date":"2019-10-16T08:31:43.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/10/16/linux-添加只读用户/","permalink":"https://www.silenceboy.com/2019/10/16/linux-%E6%B7%BB%E5%8A%A0%E5%8F%AA%E8%AF%BB%E7%94%A8%E6%88%B7/","excerpt":"","text":"这里未使用rbash新建用户，使用rbash新建只读用户不能使用cd等内置命令。 添加用户 1# useradd -m test 设置密码 1# passwd test 修改用户的shell配置文件 12# chown root. /home/test/.bash_profile# chmod 755 /home/test/.bash_profile 修改/home/test/.bash_profile配置文件 将PATH改为$HOME/.bin 123456789101112# .bash_profile# Get the aliases and functionsif [ -f ~/.bashrc ]; then . ~/.bashrcfi# User specific environment and startup programs# PATH=$PATH:$HOME/.local/bin:$HOME/binPATH=$HOME/.binexport PATH 创建用户.bin目录 1# mkdir /home/test/.bin 将允许执行的命令链接到/home/test/.bin目录 123456789ln -s /usr/bin/wc /home/test/.bin/wcln -s /usr/bin/tail /home/test/.bin/tailln -s /usr/bin/more /home/test/.bin/moreln -s /usr/bin/cat /home/test/.bin/catln -s /usr/bin/grep /home/test/.bin/grepln -s /usr/bin/find /home/test/.bin/findln -s /usr/bin/pwd /home/test/.bin/pwdln -s /usr/bin/ls /home/test/.bin/lsln -s /usr/bin/less /home/test/.bin/less 之后使用创建的用户登录系统，用户只拥有只读权限，只能使用软连接的命令。","categories":[{"name":"linux","slug":"linux","permalink":"https://www.silenceboy.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://www.silenceboy.com/tags/linux/"}]},{"title":"Docker启用TLS进行安全配置","slug":"Docker启用TLS进行安全配置","date":"2019-09-29T01:56:59.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/09/29/Docker启用TLS进行安全配置/","permalink":"https://www.silenceboy.com/2019/09/29/Docker%E5%90%AF%E7%94%A8TLS%E8%BF%9B%E8%A1%8C%E5%AE%89%E5%85%A8%E9%85%8D%E7%BD%AE/","excerpt":"","text":"之前开启了docker的2375 Remote API，由于没有启用TLS，导致服务器被入侵，安装了挖矿程序。所以如果想开通docker远程访问，就必须做好安全验证。 文中出现的$HOST指的是主机ip， 实际执行时用主机ip替换即可。 在Docker守护程序的主机上，生成CA私钥和公钥：1234567891011121314151617181920212223242526272829// 生成 CA 私钥root@docker-manager:~# openssl genrsa -aes256 -out ca-key.pem 4096Generating RSA private key, 4096 bit long modulus (2 primes).........................................................................++++.............................................++++e is 65537 (0x010001)// 需要输入两次自定义密码Enter pass phrase for ca-key.pem:Verifying - Enter pass phrase for ca-key.pem:// 生成 CA 公钥root@docker-manager:~# openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem// 这里需要输入第一步设置的密码Enter pass phrase for ca-key.pem:You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter &#x27;.&#x27;, the field will be left blank.-----Country Name (2 letter code) [AU]:CNState or Province Name (full name) [Some-State]:zhejiangLocality Name (eg, city) []:hangzhouOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Docker IncOrganizational Unit Name (eg, section) []:Common Name (e.g. server FQDN or YOUR name) []: $HOSTEmail Address []: 创建服务器密钥和证书签名请求(CSR)1234567root@docker-manager:~# openssl genrsa -out server-key.pem 4096Generating RSA private key, 4096 bit long modulus (2 primes)......................................................................................................++++................++++e is 65537 (0x010001)root@docker-manager:~# openssl req -subj &quot;/CN=$HOST&quot; -sha256 -new -key server-key.pem -out server.csr 用CA签署公钥由于可以通过IP地址和DNS名称建立TLS连接，因此在创建证书时需要指定IP地址。例如，允许使用172.16.132.200和127.0.0.1进行连接： 这里遇到一个坑，如果IP后不指定本机ip，远程无法连接，不知道是不是配置哪里有问题，目前我会在下面的命令中添加IP:$HOST去解决问题 1root@docker-manager:~# echo subjectAltName = DNS:$HOST,IP:172.16.132.200,IP:127.0.0.1 &gt;&gt; extfile.cnf 将Docker守护程序密钥的扩展用法属性设置为仅用于服务器身份验证： 1root@docker-manager:~# echo extendedKeyUsage = serverAuth &gt;&gt; extfile.cnf 生成服务端签名证书123456root@docker-manager:~# openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out server-cert.pem -extfile extfile.cnfSignature oksubject=CN = 172.16.132.200Getting CA Private Key// 输入最开始设置的密码Enter pass phrase for ca-key.pem: 创建客户端密钥和证书签名请求1234567root@docker-manager:~# openssl genrsa -out key.pem 4096Generating RSA private key, 4096 bit long modulus (2 primes)..++++...............................................................................................................................................................................................................++++e is 65537 (0x010001)root@docker-manager:~# openssl req -subj &#x27;/CN=client&#x27; -new -key key.pem -out client.csr 为了使密钥适合客户端身份验证，请创建一个新的扩展配置文件： 1root@docker-manager:~# echo extendedKeyUsage = clientAuth &gt; extfile-client.cnf 生成客户端签名证书1234567root@docker-manager:~# openssl x509 -req -days 365 -sha256 -in client.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out cert.pem -extfile extfile-client.cnfSignature oksubject=CN = clientGetting CA Private Key// 输入最开始设置的密码Enter pass phrase for ca-key.pem: 删除和修改文件权限生成后cert.pem，server-cert.pem您可以安全地删除两个证书签名请求和扩展配置文件： 12345root@docker-manager:~# rm -v client.csr server.csr extfile.cnf extfile-client.cnfremoved &#x27;client.csr&#x27;removed &#x27;server.csr&#x27;removed &#x27;extfile.cnf&#x27;removed &#x27;extfile-client.cnf&#x27; 为了保护您的钥匙免遭意外损坏，请删除其写权限。要使它们仅供您阅读，请按以下方式更改文件模式： 1234root@docker-manager:~# chmod -v 0400 ca-key.pem key.pem server-key.pemmode of &#x27;ca-key.pem&#x27; changed from 0600 (rw-------) to 0400 (r--------)mode of &#x27;key.pem&#x27; changed from 0600 (rw-------) to 0400 (r--------)mode of &#x27;server-key.pem&#x27; changed from 0600 (rw-------) to 0400 (r--------) 证书可以在世界范围内读取，但您可能希望删除写访问权限以防止意外损坏： 1234root@docker-manager:~# chmod -v 0444 ca.pem server-cert.pem cert.pemmode of &#x27;ca.pem&#x27; changed from 0644 (rw-r--r--) to 0444 (r--r--r--)mode of &#x27;server-cert.pem&#x27; changed from 0644 (rw-r--r--) to 0444 (r--r--r--)mode of &#x27;cert.pem&#x27; changed from 0644 (rw-r--r--) to 0444 (r--r--r--) 修改docker配置并重启docker编辑docker配置文件（我的是ubuntu机器）：vim /lib/systemd/system/docker.service 添加如下行： 1ExecStart=/usr/bin/dockerd --tlsverify --tlscacert=/root/docker/ca.pem --tlscert=/root/docker/server-cert.pem --tlskey=/root/docker/server-key.pem -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375 重启docker服务： 12systemctl daemon-reloadsystemctl restart docker 查看端口号： 1234567root@docker-manager:~# netstat -plntActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp6 0 0 :::80 :::* LISTEN 858/nginx: master p tcp6 0 0 :::25 :::* LISTEN 1662/master tcp6 0 0 :::2375 :::* LISTEN 25582/dockerd tcp6 0 0 :::2377 :::* LISTEN 25582/dockerd 客户端远程安全连接将 ca.pem cert.pem key.pem 三个文件通过 scp 下载到 客户端机器。 远程连接命令，路径根据实际情况填写： 123456docker --tlsverify \\ --tlscacert=/home/docker/ca.pem \\ --tlscert=/home/docker/cert.pem \\ --tlskey=/home/docker/key.pem \\ -H=172.16.132.200:2375 \\ info 把密钥放入 ~&#x2F;.docker 文件夹中： 每次操作需要跟那么多参数，太麻烦了。我们可以把ca.pem cert.pem key.pem三个文件放入客户端~/.docker中，然后配置环境变量就可以简化命令了。 123$ export DOCKER_HOST=tcp://172.16.132.200:2375 DOCKER_TLS_VERIFY=1$ docker info","categories":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/tags/docker/"}]},{"title":"docker安装jira并破解","slug":"docker安装jira并破解","date":"2019-09-03T12:15:22.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/09/03/docker安装jira并破解/","permalink":"https://www.silenceboy.com/2019/09/03/docker%E5%AE%89%E8%A3%85jira%E5%B9%B6%E7%A0%B4%E8%A7%A3/","excerpt":"","text":"下载镜像1$ docker pull cptactionhank/atlassian-jira 运行容器12$ docker volume create jira_home$ docker run -d -p 8080:8080 --name jira --restart always -v jira_home:/var/atlassian/jira cptactionhank/atlassian-jira:latest 下载破解文件123$ wget https://github.com/silenceboychen/some-software/raw/master/Jira/mysql-connector-java-5.1.25-bin.jar$ wget https://github.com/silenceboychen/some-software/raw/master/Jira/atlassian-universal-plugin-manager-plugin-2.22.4.jar$ wget https://github.com/silenceboychen/some-software/raw/master/Jira/atlassian-extras-3.2.jar 添加mysql驱动程序mysql配置 12$ docker cp mysql-connector-java-5.1.25-bin.jar jira:/opt/atlassian/jira/atlassian-jira/WEB-INF/lib/$ docker restart jira nginx配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657upstream jira&#123; server 127.0.0.1:8080;&#125;# http配置#server &#123;# listen 80;# server_name jira.domain.com;# access_log /var/log/nginx/jira.domain.com-access.log;# error_log /var/log/nginx/jira.domain.com-error.log;# location / &#123;# proxy_pass_header Server;# proxy_set_header Host $http_host;# proxy_set_header X-Real-IP $remote_addr;# proxy_set_header X-Scheme $scheme;# proxy_pass http://jira;# &#125;#&#125;# https配置server &#123; #侦听443端口，这个是ssl访问端口 listen 443; #定义使用 访问域名 server_name jira.iblackvip.com; access_log /var/log/nginx/jira.domain.com.access.log; error_log /var/log/nginx/jira.domain.com.error.log; ssl on; ssl_certificate /etc/nginx/cert/jira.domain.com.pem; ssl_certificate_key /etc/nginx/cert/jira.domain.com.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; #按照这个协议配置 ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;#按照这个套件配置 ssl_prefer_server_ciphers on; location / &#123; proxy_pass_header Server; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_pass http://jira; &#125; gzip on; gzip_min_length 1k; gzip_buffers 4 16k; gzip_comp_level 5; gzip_types text/plain application/x-javascript text/css application/xml text/javascript application/x-httpd-php;&#125;server &#123; # 80端口是http正常访问的接口 listen 80; server_name jira.domain.com; # 在这里，我做了https全加密处理，在访问http的时候自动跳转到https rewrite ^(.*) https://$host$1 permanent;&#125; Web设置 浏览器访问JiraWeb，语言可以设为中文，选择「我将设置它自己」——「下一步」 数据库设置，数据库类型选择「MySQL」，接着填入你的MySQL连接信息（需要你在你的MySQL数据库中创建数据库，数据库的字符类型必须是utf8），测试可以连接之后点击「下一步」 设置应用程序的属性——「下一步」 申请许可证关键字，点击「生成Jira试用许可证」 需要注册账号，注册完之后重新回到这个页面，选择相关信息，点击「Generate License」 点击「Yes」 页面就会带着你的许可证关键字回到Jira的设置页面，接着点击「下一步」 等待一会就进入设置管理员页面，填入一些信息即可，接着「下一步」 点击「完成」即完成设置 破解jira拷贝文件到容器内： 12$ docker cp atlassian-extras-3.2.jar jira:/opt/atlassian/jira/atlassian-jira/WEB-INF/lib/$ docker cp atlassian-universal-plugin-manager-plugin-2.22.4.jar jira:/opt/atlassian/jira/atlassian-jira/WEB-INF/atlassian-bundled-plugins/ 重启容器，破解结束： 1$ docker restart jira 查看jira页面设置-》应用程序，我们可以很明显的看到jira我们可以使用到2033年。","categories":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/tags/docker/"},{"name":"jira","slug":"jira","permalink":"https://www.silenceboy.com/tags/jira/"}]},{"title":"linux系统中root用户被提示：Operation not permitted","slug":"linux系统中root用户被提示：Operation-not-permitted","date":"2019-08-27T08:01:26.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/08/27/linux系统中root用户被提示：Operation-not-permitted/","permalink":"https://www.silenceboy.com/2019/08/27/linux%E7%B3%BB%E7%BB%9F%E4%B8%ADroot%E7%94%A8%E6%88%B7%E8%A2%AB%E6%8F%90%E7%A4%BA%EF%BC%9AOperation-not-permitted/","excerpt":"","text":"问题在修改文件权限时遇到如下报错： 12root@docker-manager:~/.ssh# chmod 600 authorized_keys chmod: changing permissions of &#x27;authorized_keys&#x27;: Operation not permitted 解决方法这里涉及到chattr和lsattr的知识： chattr是用来更改文件属性，lsattr可用来查看文件的属性，执行命令lsattr authorized_keys便可以看到当前文件的属性； 12root@docker-manager:~/.ssh# lsattr authorized_keys ----ia--------e--- authorized_keys 可以发现当前文件有个i属性，查阅命令帮助文档可以看到有i属性的文件是不能修改的，更不可被删除，即使是root用户也不可。 这里只需要去除i属性就可以修改文件权限。 123root@docker-manager:~/.ssh# chattr -i authorized_keys root@docker-manager:~/.ssh# lsattr authorized_keys -----a--------e--- authorized_keys chattr命令Linux chattr命令用于改变文件属性。 这项指令可改变存放在ext2文件系统上的文件或目录属性，这些属性共有以下8种模式： 12345678a：让文件或目录仅供附加用途。b：不更新文件或目录的最后存取时间。c：将文件或目录压缩后存放。d：将文件或目录排除在倾倒操作之外。i：不得任意更动文件或目录。s：保密性删除文件或目录。S：即时更新文件或目录。u：预防意外删除。 参数 1234567891011-R 递归处理，将指定目录下的所有文件及子目录一并处理。-v&lt;版本编号&gt; 设置文件或目录版本。-V 显示指令执行过程。+&lt;属性&gt; 开启文件或目录的该项属性。-&lt;属性&gt; 关闭文件或目录的该项属性。=&lt;属性&gt; 指定文件或目录的该项属性。","categories":[{"name":"linux","slug":"linux","permalink":"https://www.silenceboy.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://www.silenceboy.com/tags/linux/"}]},{"title":"部署Docker集群并使用Portainer管理","slug":"部署Docker集群并使用Portainer管理","date":"2019-08-14T04:34:06.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/08/14/部署Docker集群并使用Portainer管理/","permalink":"https://www.silenceboy.com/2019/08/14/%E9%83%A8%E7%BD%B2Docker%E9%9B%86%E7%BE%A4%E5%B9%B6%E4%BD%BF%E7%94%A8Portainer%E7%AE%A1%E7%90%86/","excerpt":"","text":"在有多台Docker的情况下，进行集群管理就十分重要了，Portainer也支持集群管理，Portainer可以和Swarm一起来进行集群管理操作。 环境要求 需要提前安装docker环境，docker安装教程。 使用docker安装portainer， 安装教程 搭建Swarm集群环境基本环境 用两台机器来搭建(都是ubuntu18.04系统) 12172.16.132.200 docker-manager172.16.132.201 dcoker-worker01 修改两台机器的主机名并做hosts 172.16.132.200机器 12345# hostnamectl set-hostname docker-manager# echo &quot;docker-manager&quot; &gt; /etc/hostname# vim /etc/hosts172.16.132.200 docker-manager172.16.132.201 dcoker-worker01 172.16.132.201机器 12345# hostnamectl set-hostname docker-worker01# echo &quot;docker-worker01&quot; &gt; /etc/hostname# vim /etc/hosts172.16.132.200 docker-manager172.16.132.201 dcoker-worker01 开通对外2375端口（方便portainer管理）12345678910111213// 先做备份# cp /lib/systemd/system/docker.service /lib/systemd/system/docker.service.bak # vim /lib/systemd/system/docker.service 找到ExecStart行改成这样的： ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2375# systemctl daemon-reload# systemctl restart docker 重启docker服务，使用service docker restart也可以# netstat -plnt 查看端口号使用Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 20917/nginx: master tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 742/sshd tcp6 0 0 :::2375 :::* LISTEN 5836/dockerd tcp6 0 0 :::7946 :::* LISTEN 5836/dockerd tcp6 0 0 :::80 :::* LISTEN 20917/nginx: master Swarm集群创建初始化Swarm 12345678# docker swarm init --advertise-addr 172.16.132.200Swarm initialized: current node (7ggeai3dlqn0j8gkxjs46y250) is now a manager.To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-0n04bao3bkte48prmcf1xfmlfrk9zh19b9u16ysb63yhvgjiyi-3w0rd58keboh52zul8xcjfrof 172.16.132.200:2377To add a manager to this swarm, run &#x27;docker swarm join-token manager&#x27; and follow the instructions. 上面命令执行后，该机器自动加入到swarm集群。这个会创建一个集群token，获取全球唯一的 token，作为集群唯一标识。后续将其他节点加入集群都会用到这个token值。 123456789--advertise-addr 指定与其他 node 通信的地址。docker swarm init 输出告诉我们：① swarm 创建成功，swarm-manager 成为 manager node。② 添加 worker node 需要执行的命令。③ 添加 manager node 需要执行的命令。 添加集群节点 在docker-worker01机器上执行以下添加集群节点的操作命令: 1# docker swarm join --token SWMTKN-1-0n04bao3bkte48prmcf1xfmlfrk9zh19b9u16ysb63yhvgjiyi-3w0rd58keboh52zul8xcjfrof 172.16.132.200:2377 如后续要加入其他更多的节点,添加操作也是执行这个命令. 查看集群节点 在docker-manager机器上执行查看,因为此时它是swarm集群的leader节点: 1234root@docker-manager:~# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION7ggeai3dlqn0j8gkxjs46y250 * docker-manager Ready Active Leader 18.09.71v09483w8dczd36bmtruzm2ix docker-node01 Ready Active 19.03.1 最后查看下两个机器上的2375端口是否都已经开启了 1234567[root@docker-manager ~]# lsof -i:2375COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEdockerd 13785 root 5u IPv6 4518841 0t0 TCP *:2375 (LISTEN)[root@docker-woeker01 ~]# lsof -i:2375COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEdockerd-c 2966 root 5u IPv6 3602947 0t0 TCP *:2375 (LISTEN) 部署Portainer12345$ docker volume create portainer_data$ docker run -d -p 8000:8000 -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES9b051147a4c2 portainer/portainer &quot;/portainer&quot; 20 hours ago Up 18 hours 0.0.0.0:9000-&gt;9000/tcp portainer 访问http://172.16.132.200:9000, 同样首次登陆需要注册用户，给admin用户设置密码： 集群模式, 这样一定要选择Remote, 输入docker-worker01的ip，然后点击Connect。 同样点击左边栏的”Endpoints” - “+add endpoint”, 添加集群节点: 添加之后,点击左边栏的”Home”, 右边就可以看到节点信息了,可以进行切换操作.","categories":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/tags/docker/"},{"name":"portainer","slug":"portainer","permalink":"https://www.silenceboy.com/tags/portainer/"}]},{"title":"ubuntu18.04安装mysql并允许远程访问","slug":"ubuntu18-04安装mysql并允许远程访问","date":"2019-07-23T10:29:37.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/07/23/ubuntu18-04安装mysql并允许远程访问/","permalink":"https://www.silenceboy.com/2019/07/23/ubuntu18-04%E5%AE%89%E8%A3%85mysql%E5%B9%B6%E5%85%81%E8%AE%B8%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE/","excerpt":"","text":"安装1$ apt-get install mysql-server mysql-client 字符集修改utf8进入mysql命令终端：（默认root密码为空）: 1$ mysql -u root -p 12345678910111213mysql&gt; show variables like &#x27;char%&#x27;;+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | latin1 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | latin1 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+ 12345678mysql&gt; show variables like &#x27;collation%&#x27;;+----------------------+-------------------+| Variable_name | Value |+----------------------+-------------------+| collation_connection | utf8_general_ci || collation_database | latin1_swedish_ci || collation_server | latin1_swedish_ci |+----------------------+-------------------+ 修改字符集： 123456$ vim /etc/mysql/mysql.conf.d/mysqld.cnf// 在文件末尾添加以下内容：collation-server = utf8_unicode_ciinit-connect=&#x27;SET NAMES utf8&#x27;character-set-server = utf8 重启后字符集修改为utf8： 1$ service mysql restart 修改端口号修改mysql配置文件，然后重启即可生效： 1234$ vim /etc/mysql/mysql.conf.d/mysqld.cnf// 修改以下内容：port = 6033 登录权限问题查看当前用户： 12345678910mysql&gt; SELECT User,Host FROM mysql.user;+------------------+-----------+| User | Host |+------------------+-----------+| debian-sys-maint | localhost || mysql.session | localhost || mysql.sys | localhost || root | localhost |+------------------+-----------+4 rows in set (0.00 sec) 删除root账号： 123456789101112mysql&gt; DROP USER &#x27;root&#x27;@&#x27;localhost&#x27;;Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT User,Host FROM mysql.user;+------------------+-----------+| User | Host |+------------------+-----------+| debian-sys-maint | localhost || mysql.session | localhost || mysql.sys | localhost |+------------------+-----------+3 rows in set (0.00 sec) 重新创建root： 12345678910111213mysql&gt; CREATE USER &#x27;root&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;123456&#x27;;Query OK, 0 rows affected (0.00 sec)mysql&gt; SELECT User,Host FROM mysql.user;+------------------+-----------+| User | Host |+------------------+-----------+| root | % || debian-sys-maint | localhost || mysql.session | localhost || mysql.sys | localhost |+------------------+-----------+4 rows in set (0.00 sec) 授权： 12345mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;%&#x27; WITH GRANT OPTION;Query OK, 0 rows affected (0.01 sec)mysql&gt; FLUSH PRIVILEGES;Query OK, 0 rows affected (0.00 sec) 退出mysql，修改配置文件： 12$ vim /etc/mysql/mysql.conf.d/mysqld.cnf注释这一行：bind-address:127.0.0.1 重新启动mysql: 1$ service mysql restart","categories":[{"name":"mysql","slug":"mysql","permalink":"https://www.silenceboy.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://www.silenceboy.com/tags/mysql/"},{"name":"ubuntu","slug":"ubuntu","permalink":"https://www.silenceboy.com/tags/ubuntu/"}]},{"title":"docker安装gitlab","slug":"docker安装gitlab","date":"2019-07-15T02:49:01.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/07/15/docker安装gitlab/","permalink":"https://www.silenceboy.com/2019/07/15/docker%E5%AE%89%E8%A3%85gitlab/","excerpt":"","text":"docker安装gitlabGitLab 分为 社区版（Community Edition，缩写为 CE）和 企业版（Enterprise Edition，缩写为 EE）。社区版是免费的，而企业版包含一些收费服务，一般来说个人开发者用社区版就足够了。 下载镜像 首先需要先下载 GitLab CE 的镜像，使用下面的命令进行下载，因为文件较大，所以可能需要一点时间，耐心等待即可。 12# 不加 tag 则默认为最新版本 latest$ docker pull gitlab/gitlab-ce 启动运行123456789$ docker run --detach \\ --hostname gitlab.example.com \\ --publish 8443:443 --publish 8880:80 --publish 8222:22 \\ --name gitlab \\ --restart always \\ --volume /srv/gitlab/config:/etc/gitlab \\ --volume /srv/gitlab/logs:/var/log/gitlab \\ --volume /srv/gitlab/data:/var/opt/gitlab \\ gitlab/gitlab-ce:latest 说明: –hostname gitlab.example.com: 设置主机名或域名 –publish 8443:443：将http：443映射到外部端口8443 –publish 8880:80：将web：80映射到外部端口8880 –publish 8222:22：将ssh：22映射到外部端口8222 –name gitlab: 运行容器名 –restart always: 自动重启 –volume &#x2F;srv&#x2F;gitlab&#x2F;config:&#x2F;etc&#x2F;gitlab: 挂载目录 –volume &#x2F;srv&#x2F;gitlab&#x2F;logs:&#x2F;var&#x2F;log&#x2F;gitlab: 挂载目录 –volume &#x2F;srv&#x2F;gitlab&#x2F;data:&#x2F;var&#x2F;opt&#x2F;gitlab: 挂载目录 运行成功之后，可以使用下面的命令查看容器运行状态： 1$ docker ps 可以看到 GitLab 已经在运行了，有一个属性 STATUS 为 health: starting，说明 gitlab 的服务正在启动中，还没有启动完毕。等这个状态变成 healthy 时则说明已经部署完成，可以访问了。 访问gitlab启动成功后，浏览器访问http://ip:8880, 即可访问。为了使用域名访问，需要配置nginx： 1234567891011121314151617upstream gitlab&#123; server 127.0.0.1:8880;&#125;server &#123; listen 80; server_name gitlab.example.com; access_log /var/log/nginx/gitlab.example.com-access.log; error_log /var/log/nginx/gitlab.example.com-error.log; location / &#123; proxy_pass_header Server; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Scheme $scheme; proxy_pass http://gitlab; &#125;&#125; nginx重启配置生效后，浏览器访问http://gitlab.example.com 即可正常访问。 首次访问需要为root用户设置密码，设置完成后需要登录，默认用户名为：root， 密码为刚刚设置的密码。 配置邮件服务器想要让 GitLab 给你发送邮件，还要配置一下邮件服务器，这里以QQ邮箱的 IMAP&#x2F;SMTP服务 来配置。 打开邮箱-&gt;设置-&gt;账户，然后开启 IMAP&#x2F;SMTP服务，然后根据文档获取 授权码 ，这步比较重要。 然后跳转至挂载目录 /srv/gitlab/config/ 编辑gitlab.rb 文件，找到 Email Settings的注释位置，然后修改以下内容： 1234567891011### Email Settingsgitlab_rails[&#x27;smtp_enable&#x27;] = true # 开启 SMTP 功能gitlab_rails[&#x27;smtp_address&#x27;] = &quot;smtp.qq.com&quot;gitlab_rails[&#x27;smtp_port&#x27;] = 465 # 端口不可以选择587，测试过会发送邮件失败gitlab_rails[&#x27;smtp_user_name&#x27;] = &quot;test@qq.com&quot; # 你的邮箱账号gitlab_rails[&#x27;smtp_password&#x27;] = &quot;1324dasd&quot; # 授权码，不是密码gitlab_rails[&#x27;smtp_authentication&#x27;] = &quot;login&quot;gitlab_rails[&#x27;smtp_enable_starttls_auto&#x27;] = truegitlab_rails[&#x27;smtp_tls&#x27;] = truegitlab_rails[&#x27;gitlab_email_from&#x27;] = &#x27;test@qq.com&#x27; # 发件人信息，必须跟‘smtp_user_name’保持一致，否则报错gitlab_rails[&#x27;smtp_domain&#x27;] = &quot;qq.com&quot; # 修改并不影响 配置完成后保存，然后输入下面的命令使配置生效。 1$ docker exec gitlab gitlab-ctl reconfigure 使配置生效之后我们可以使用 gitlab 自带的工具进行一下测试。依次执行下面的命令： 12345678# 开启 gitlab 的 bash 工具$ docker exec -it gitlab bash# 开启 gitlab-rails 工具$ gitlab-rails console production# 发送邮件进行测试Notify.test_email(&#x27;test_001@123.com&#x27;, &#x27;Message Subject&#x27;, &#x27;Message Body&#x27;).deliver_now 测试完成之后退出gitlab的bash工具，重启 gitlab 即可。 1$ docker restart gitlab 修改SSH因为项目启动时gitlab内部的22端口号映射到宿主机的端口号是8222，所以需要配置gitlab的ssh端口号： 找到如下内容，将端口号修改为8222. 1gitlab_rails[&#x27;gitlab_shell_ssh_port&#x27;] = 8222 配置完成后保存，然后输入下面的命令使配置生效。 12$ docker exec gitlab gitlab-ctl reconfigure$ docker restart gitlab 配置 Git 仓库访问路径在之前第一次运行 gitlab 容器的时候，有一个参数 hostname 为 gitlab.example.com , 如果配置了域名可以忽略这一步，如果你没有配置相应域名的话，你的仓库的地址将会变为下面这样： 12ssh : git@gitlab.example.com:test/test.githttp：gitlab.example.com/test/test.git 如果域名不存在的话，这个地址是无法进行 clone 的。 为了解决这个问题，我们可以设置成 IP 或 你配置了的域名来访问。 打开文件 /srv/gitlab/config/gitlab.rb 文件并找到 1# external_url &#x27;GENERATED_EXTERNAL_URL&#x27; 这行，去掉注释，并按照下面的格式修改。 1234567891011# ip 形式external_url &#x27;http://192.168.1.44&#x27;# 域名形式external_url &#x27;http://JemGeek.com&#x27;# 子域名external_url &#x27;http://gitlab.JemGeek.com&#x27;# 其他形式external_url &#x27;http://JemGeek.com/gitlab&#x27; 以上形式都是可以的。修改完成后，输入命令: 1$ docker exec gitlab gitlab-ctl reconfigure 使配置生效，然后重启 gitlab 即可。 升级参照官方的说明， 将原来的容器停止， 然后删除： 12$ docker stop gitlab$ docker rm gitlab 然后重新拉一个新版本的镜像下来， 1$ docker pull gitlab/gitlab-ce 使用原来的运行命令运行： 12345678910$ docker run --detach \\ --hostname gitlab.example.com \\ --publish 8443:443 --publish 8880:80 --publish 8222:22 \\ --name gitlab \\ --restart always \\ --volume /srv/gitlab/config:/etc/gitlab \\ --volume /srv/gitlab/logs:/var/log/gitlab \\ --volume /srv/gitlab/data:/var/opt/gitlab \\ --privileged=true \\ gitlab/gitlab-ce:latest GitLab 在初次运行的时候会自动升级， 为了预防万一， 还是建议先备份一下 /srv/gitlab/ 这个目录。 大版本升级（例如从 8.7.x 升级到 8.8.x）用上面的操作有可能会出现错误， 如果出现错误可以尝试登录到容器内部, 依次执行下面的命令： 12$ gitlab-ctl reconfigure$ gitlab-ctl restart","categories":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/tags/docker/"},{"name":"gitlab","slug":"gitlab","permalink":"https://www.silenceboy.com/tags/gitlab/"}]},{"title":"js打印带颜色的 console 信息","slug":"js打印带颜色的-console-信息","date":"2019-07-12T06:30:24.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/07/12/js打印带颜色的-console-信息/","permalink":"https://www.silenceboy.com/2019/07/12/js%E6%89%93%E5%8D%B0%E5%B8%A6%E9%A2%9C%E8%89%B2%E7%9A%84-console-%E4%BF%A1%E6%81%AF/","excerpt":"","text":"打印红色的hello world: 1console.log(`\\x1b[31mhello world\\x1b[31m`) 以下是可以使用的文本命令的参考： 前景色（文字颜色）： 12345678\\x1b[30m = 黑色\\x1b[31m = 红色\\x1b[32m = 绿色\\x1b[33m = 黄色\\x1b[34m = 蓝色\\x1b[35m = 洋红色\\x1b[36m = 青色\\x1b[37m = 白色 背景色： 12345678\\x1b[40m = 黑色\\x1b[41m = 红色\\x1b[42m = 绿色\\x1b[43m = 黄色\\x1b[44m = 蓝色\\x1b[45m = 洋红色\\x1b[46m = 青色\\x1b[47m = 白色 其他： 1234567\\x1b[0m = 清除样式\\x1b[1m = 加粗\\x1b[2m = 半透明\\x1b[4m = 下划线\\x1b[5m = 闪动\\x1b[7m = 取反：背景色变前景色 前景色变背景色\\x1b[8m = 看不见 但位置还留着","categories":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/tags/javascript/"}]},{"title":"eval()与new Function()","slug":"eval-与new-Function","date":"2019-04-14T12:55:54.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/04/14/eval-与new-Function/","permalink":"https://www.silenceboy.com/2019/04/14/eval-%E4%B8%8Enew-Function/","excerpt":"","text":"evaleval接受字符串参数，解析其中的js代码。如果编译失败，会抛出异常，否则执行其中的代码，计算返回值。 12eval(&#x27;2+2&#x27;); // 4eval(&#x27;console.log(&quot;ok&quot;)&#x27;); // ok 在实际应用中，通常这样转换JSON。 12var jsonStr = &#x27;&#123; &quot;age&quot;: 20, &quot;name&quot;: &quot;jack&quot; &#125;&#x27;;eval(&#x27;(&#x27; + jsonStr + &#x27;)&#x27;); 为什么要加括号呢？ 因为js中{}通常是表示一个语句块，eval只会计算语句块内的值进行返回。加上括号就变成一个整体的表达式。 12console.log( eval(&#x27;&#123;&#125;&#x27;) ); // undefindconsole.log( eval(&#x27;(&#123;&#125;)&#x27;) ); // Object &#123;&#125; 使用eval需要注意执行作用域 1234567var s = 1;function a() &#123; eval(&#x27;var s=2&#x27;); console.log(s);&#125;a(); // 2console.log(s); // 1 在局部环境使用eval便会创建局部变量。可以显示指定eval调用者来改变上下文环境。 1234567var s = &#x27;global&#x27;;function a() &#123; eval(&#x27;var s = &quot;local&quot;&#x27;); console.log(s); // local console.log(eval(&#x27;s&#x27;)); // local console.log(window.eval(&#x27;s&#x27;)); // global&#125; Function在之前我对于Function的了解只限于“定义方法的一种非主流方式”。却忽略了Function与eval相同的字符串参数特性。 语法：var func = new Function(arg1, arg2, ..., functionBody); 实例： 12var add = new Function(&#x27;a&#x27;, &#x27;b&#x27;, &#x27;return a+b;&#x27;);console.log( add(2, 3) ); // 5 由于其形参使用字符串的方式表示，也可以使用1个字符串来描述多个形参。 12var add = new Function(&#x27;a, b&#x27;, &#x27;return a+b;&#x27;);console.log( add(2, 3) ); // 5 在转换JSON的实际应用中，只需要这么做。 12var jsonStr = &#x27;&#123; &quot;age&quot;: 20, &quot;name&quot;: &quot;jack&quot; &#125;&#x27;, json = (new Function(&#x27;return &#x27; + jsonStr))(); eval 与 Function 都有着动态编译js代码的作用，但是在实际的编程中并不推荐使用。如果可以，请用更好的方法替代。 在一些特殊的运用场合，也有一些合理运用的实践。比如模板解析等。","categories":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/tags/javascript/"}]},{"title":"js实现凯撒密码 (Caesars Cipher)","slug":"js实现凯撒密码-Caesars-Cipher","date":"2019-04-04T09:31:04.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/04/04/js实现凯撒密码-Caesars-Cipher/","permalink":"https://www.silenceboy.com/2019/04/04/js%E5%AE%9E%E7%8E%B0%E5%87%AF%E6%92%92%E5%AF%86%E7%A0%81-Caesars-Cipher/","excerpt":"","text":"凯撒密码Caesar cipher，又叫移位密码。移位密码也就是密码中的字母会按照指定的数量来做移位。一个常见的案例就是ROT13密码，字母会移位13个位置。由’A’ ↔ ‘N’, ‘B’ ↔ ‘O’，以此类推。 所有的字母都是大写，不要转化任何非字母形式的字符(例如：空格，标点符号)，遇到这些特殊字符，跳过它们。 判断是否为大写也不难，我们可以通过 .charCodeAt() 返回的 ASCII 码来判断 至于加密的实现，我们可以像这样分情况讨论： 如果当前字符为 A - M 之间，对应的 ASCII 码范围就是 65 - 77，那么 ROT13 加密应该给它的 ASCII 码加 13 如果当前字符为 N - Z 之间，对应的 ASCII 码范围就是 78 - 90，那么 ROT13 加密应该给它的 ASCII 码减 13 如果当前字符为其他 (小写，空格或特殊符号)，那就不应该执行任何操作 12345678910111213141516171819function rot13(str) &#123; let result = &quot;&quot;; for (let i = 0; i &lt; str.length; i++) &#123; const currentCode = str[i].charCodeAt(); if (currentCode &gt; 90 || currentCode &lt; 65) &#123; // 非大写字符 result += String.fromCharCode(currentCode); &#125; else if (currentCode &lt; 78) &#123; // 大写字符 A - M result += String.fromCharCode(currentCode + 13); &#125; else &#123; // 大写字符 N - Z result += String.fromCharCode(currentCode - 13); &#125; &#125; return result;&#125; 通过求余数来进行优化： 123function rot13(str) &#123; return str.replace(/[A-Z]/g, char =&gt; String.fromCharCode(char.charCodeAt() % 26 + 65));&#125;","categories":[{"name":"算法","slug":"算法","permalink":"https://www.silenceboy.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/tags/javascript/"},{"name":"算法","slug":"算法","permalink":"https://www.silenceboy.com/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"使用git进行远程分支同步","slug":"使用git进行远程分支同步","date":"2019-04-03T02:48:12.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/04/03/使用git进行远程分支同步/","permalink":"https://www.silenceboy.com/2019/04/03/%E4%BD%BF%E7%94%A8git%E8%BF%9B%E8%A1%8C%E8%BF%9C%E7%A8%8B%E5%88%86%E6%94%AF%E5%90%8C%E6%AD%A5/","excerpt":"","text":"在git项目目录下执行git remote show origin命令： 12345678910111213➜ api git:(419788c) ✗ git remote show origin* remote origin Fetch URL: git@github.com:silenceboychen/blog.git Push URL: git@github.com:silenceboychen/blog.git HEAD branch: master Remote branches: alibeta tracked dev tracked master tracked production tracked refs/remotes/origin/feat-10.9 stale (use &#x27;git remote prune&#x27; to remove) refs/remotes/origin/feat-docker stale (use &#x27;git remote prune&#x27; to remove) refs/remotes/origin/feat_avatar stale (use &#x27;git remote prune&#x27; to remove) 可以看到有一些状态为stale的分支，这些分支都已经被删除了，但是我们本机上还有记录，这些记录不会通过git pull自动清除。 为了删除这些分支，实现和远程分支的同步，可以执行git remote prune origin 123456➜ api git:(419788c) ✗ git remote prune originPruning originURL: git@github.com:silenceboychen/blog.git * [pruned] origin/feat-10.9 * [pruned] origin/feat-docker * [pruned] origin/feat_avatar 再次查看，发现那些无效分支已经在本机被删除： 1234567891011121314➜ api git:(419788c) ✗ git remote show origin* remote origin Fetch URL: git@github.com:silenceboychen/blog.git Push URL: git@github.com:silenceboychen/blog.git HEAD branch: master Remote branches: alibeta tracked dev tracked master tracked production tracked Local branch configured for &#x27;git pull&#x27;: master merges with remote master Local ref configured for &#x27;git push&#x27;: master pushes to master (local out of date)","categories":[{"name":"git","slug":"git","permalink":"https://www.silenceboy.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://www.silenceboy.com/tags/git/"}]},{"title":"关于js的浅拷贝与深拷贝","slug":"关于js的浅拷贝与深拷贝","date":"2019-04-02T03:10:59.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/04/02/关于js的浅拷贝与深拷贝/","permalink":"https://www.silenceboy.com/2019/04/02/%E5%85%B3%E4%BA%8Ejs%E7%9A%84%E6%B5%85%E6%8B%B7%E8%B4%9D%E4%B8%8E%E6%B7%B1%E6%8B%B7%E8%B4%9D/","excerpt":"","text":"浅拷贝和深拷贝只针对像Object, Array这样的复杂对象的.简单来说，浅拷贝只拷贝一层对象的属性，而深拷贝则递归拷贝了所有层级。 浅拷贝通过 Object.assign 来实现浅拷贝。123456let a = &#123; num: 1&#125;let b = Object.assign(&#123;&#125;, a)a.num = 2console.log(b.num) // 1 通过展开运算符(…)来实现浅拷贝123456let a = &#123; num: 1&#125;let b = &#123;...a&#125;a.num = 2console.log(b.num) // 1 通过属性赋值来实现浅拷贝:123456789101112const obj = &#123; a:1, arr: [2,3] &#125;;const shallowObj = shallowCopy(obj);function shallowCopy(src) &#123; var dst = &#123;&#125;; for (var prop in src) &#123; if (src.hasOwnProperty(prop)) &#123; dst[prop] = src[prop]; &#125; &#125; return dst;&#125; 该方法体现了浅拷贝的问题．因为浅拷贝只会将对象的各个属性进行依次拷贝，并不会进行递归拷贝，而 JavaScript 存储对象都是存地址的，所以浅拷贝会导致 obj.arr 和 shallowObj.arr 指向同一块内存地址． 导致的结果就是： 12shallowObj.arr[1] = 5;obj.arr[1] // = 5 这种情况就需要用到深拷贝了． 深拷贝通过JSON序列化实现深拷贝许多JavaScript框架都提出了自己的解决办法,但是Javascript应该采用那种方法作为标准呐? 在很长一段时间里,这个问题都没有明确的答案.对于JSON安全(也就是说可以被序列化为一个JSON字符串并且可以根据这个字符串解析出一个结构和值完全一样的对象)的对象来说,有一种巧妙的复制方法: var newObj &#x3D; JSON.parse(JSON.stringify(someObj)); 当然,这种方法需要保证对象是JSON安全的,所以只适用于部分情况. 你不知道的JavaScript(上) 该方法的局限性: 会忽略 undefined 会忽略 symbol 不能序列化函数 不能解决循环引用的对象 递归完成深拷贝1234567891011121314function deepCopy(obj)&#123; //判断是否是简单数据类型， if(typeof obj == &quot;object&quot;)&#123; //复杂数据类型 var result = obj.constructor == Array ? [] : &#123;&#125;; for(let i in obj)&#123; result[i] = typeof obj[i] == &quot;object&quot; ? deepCopy(obj[i]) : obj[i]; &#125; &#125;else &#123; //简单数据类型 直接 == 赋值 var result = obj; &#125; return result;&#125;","categories":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/tags/javascript/"}]},{"title":"Linux Shell >/dev/null 2>&1 &含义","slug":"Linux-Shell-dev-null-2-1-含义","date":"2019-04-01T10:04:28.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/04/01/Linux-Shell-dev-null-2-1-含义/","permalink":"https://www.silenceboy.com/2019/04/01/Linux-Shell-dev-null-2-1-%E5%90%AB%E4%B9%89/","excerpt":"","text":"shell中可能经常能看到：echo log &gt; /dev/null 2&gt;&amp;1 &amp;!, 但具体代表什么意思，很多人都不理解，下面对此进行一一讲解： &gt;: 代表重定向到哪里，例如：echo &quot;123&quot; &gt; /home/123.txt, 表示将字符串&#39;123&#39;写入文件/home/123.txt中, /dev/null: 代表空设备文件. %&gt;: 用来定义输出形式，其中&#39;%&#39;有几种可选值: 0: 标准输入 1: 表示stdout标准输出，系统默认值是1，所以&quot;&gt;/dev/null&quot;等同于&quot;1&gt;/dev/null“ 2: 表示stderr标准错误 &amp;: 表示等同于的意思，2&gt;&amp;1，表示2的输出重定向等同于1 所以&gt; /dev/null 2&gt;&amp;1可以分两步理解为: 1 &gt; /dev/null: 首先表示标准输出重定向到空设备文件，也就是不输出任何信息到终端，说白了就是不显示任何信息。 2&gt;&amp;1: 接着，标准错误输出重定向（等同于）标准输出，因为之前标准输出已经重定向到了空设备文件，所以标准错误输出也重定向到空设备文件。 最后一个&amp;! ， 是让该命令在后台执行，并且关闭终端后不退出。","categories":[{"name":"shell","slug":"shell","permalink":"https://www.silenceboy.com/categories/shell/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://www.silenceboy.com/tags/linux/"},{"name":"shell","slug":"shell","permalink":"https://www.silenceboy.com/tags/shell/"}]},{"title":"[转]Node.js的模块-exports和module.exports","slug":"转-Node-js的模块-exports和module-exports","date":"2019-03-26T03:41:32.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/03/26/转-Node-js的模块-exports和module-exports/","permalink":"https://www.silenceboy.com/2019/03/26/%E8%BD%AC-Node-js%E7%9A%84%E6%A8%A1%E5%9D%97-exports%E5%92%8Cmodule-exports/","excerpt":"","text":"原文链接： http://zhanglun.xyz/2014/04/26/%E8%AF%91-node-js%E7%9A%84%E6%A8%A1%E5%9D%97-exports-%E5%92%8C-module-exports/ exports 和 module.exports 有什么区别？你一定很熟悉 Node.js 模块中的用来在你的模块中创建函数的 exports 对象，就像下面这样。 创建一个叫做 rocker.js 的文件： 123exports.name = function() &#123; console.log(&#x27;My name is Lemmy Kilmister&#x27;);&#125;; 然后可以在另外一个文件中调用 rocker.js : 12var rocker = require(&#x27;./rocker.js&#x27;);rocker.name(); // &#x27;My name is Lemmy Kilmister&#x27; 但是，module.exports 到底什么？它是合法的吗？ 令人吃惊的是：module.exports 是真实存在的。exports 只不过是 module.exports 的帮手而已。你的模块直接返回返回 module.exports 给调用者，而不是 exports 。所有的 exports 做的工作实际上是收集属性，如果 module.exports 当前没有任何属性，exports便将收集到的属性添加到 module.exports 上。如果 module.exports已经存在若干属性，所以 exports 上的属性都会被忽略。 修改 rocker.js 文件： 1234module.exports = &#x27;ROCK IT!&#x27;;exports.name = function() &#123; console.log(&#x27;My name is Lemmy Kilmister&#x27;);&#125;; 在另一个文件中调用 rocker.js: 12var rocker = require(&#x27;./rocker.js&#x27;);rocker.name(); // TypeError: Object ROCK IT! has no method &#x27;name&#x27; 上述例子中的 rocker 模块完全将 exports.name 忽略了，只返回了一个 String 字符串：‘ROCK IT!’ 。 从这个例子你大概明白了：你的模块并不一定总是一个模块的实例(module instance)，它可以是任何合法的 JavaScript 对象——boolean, number, date, JSON, string, function, array 和其他的。你的模块可以是任何你设置的 module.exports 的值。如果你没有明确地为 module.exports 设置任何值，那么 exports 中的属性会自动添加到 module.exports 中，然后并返回它。 在这种情况下，你的模块是一个类： 1234567module.exports = function(name, age) &#123; this.name = name; this.age = age; this.about = function() &#123; console.log(this.name +&#x27; is &#x27;+ this.age +&#x27; years old&#x27;); &#125;;&#125;; 而你可以像这样使用： 123var Rocker = require(&#x27;./rocker.js&#x27;);var r = new Rocker(&#x27;Ozzy&#x27;, 62);r.about(); // Ozzy is 62 years old 在这时候你的模块是一个数组： 1234567module.exports = [ &#x27;Lemmy Kilmister&#x27;, &#x27;Ozzy Osbourne&#x27;, &#x27;Ronnie James Dio&#x27;, &#x27;Steven Tyler&#x27;, &#x27;Mick Jagger&#x27;]; 而你可以这样使用： 12var rocker = require(&#x27;./rocker.js&#x27;);console.log(&#x27;Rockin in heaven: &#x27; + rocker[2]); //Rockin in heaven: Ronnie James Dio 现在你应该明白了点什么：如果你想让你的模块返回一个特殊的对象类型，比如构造函数，那么你得使用 module.exports ；如果你只想模块作为一个典型的模块实例（module instance），那么就用exports。 把属性添加到 module.exports 中和添加到 exports 中的结果是一样的。比如像这样： 123module.exports.name = function() &#123; console.log(&#x27;My name is Lemmy Kilmister&#x27;);&#125;; 其实和下面的是一样的： 123exports.name = function() &#123; console.log(&#x27;My name is Lemmy Kilmister&#x27;);&#125;; 但是要注意，他们不是同一个东西。就像之前说的一样，exports 只不过是 module.exports 的帮手而已。话虽如此，exports还是推荐的对象，除非你想把你模块的对象类型从传统的模块实例（module instance）修改为其他的。 只要你没有使用赋值运算重写module.exports对象，任何添加到 module.exports和exports的属性都能够在 require模块中。 比如这是你的模块中的内容： 12module.exports.age = 68;exports.name = &#x27;Lemmy Kilmister&#x27;; 下面的代码可以很好的工作： 12var rocker = require(&#x27;./rocker.js&#x27;);console.log(&#x27;%s is %s&#x27;, rocker.name, rocker.age); // Lemmy Kilmister is 68 但是，如果你在你的模块中重写了module.exports中的任何地方，代码便会出错： 123module.exports = &#x27;LOL&#x27;;module.exports.age = 68;exports.name = &#x27;Lemmy Kilmister&#x27;; 或者这样： 123module.exports.age = 68;exports.name = &#x27;Lemmy Kilmister&#x27;;module.exports = &#x27;WTF&#x27;; 顺序没有关系，rocker.age 和 rocker.name 将显示为 undefined。 并且注意：只是因为 module.exports 和 exports 都能输出模块，并不意味这你可以组合使用。我的建议是，坚持使用 exports.*，明白module.exports 我希望这篇文章能帮助你理解exports和module.exports之间的不同，并且能进一步的理解模块在Node.js中是怎么工作的。","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"}]},{"title":"关于LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to github.com:443错误的两种解决方案","slug":"关于LibreSSL-SSL-connect-SSL-ERROR-SYSCALL-in-connection-to-github-com-443错误的两种解决方案","date":"2019-03-23T04:54:30.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/03/23/关于LibreSSL-SSL-connect-SSL-ERROR-SYSCALL-in-connection-to-github-com-443错误的两种解决方案/","permalink":"https://www.silenceboy.com/2019/03/23/%E5%85%B3%E4%BA%8ELibreSSL-SSL-connect-SSL-ERROR-SYSCALL-in-connection-to-github-com-443%E9%94%99%E8%AF%AF%E7%9A%84%E4%B8%A4%E7%A7%8D%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88/","excerpt":"","text":"错误来源使用使用hexo部署博客是，遇到以下错误: 123456fatal: unable to access &#x27;https://github.com/silenceboychen/silenceboychen.github.io.git/&#x27;: LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to github.com:443FATAL Something&#x27;s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlError: Spawn failed at ChildProcess.&lt;anonymous&gt; (/Users/chenhao/new_start/nodejs/blog/node_modules/hexo-util/lib/spawn.js:52:19) at ChildProcess.emit (events.js:182:13) at Process.ChildProcess._handle.onexit (internal/child_process.js:239:12) 两种解决方案：方案一取消http代理： 12$ git config --global --unset http.proxy$ git config --global --unset https.proxy 设置env GIT_SSL_NO_VERIFY为true然后再次部署： 1$ env GIT_SSL_NO_VERIFY=true hexo d 问题解决。 方案二在hexo项目的根目录下的_config.yml文件中把仓库链接地址由https修改为ssh的地址。","categories":[{"name":"git","slug":"git","permalink":"https://www.silenceboy.com/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://www.silenceboy.com/tags/git/"},{"name":"hexo","slug":"hexo","permalink":"https://www.silenceboy.com/tags/hexo/"}]},{"title":"统计指定目录里的代码行数","slug":"统计指定目录里的代码行数","date":"2019-03-21T01:34:04.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/03/21/统计指定目录里的代码行数/","permalink":"https://www.silenceboy.com/2019/03/21/%E7%BB%9F%E8%AE%A1%E6%8C%87%E5%AE%9A%E7%9B%AE%E5%BD%95%E9%87%8C%E7%9A%84%E4%BB%A3%E7%A0%81%E8%A1%8C%E6%95%B0/","excerpt":"","text":"codeLineCount一个用来统计项目内代码行数的工具. Installation1$ npm install -g code-rows-count or 1$ yarn global add code-rows-count Usage123456789$ codeLineCount -hUsage: codeLineCount [options]Options: -V, --version output the version number -p, --filePath [filePath] 文件路径 -i, --ignoreFile [ignoreFile] 忽略文件 -h, --help output usage information -p: 需要统计代码行数项目的绝对路径 -i: 不需要参与统计的项目内的文件名, 多个文件名使用逗号分割 example: 12345678910111213141516$ codeLineCount -p /home/silence/nodejs/test -i node_modules,yarn.lock,.git,package.json.lock文件路径:/home/silence/nodejs/test/color.js, 文件行数:19文件路径:/home/silence/nodejs/test/commander.js, 文件行数:14文件路径:/home/silence/nodejs/test/http.js, 文件行数:6文件路径:/home/silence/nodejs/test/package.json, 文件行数:6文件路径:/home/silence/nodejs/test/aaa/color.js, 文件行数:19------------------分割线start------------------done, 总耗时: 5 ms总文件数:5, 总代码行数: 64------------------分割线end------------------","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"}]},{"title":"express.js中间件说明","slug":"express-js中间件说明","date":"2019-03-19T01:39:06.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/03/19/express-js中间件说明/","permalink":"https://www.silenceboy.com/2019/03/19/express-js%E4%B8%AD%E9%97%B4%E4%BB%B6%E8%AF%B4%E6%98%8E/","excerpt":"","text":"express的新开发人员往往对路由处理程序和中间件之间的区别感到困惑。因此他们也对app.use(),app.all(),app.get(),app.post(),app.delete()和app.put()方法的区别感到困惑。 在本文中,我将解释中间件和路由处理程序之间的区别。以及如何正确使用app.use(),app.all(),app.get(),app.post(),app.delete()和app.put()方法。 路由处理app.use(),app.all(),app.get(),app.post(),app.delete()和app.put()全部是用来定义路由的。这些方法都用于定义路由。路由用于处理HTTP请求。路由是路径和回调的组合，在请求的路径匹配时执行。回调被称为路由处理程序。 它们之间的区别是处理不同类型的HTTP请求。例如： app.get()方法仅仅处理get请求，而app.all()处理GET、POST等请求。 下面是一个例子,如何定义一个路由： 1234567var app = require(&quot;express&quot;)();app.get(&quot;/&quot;, function(req, res, next)&#123; res.send(&quot;Hello World!!!!&quot;);&#125;);app.listen(8080); 每个路由处理程序都获得对当前正在提供的HTTP请求的请求和响应对象的引用。 可以为单个HTTP请求执行多个路由处理程序。这是一个例子： 1234567891011121314var app = require(&quot;express&quot;)();app.get(&quot;/&quot;, function(req, res, next)&#123; res.write(&quot;Hello&quot;); next();&#125;);app.get(&quot;/&quot;, function(req, res, next)&#123; res.write(&quot; World !!!&quot;); res.end();&#125;);app.listen(8080); 这里第一个句柄写入一些响应，然后调用next()。 next()方法用于调用与路径路径匹配的下一个路由处理程序。 路由处理程序必须结束请求或调用下一个路由处理程序。 我们还可以将多个路由处理程序传递给app.all()，app.get()，app.post()，app.delete()和app.put()方法。 这是一个证明这一点的例子： 1234567891011var app = require(&quot;express&quot;)();app.get(&quot;/&quot;, function(req, res, next)&#123; res.write(&quot;Hello&quot;); next();&#125;, function(req, res, next)&#123; res.write(&quot; World !!!&quot;); res.end();&#125;);app.listen(8080); 中间件中间件是一个位于实际请求处理程序之上的回调。它采用与路由处理程序相同的参数。 要了解中间件，我们来看一个带有dashboard和profile页面的示例站点。要访问这些页面，用户必须登录。还会记录对这些页面的请求。 以下是这些页面的路由处理程序的代码： 123456789101112131415161718192021222324252627282930313233343536var app = require(&quot;express&quot;)();function checkLogin()&#123; return false;&#125;function logRequest()&#123; console.log(&quot;New request&quot;);&#125;app.get(&quot;/dashboard&quot;, function(req, res, next)&#123; logRequest(); if(checkLogin())&#123; res.send(&quot;This is the dashboard page&quot;); &#125; else&#123; res.send(&quot;You are not logged in!!!&quot;); &#125;&#125;);app.get(&quot;/profile&quot;, function(req, res, next)&#123; logRequest(); if(checkLogin())&#123; res.send(&quot;This is the dashboard page&quot;); &#125; else&#123; res.send(&quot;You are not logged in!!!&quot;); &#125;&#125;);app.listen(8080); 这里的问题是有很多重复的代码，即我们不得不多次使用logRequest()和checkLogin()函数。这也使得更新代码变得困难。因此，为了解决这个问题，我们可以为这两条路径编写一条通用路径。 这是重写的代码： 123456789101112131415161718192021222324252627282930313233var app = require(&quot;express&quot;)();function checkLogin()&#123; return false;&#125;function logRequest()&#123; console.log(&quot;New request&quot;);&#125;app.get(&quot;/*&quot;, function(req, res, next)&#123; logRequest(); next();&#125;)app.get(&quot;/*&quot;, function(req, res, next)&#123; if(checkLogin())&#123; next(); &#125; else&#123; res(&quot;You are not logged in!!!&quot;); &#125;&#125;)app.get(&quot;/dashboard&quot;, function(req, res, next)&#123; res.send(&quot;This is the dashboard page&quot;);&#125;);app.get(&quot;/profile&quot;, function(req, res, next)&#123; res.send(&quot;This is the dashboard page&quot;);&#125;);app.listen(8080); 这里的代码看起来更清晰，更易于维护和更新。这里将前两个定义的路由处理程序称为中间件，因为它们不处理请求，而是负责预处理请求。 Express为我们提供了app.use()方法，该方法专门用于定义中间件。 app.use()方法可能看起来与app.all()类似，但它们之间存在很多差异，这使得app.use()非常适合于声明中间件。让我们看看app.use()方法是如何工作的： app.use() 和 app.all() 的不同:CALLBACKapp.use()只需要一个回调，而app.all()可以进行多次回调。 PATHapp.use()只查看url是否以指定路径开头,app.all()匹配完整路径。 这里有一个例子来说明: 123456789101112131415app.use( &quot;/product&quot; , mymiddleware);// will match /product// will match /product/cool// will match /product/fooapp.all( &quot;/product&quot; , handler);// will match /product// won&#x27;t match /product/cool &lt;-- important// won&#x27;t match /product/foo &lt;-- importantapp.all( &quot;/product/*&quot; , handler);// won&#x27;t match /product &lt;-- Important// will match /product/cool// will match /product/foo NEXT()中间件内的next()调用下一个中间件或路由处理程序，具体取决于接下来声明的那个。但是路由处理程序中的next()仅调用下一个路由处理程序。如果接下来有中间件，则跳过它。因此，必须在所有路由处理程序之前声明中间件。 这里有一个例子来说明: 1234567891011121314151617181920212223242526272829var express = require(&#x27;express&#x27;);var app = express();app.use(function frontControllerMiddlewareExecuted(req, res, next)&#123; console.log(&#x27;(1) this frontControllerMiddlewareExecuted is executed&#x27;); next();&#125;);app.all(&#x27;*&#x27;, function(req, res, next)&#123; console.log(&#x27;(2) route middleware for all method and path pattern &quot;*&quot;, executed first and can do stuff before going next&#x27;); next();&#125;);app.all(&#x27;/hello&#x27;, function(req, res, next)&#123; console.log(&#x27;(3) route middleware for all method and path pattern &quot;/hello&quot;, executed second and can do stuff before going next&#x27;); next();&#125;);app.use(function frontControllerMiddlewareNotExecuted(req, res, next)&#123; console.log(&#x27;(4) this frontControllerMiddlewareNotExecuted is not executed&#x27;); next();&#125;);app.get(&#x27;/hello&#x27;, function(req, res)&#123; console.log(&#x27;(5) route middleware for method GET and path patter &quot;/hello&quot;, executed last and I do my stuff sending response&#x27;); res.send(&#x27;Hello World&#x27;);&#125;);app.listen(80); 现在我们看到了app.use()方法的唯一性以及它用于声明中间件的原因。 让我们重写我们的示例站点代码： 12345678910111213141516171819202122232425262728293031323334var app = require(&quot;express&quot;)();function checkLogin()&#123; return false;&#125;function logRequest()&#123; console.log(&quot;New request&quot;);&#125;app.use(function(req, res, next)&#123; logRequest(); next();&#125;)app.use(function(req, res, next)&#123; if(checkLogin())&#123; next(); &#125; else&#123; res.send(&quot;You are not logged in!!!&quot;); &#125;&#125;)app.get(&quot;/dashboard&quot;, function(req, res, next)&#123; res.send(&quot;This is the dashboard page&quot;);&#125;);app.get(&quot;/profile&quot;, function(req, res, next)&#123; res.send(&quot;This is the dashboard page&quot;);&#125;);app.listen(8080);","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"express.js","slug":"express-js","permalink":"https://www.silenceboy.com/tags/express-js/"},{"name":"middleware","slug":"middleware","permalink":"https://www.silenceboy.com/tags/middleware/"}]},{"title":"nodejs实现tail -f功能","slug":"nodejs实现tail-f功能","date":"2019-03-15T01:37:54.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2019/03/15/nodejs实现tail-f功能/","permalink":"https://www.silenceboy.com/2019/03/15/nodejs%E5%AE%9E%E7%8E%B0tail-f%E5%8A%9F%E8%83%BD/","excerpt":"","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&#x27;use strict&#x27;;const fs = require(&#x27;fs&#x27;);/** * tailf * * @param &#123;String&#125; filename 文件名 * @param &#123;Number&#125; delay 读取不到内容时等待的时间，ms * @param &#123;Function&#125; onError 操作出错时的回调函数，onError(err) * @param &#123;Function&#125; onData 读取到文件内容时的回调函数，onData(data) */function tailf(filename, delay, onError, onData) &#123; // 每次读取文件块大小，16K const CHUNK_SIZE = 16 * 1024; // 打开文件，获取文件句柄 fs.open(filename, &#x27;r&#x27;, (err, fd) =&gt; &#123; if (err) return onError(err); // 文件开始位置 fs.fstat(fd, (err, stats) =&gt; &#123; if (err) return onError(err); // 文件开始位置 let position = stats.size; // 循环读取 const loop = () =&gt; &#123; const buf = Buffer.alloc(CHUNK_SIZE); fs.read(fd, buf, 0, CHUNK_SIZE, position, (err, bytesRead, buf) =&gt; &#123; if (err) return onError(err); // 实际读取的内容长度以 bytesRead 为准 // 并且更新 position 位置 position += bytesRead; onData(buf.slice(0, bytesRead)); if (bytesRead &lt; CHUNK_SIZE) &#123; // 如果当前已到达文件末尾，则先等待一段时间再继续 // setTimeout(loop, delay); &#125; else &#123; loop(); &#125; &#125;); &#125;; loop(); // 监听文件变化，如果收到 change 事件则尝试读取文件内容 fs.watch(filename, (event, filename) =&gt; &#123; if (event === &#x27;change&#x27;) &#123; loop(); &#125; &#125;); &#125;); &#125;);&#125;const filename = process.argv[2];if (filename) &#123; tailf(filename, 100, err =&gt; &#123; if (err) console.error(err); &#125;, data =&gt; &#123; process.stdout.write(data); &#125;);&#125; else &#123; console.log(&#x27;使用方法： node tailf &lt;文件名&gt;&#x27;);&#125;","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"},{"name":"tailf","slug":"tailf","permalink":"https://www.silenceboy.com/tags/tailf/"}]},{"title":"在docker中执行gitlab-runner","slug":"在docker中执行gitlab-runner","date":"2018-05-20T03:45:45.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2018/05/20/在docker中执行gitlab-runner/","permalink":"https://www.silenceboy.com/2018/05/20/%E5%9C%A8docker%E4%B8%AD%E6%89%A7%E8%A1%8Cgitlab-runner/","excerpt":"","text":"环境:ubuntu 16.04 LTS目的：使用Docker安装和配置GitLab Runner，搭建GitLab CI持续集成环境。 安装gitlab-runner1234$ docker run -d --name gitlab-runner --restart always \\ -v /srv/gitlab-runner/config:/etc/gitlab-runner \\ -v /var/run/docker.sock:/var/run/docker.sock \\ gitlab/gitlab-runner:latest 参数说明： -d: 设置容器后台运行 –name：容器名称 -restart always：每次启动容器就重启 gitlab-runner -v: 共享目录挂载 安装好后，执行$ docker ps 查看容器是否运行。 注册和初始化1$ docker exec -it gitlab-runner gitlab-ci-multi-runner register gitlab-runner register是进入gitlab-runner容器的执行命令，用于注册和初始化gitlab-runner。以下是我的配置：注意：docker image为满足你项目构建所需环境的镜像。 我们也可以编辑vim /srv/gitlab-runner/config/config.toml，手动修改配置： 12345678910111213141516171819concurrent = 1check_interval = 0[[runners]] name = &quot;test&quot; url = &quot;https://xxxx.oooo.com&quot; token = &quot;3894a417b64744e942008bcc51123a&quot; executor = &quot;docker&quot; builds_dir = &quot;/gitlab/runner-builds&quot; cache_dir = &quot;/gitlab/runner-cache&quot; [runners.docker] tls_verify = false image = &quot;node:latest&quot; privileged = false disable_cache = false volumes = [&quot;/data/gitlab-runner:/gitlab&quot;] shm_size = 0 pull_policy = &quot;if-not-present&quot; [runners.cache] gitlab-ci token可以从gitlab上的项目的CI设置中获得。builds_dir 为文件存放位置volumes 挂载目录pull_policy 设置gitlab是否从远程拉去image,如果iamge是本地的需要配置该属性的值为: if-not-present 或者 never 创建.gitlab-ci.yml文件我的项目为nodejs项目，以下为测试配置。 1234567891011121314151617181920212223stages: - installcache: key: $&#123;CI_BUILD_REF_NAME&#125; paths: - node_modules/job-install: stage: install script: - whoami - echo $SHELL - rm -rf node_modules/ - pwd - source ~/.bashrc - nvm use 8 - node -v - yarn only: - preview tags: - test 配置好gitlab-ci文件之后，提交修改，并将最新的修改推送到origin&#x2F;preview分支，即可触发CI: 12345678910111213141516171819202122232425262728293031323334353637Running with gitlab-runner 10.2.0 (0a75cdd1) on test (3894a417)Using Docker executor with image followme/node:v1 ...Using docker image sha256:07e33b24b6a9bebc0e0d8ba24f15b4b3c0f6fcf321a3809371a6211ac1afc38e for predefined container...Using locally found image version due to if-not-present pull policyUsing docker image followme/node:v1 ID=sha256:c99c549e8227e2323d1cebb6f988d5d8f6de7f77e1967fe0f02878b85cb72b0f for build container...Running on runner-3894a417-project-643-concurrent-0 via 304e3efed168...Cloning repository...Cloning into &#x27;/gitlab/runner-builds/3894a417/0/Frontend/api-member&#x27;...Checking out 311e85cb as preview...Skipping Git submodules setupChecking cache for preview...Successfully extracted cache$ whoamiroot$ echo $SHELL/bin/bash$ rm -rf node_modules/$ pwd/gitlab/runner-builds/3894a417/0/Frontend/api-member$ source ~/.bashrc$ nvm use 8Now using node v8.3.0 (npm v5.3.0)$ node -vv8.3.0$ yarnyarn install v1.3.2[1/4] Resolving packages...[2/4] Fetching packages...[3/4] Linking dependencies...[4/4] Building fresh packages...Done in 7.21s.Creating cache preview...node_modules/: found 5627 matching files Created cacheJob succeeded 注意：之前我是在Ubuntu14.04版本的系统上做这些配置，但是当执行CI的时候总会遇到以下报错:ERROR: Preparation failed: Error reading remote info: json: cannot unmarshal number into Go struct field Info.Debug of type bool 将系统升级为16.04后解决该问题","categories":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/tags/docker/"},{"name":"gitlab-runner","slug":"gitlab-runner","permalink":"https://www.silenceboy.com/tags/gitlab-runner/"}]},{"title":"mysql，sql server等数据库连接集成库","slug":"mysql，sql-server等数据库连接集成库","date":"2018-04-08T09:33:08.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2018/04/08/mysql，sql-server等数据库连接集成库/","permalink":"https://www.silenceboy.com/2018/04/08/mysql%EF%BC%8Csql-server%E7%AD%89%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E9%9B%86%E6%88%90%E5%BA%93/","excerpt":"","text":"地址: https://github.com/fmfe/lib-sql Installation12$ npm install @fmfe/lib-sql or 1$ yarn add @fmfe/lib-sql Usage有两种传入配置信息的方式: 使用config来管理我们的配置文件.假设我们的项目目录下有一个config目录,config目录里有一个dev.json文件. config&#x2F;dev.json 1234567891011121314151617181920212223&#123; &quot;mysql&quot;: &#123; &quot;host&quot;: &quot;127.0.0.1&quot;, &quot;port&quot;: 3306, &quot;database&quot;: &quot;test&quot;, &quot;user&quot;: &quot;root&quot;, &quot;password&quot;: &quot;123456&quot; &#125;, &quot;mssql&quot;: &#123; &quot;user&quot;: &quot;sa&quot;, &quot;password&quot;: &quot;123456&quot;, &quot;server&quot;: &quot;127.0.0.1&quot;, &quot;database&quot;: &quot;test&quot;, &quot;port&quot;: 1433, &quot;pool&quot;: &#123; &quot;min&quot;: 0, &quot;max&quot;: 10, &quot;idleTimeoutMillis&quot;: 3000 &#125; &#125;&#125; mysql.js 12345678910111213141516171819202122232425262728293031const &#123; mysql &#125; = require(&#x27;@fmfe/lib-sql&#x27;);const mysqlPool = mysql.init();const _getNewSqlParamEntity = mysql._getNewSqlParamEntity;// 执行单条sql语句 // mysql.exec(mysqlPool, sql, params);async function exec() &#123; const sql1 = &#x27;select * from ?? limit 2&#x27;; const data = await mysql.exec(mysqlPool, sql1, [&#x27;tbl_user&#x27;]);&#125;// 执行mysql事务,可以传入多条增/删/改sql语句 // mysql.exectrans(mysqlpool, sqlParamsEntity);async function execTrans() &#123; const sqlParamsEntity = []; const sql1 = &#x27;insert into ?? (name, age, sex) values (?, ?, ?)&#x27;; const param1 = [&#x27;tbl_user&#x27;, &#x27;aaa&#x27;, 20, 1]; sqlParamsEntity.push(_getNewSqlParamEntity(sql1, param1)); const sql2 = &#x27;insert into ?? (name, age, sex) values (?, ?, ?)&#x27;; const param2 = [&#x27;tbl_user&#x27;, &#x27;bbb&#x27;, 22, 0]; sqlParamsEntity.push(_getNewSqlParamEntity(sql2, param2)); const sql3 = &#x27;update ?? set age = ? where id = ?&#x27;; const param3 = [&#x27;tbl_user&#x27;, 10, 1]; sqlParamsEntity.push(_getNewSqlParamEntity(sql3, param3)); // .... const data = await mysql.execTrans(mysqlPool, sqlParamsEntity); &#125; mssql.js 123456789101112131415161718192021222324const &#123; mssql &#125; = require(&#x27;@fmfe/lib-sql&#x27;);const _getNewSqlParamEntity = mssql._getNewSqlParamEntity;// 执行单条语句 // mssql.exec(sql)async function exec() &#123; const sql1 = &#x27;select Top 3 name, age, sex from tbl_user order by age desc&#x27;; const data = await mssql.exec(sql1); &#125;// 执行sql server 事务, 最好执行增/删/改语句,这里只是用select演示使用方法// mssql.exectrans(sqlParamsEntity); async function exectrans() &#123; const sqlParamsEntity = []; const sql1 = &#x27;select * from tbl_user where id = 1&#x27;; sqlParamsEntity.push(_getNewSqlParamEntity(sql1)); const sql2 = &#x27;select * from tbl_user where id = 2&#x27;; sqlParamsEntity.push(_getNewSqlParamEntity(sql2)); // ... const data = await mssql.execTrans(sqlParamsEntity); &#125; 由于使用config管理配置文件, 运行项目时通过使用命令: NODE_ENV=dev node ..., @fmfe/lib-sql即可自动获取到数据库相关配置. 通过传入配置文件来调用库我们引用上边的代码示例,只需做一点改动: mysql.js 只需在初始化时传入mysql数据库配置就好. 123456789101112const &#123; mysql &#125; = require(&#x27;@fmfe/lib-sql&#x27;);const mysqlPool = mysql.init(&#123; host: &#x27;127.0.0.1&#x27;, port: 3306, database: &#x27;test&#x27;, user: &#x27;root&#x27;, password: &#x27;123456&#x27;&#125;);...... mssql.js 在每次调用方法时传入配置 12345678910111213141516171819202122232425const &#123; mssql &#125; = require(&#x27;@fmfe/lib-sql&#x27;);const config = &#123; user: &#x27;sa&#x27;, password: &#x27;123456&#x27;, server: &#x27;127.0.0.1&#x27;, database: &#x27;test&#x27;, port: 1433, pool: &#123; min: 0, max: 10, idleTimeoutMillis: 3000 &#125; &#125;async function exec() &#123; ...... const data = await mssql.exec(sql1, config); &#125;async function exectrans() &#123; ...... const data = await mssql.execTrans(sqlParamsEntity, config); &#125;","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://www.silenceboy.com/tags/mysql/"},{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"},{"name":"sql server","slug":"sql-server","permalink":"https://www.silenceboy.com/tags/sql-server/"}]},{"title":"docker搭建私有仓库、自签发证书、登录认证","slug":"docker搭建私有仓库、自签发证书、登录认证","date":"2018-03-28T01:24:21.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2018/03/28/docker搭建私有仓库、自签发证书、登录认证/","permalink":"https://www.silenceboy.com/2018/03/28/docker%E6%90%AD%E5%BB%BA%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E3%80%81%E8%87%AA%E7%AD%BE%E5%8F%91%E8%AF%81%E4%B9%A6%E3%80%81%E7%99%BB%E5%BD%95%E8%AE%A4%E8%AF%81/","excerpt":"","text":"docker官方文档对如何搭建私有仓库说的已经很详细了，我在这里主要介绍一下使用自签发证书如何搭建私有仓库并认证成功。 假设registry的域名为：registry.domain.com。 生成自签发证书。12&gt; mkdir -p certs&gt; openssl req -newkey rsa:2048 -nodes -sha256 -keyout certs/domain.key -x509 -days 365 -out certs/domain.crt 执行以上命令，生成证书，Common Name那里要输入我们registry的域名，生成的证书只对该域名有效。其他的可以任意填。生成后可以在certs目录下查看到证书。 生成鉴权密码文件 注意使用时username替换为你自己的用户名，password替换为你自己的密码。 123$ mkdir auth$ docker run --entrypoint htpasswd registry:2 -Bbn username password &gt; auth/htpasswd$ ls auth 启动Registry1234567891011docker run -d -p 5000:5000 --restart=always --name registry \\ -v `pwd`/auth:/auth \\ -e &quot;REGISTRY_AUTH=htpasswd&quot; \\ -e &quot;REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm&quot; \\ -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \\ -v `pwd`/data:/var/lib/registry \\ -v `pwd`/certs:/certs \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\ registry:2199ad0b3591fb9613b21b1c96f017267f3c39661a7025d30df636c6805e7ab50 如果没有registry镜像会自动下载然后启动，可以使用阿里云提供的加速器。现在完成了registry服务器的搭建，可以尝试pull image到registry： 123$ docker pull busybox // 从官方拉去镜像作为我们的测试镜像$ docker tag busybox:latest registry.domain.com:5000/busybox:latest // 为busybox打tag,tag的前缀一定要为我们registry服务器的域名。$ docker push registry.domain.com:5000/busybox:latest // 将镜像推送到我们的registry服务器 如果直接这样去push，会失败，并且出现 no basic auth credentials的错误，这是因为我们没有进行登录认证。 12345$ docker login registry.domain.com:5000$ Username: username$ Password: passwordWARNING: login credentials saved in ~/.docker/config.jsonLogin Succeeded 登录成功后再次执行push操作，会出现x509: certificate signed by unknown authority的报错。这是因为docker client认为server传输过来的证书的签署方是一个unknown authority（未知的CA），因此验证失败。我们需要让docker client安装我们的CA证书： 123$ sudo mkdir -p /etc/docker/certs.d/registry.domain.com:5000$ sudo cp certs/domain.crt /etc/docker/certs.d/registry.domain.com:5000/ca.crt$ sudo service docker restart //安装证书后，重启Docker Daemon 再次执行push操作，成功推送： 1234$ docker push registry.domain.com:5000/busybox:latestThe push refers to a repository [registry.domain.com:5000/busybox]0271b8eebde3: Pushedlatest: digest: sha256:3571ca1b0e90e159de4fc07b3bf94ef189a0645314704f629204adb7035ecf45 size: 527 这里需要注意：如果使用自签署的证书，那么所有要与Registry交互的Docker主机都需要安装registry.domain.com的ca.crt(domain.crt)。但如果你使用知名CA，这一步也就可以忽略。如果是MacOS版的docker也是一样的操作，尽管它连&#x2F;etc&#x2F;docker都不存在,一样去创建目录就好了。只是macOS的用户想要认证生效需要执行额外的命令： 1$ sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain /etc/docker/certs.d/registry.domain.com:5000/ca.crt 此时在mac上执行docker login才可成功。","categories":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/tags/docker/"}]},{"title":"容联云发送短信模块","slug":"容联云发送短信模块","date":"2018-01-28T01:22:30.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2018/01/28/容联云发送短信模块/","permalink":"https://www.silenceboy.com/2018/01/28/%E5%AE%B9%E8%81%94%E4%BA%91%E5%8F%91%E9%80%81%E7%9F%AD%E4%BF%A1%E6%A8%A1%E5%9D%97/","excerpt":"","text":"rongSMS一个nodejs实现的容联云发送短信模板模块（云联云官方没有提供nodejs实现的版本），支持node7.6以上版本。支持所有短信模板。一些常用的容联云返回码： code 含义 000000 发送成功 160038 短信验证码发送过频繁 160040 该手机号短信验证码发送次数超过当日限制 Installation1$ npm install rongsms or 1$ yarn add rongsms Usage1234567891011121314151617const rongSms = require(&#x27;rongSms&#x27;);//生成验证码，在发送验证码是可以用来生成6位验证码。如果不是发送验证码可以不用。const code = rongSms.generate_code();//初始化//account_sid: 主账户sid，登陆云通讯网站后，可在控制台首页看到开发者主账号ACCOUNT SID//account_token： 主账户Token，登陆云通讯网站后，可在控制台首页看到开发者主账号AUTH TOKEN。//app_id: 请使用管理控制台中已创建应用的APPID。rongSms.init_sms(account_sid, account_token, app_id);//发送短信//phone:接受短信手机号//arr: 数组，对应短信模板中的替换内容。如短信模板为：您的验证码为&#123;1&#125;，请于&#123;2&#125;内正确输入，如非本人操作，请忽略此短信。arr取值为：[&#x27;123456&#x27;, &#x27;10分钟&#x27;]//template_id：模板idrongSms.send_sms(phone, arr, template_id);","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"},{"name":"npm","slug":"npm","permalink":"https://www.silenceboy.com/tags/npm/"},{"name":"sms","slug":"sms","permalink":"https://www.silenceboy.com/tags/sms/"}]},{"title":"搭建shadowsock服务器","slug":"搭建shadowsock服务器","date":"2018-01-19T01:37:28.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2018/01/19/搭建shadowsock服务器/","permalink":"https://www.silenceboy.com/2018/01/19/%E6%90%AD%E5%BB%BAshadowsock%E6%9C%8D%E5%8A%A1%E5%99%A8/","excerpt":"","text":"安装Debian &#x2F; Ubuntu: 12$ apt-get install python-pip$ pip install shadowsocks CentOS: 12$ yum install python-setuptools &amp;&amp; easy_install pip$ pip install shadowsocks 启动 有两种启动方式，建议使用配置文件的方式启动 直接启动： 1ssserver -p 8388 -k password -m rc4-md5 -d start 使用配置文件启动： 执行vim /etc/shadowsocks.json 添加如下内容： 123456789&#123; &quot;server&quot;:&quot;0.0.0.0&quot;, &quot;server_port&quot;:8388, &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;password&quot;:&quot;mypassword&quot;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;rc4-md5&quot;&#125; 多用户配置如下： 1234567891011121314&#123; &quot;server&quot;:&quot;0.0.0.0&quot;， &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;:1080, &quot;port_password&quot;: &#123; &quot;8388&quot;: &quot;password&quot;, &quot;8387&quot;: &quot;password&quot;, &quot;8386&quot;: &quot;password&quot;, &quot;8385&quot;: &quot;password&quot; &#125;, &quot;timeout&quot;:300, &quot;method&quot;:&quot;rc4-md5&quot;, &quot;fast_open&quot;: false &#125; 然后通过执行一下命令启动： 如果要停止运行，将命令中的start改成stop。 1$ ssserver -c /etc/shadowsocks.json -d start TIPS: 加密方式推荐使用rc4-md5，因为 RC4 比 AES 速度快好几倍，如果用在路由器上会带来显著性能提升。旧的 RC4 加密之所以不安全是因为 Shadowsocks 在每个连接上重复使用 key，没有使用 IV。现在已经重新正确实现，可以放心使用。更多可以看 issue。 开机自启编辑一下&#x2F;etc&#x2F;supervisord.conf文件，命令如下： 1$ vim /etc/supervisord.conf 把下面的内容粘贴到文件尾部的空行处，然后保存： 1234567[program:shadowsocks]command=ssserver -c /etc/shadowsocks.jsonautostart=trueautorestart=trueuser=rootlog_stderr=truelogfile=/var/log/shadowsocks.log 接下来需要编辑一下&#x2F;etc&#x2F;rc.local文件，请执行以下命令： 1$ vi /etc/rc.local 请把以下内容粘贴到文件中部的空白处，然后保存 1$ service supervisord start 完成以上步骤后，重启之后，shadowsock会自动运行。 问题 如果通过客户端始终代理失败，可以通过一下方法查找问题。 1.客户端通过 telnet ip port 确认 ss-server 是否正常开启，如果没有正常开启，有可能是设定的端口没有开放， 1$ iptables -A INPUT -p tcp --dport 8388 -j ACCEPT 执行上述命令，将 8388 修改为你设定的端口即可。 2.如果第一步中连接正常，可以查看下 ss-server 的日志 1$ ssserver -c /etc/shadowsocks.json --log-file /var/log/shadowsocks.log -d start 启动的时候添加 --log-file 参数，然后通过 1$ tail -f /var/log/shadowsocks.log 查看实时日志，一般可以看出一点端倪。","categories":[{"name":"shadowsock","slug":"shadowsock","permalink":"https://www.silenceboy.com/categories/shadowsock/"}],"tags":[{"name":"shadowsock","slug":"shadowsock","permalink":"https://www.silenceboy.com/tags/shadowsock/"}]},{"title":"搭建指定版本node环境的docker镜像","slug":"搭建指定版本node环境的docker镜像","date":"2017-12-07T09:36:12.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2017/12/07/搭建指定版本node环境的docker镜像/","permalink":"https://www.silenceboy.com/2017/12/07/%E6%90%AD%E5%BB%BA%E6%8C%87%E5%AE%9A%E7%89%88%E6%9C%ACnode%E7%8E%AF%E5%A2%83%E7%9A%84docker%E9%95%9C%E5%83%8F/","excerpt":"","text":"基于ubuntu16.04的docker镜像去打包安装了nodejs环境的docker镜像 前置条件1.获取ubuntu16.04镜像 1# docker pull ubuntu:16.04 2.基于ubuntu16.04镜像启动容器 1# docker run -ti --name ubuntu ubuntu:16.04 /bin/bash 从源代码安装Node.JS 安装node过程均在容器内进行 1.更新源并安装必要工具 12# apt-get update# apt-get install git wget python gcc make g++ 2.获取指定版本的node源代码 这里我们使用v8.9.0版,目前为长期支持版,可以使用两中获取源码的方式. 123# wget http://nodejs.org/dist/v8.9.0/node-v8.9.0.tar.gz# tar zxvf node-v8.9.0.tar.gz# mv node-v8.9.0 node or 1# git clone -b v8.9.0 git@github.com:nodejs/node.git 3.修改目录权限 1# chmod -R 755 node 4.编译安装node 1234# cd node# ./configure# make# make install 5.查看node版本 12# node --versionv8.9.0 安装完成后退出镜像 利用包管理器安装Node.JS 安装在镜像内进行 1.更新源并安装必要工具 setup_8.x为安装8.x版本,若安装9.x版本为:setup_9.x 123# apt-get update# apt-get install curl# curl -sL https://deb.nodesource.com/setup_8.x | bash - 2.安装nodejs 12# apt-get install -y nodejs 3.查看node版本 12# node --versionv8.9.0 安装完成后退出镜像 从容器创建一个新的镜像 注意: 在上一步已经退出容器,下面的操作是在本机上进行的. 1.执行 docker ps -a 查看name为ubuntu的ID 2.创建新的镜像 12$ docker commit -a &quot;author&quot; -m &quot;commit message&quot; b0084b239645 xxx/node8.9:v1sha256:bc03d86ef63bab18deafe643f99b2aa1da5697860e1432102dbbcbb281fdf335 -a: 作者信息 -m: 提交信息 b0084b239645: docker ps -a中查看的ID xxx&#x2F;node8.9:v1: 新的镜像名称 3.上传到镜像仓库 镜像制作完成可以将镜像上传到镜像仓库,便于以后使用,可以指定仓库地址,也可以使用官方的仓库. 1$ docker push xxx/node8.9:v1","categories":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/tags/docker/"},{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"}]},{"title":"使用Dockerfile部署nodejs服务","slug":"使用Dockerfile部署nodejs服务","date":"2017-12-04T09:49:49.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2017/12/04/使用Dockerfile部署nodejs服务/","permalink":"https://www.silenceboy.com/2017/12/04/%E4%BD%BF%E7%94%A8Dockerfile%E9%83%A8%E7%BD%B2nodejs%E6%9C%8D%E5%8A%A1/","excerpt":"","text":"初始化Dockerfile假设我们的项目名为express,在express项目中创建编辑Dockerfile文件： 123456789101112131415$ vim DockerfileFROM node:latestRUN mkdir -p /home/www/expressWORKDIR /home/www/expressCOPY . /home/www/expressRUN npm installEXPOSE 3000ENTRYPOINT [&quot;npm&quot;, &quot;run&quot;]CMD [&quot;start&quot;] 这个文件包含了以下命令： FROM node:latest - 指定使用最新版本的node基础镜像 RUN mkdir -p /home/www/express - 在容器内创建&#x2F;home&#x2F;www&#x2F;express目录 WORKDIR /home/www/express - 将容器内工作目录设置为&#x2F;home&#x2F;www&#x2F;express COPY . /home/www/express - 将宿主机当前目录下内容复制到镜像&#x2F;home&#x2F;www&#x2F;express目录下 RUN npm install - npm install安装应用所需的NPM包 EXPOSE 3000 - 对外开放容器的3000端口 ENTRYPOINT [&quot;npm&quot;, &quot;run&quot;] - 容器启动后执行的命令。不可被docker run提供的参数覆盖 CMD [&quot;start&quot;] - 在容器启动时，执行的命令，可被docker run提供的参数覆盖 构建镜像编写完Dockerfile文件后，就可以通过docker build命令来构建镜像： 1$ sudo docker build -t test/express . 我们通过-t参数，将镜像命名为test&#x2F;express。构建过程类似如下： 123456789101112131415161718192021222324252627282930313233343536Sending build context to Docker daemon 29.7 kBStep 1/8 : FROM registry.src.followme.com:5000/node:v1 ---&gt; c99c549e8227Step 2/8 : RUN mkdir -p /home/www/express-app ---&gt; Running in 8be9a90629b0 ---&gt; b9f584851225Removing intermediate container 8be9a90629b0Step 3/8 : WORKDIR /home/www/express-app ---&gt; 5072c31f9dd9Removing intermediate container e9dbf4ce3d8bStep 4/8 : COPY . /home/www/express-app ---&gt; a4d1725f15edRemoving intermediate container 30aa49765015Step 5/8 : RUN yarn ---&gt; Running in f181c243deaayarn install v1.3.2[1/4] Resolving packages...[2/4] Fetching packages...[3/4] Linking dependencies...[4/4] Building fresh packages...Done in 9.46s. ---&gt; d390931d73e6Removing intermediate container f181c243deaaStep 6/8 : EXPOSE 3000 ---&gt; Running in 94101ab38864 ---&gt; 43199a8a5a90Removing intermediate container 94101ab38864Step 7/8 : ENTRYPOINT npm run ---&gt; Running in 80b1318962cf ---&gt; 6b203c50e855Removing intermediate container 80b1318962cfStep 8/8 : CMD start ---&gt; Running in a9909e537f59 ---&gt; d56eae48377cRemoving intermediate container a9909e537f59Successfully built d56eae48377c 运行容器镜像构建完成后，可以通过所构建的镜像创建&#x2F;运行容器，从而实现express应用的 Docker 化部暑。 使用tets&#x2F;express镜像运行一个容器： 1$ sudo docker run -d --name experss-app -p 3000:3000 test/express 在以上操作中，我们通过test/express镜像运行了容器，并将容器命名为experss-app。运行容器，我们还指定了-d参数，该参数使容器以后台的方式运行。而-p参数将宿主机的3000端口映射到了容器的3000端口。运行容器后，可以通过docker ps命令看到运行中的容器。此时可通过localhost:3000访问服务。","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/tags/docker/"},{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"},{"name":"dockerfile","slug":"dockerfile","permalink":"https://www.silenceboy.com/tags/dockerfile/"}]},{"title":"JavaScript数组随机排序","slug":"JavaScript数组随机排序","date":"2017-08-08T04:21:39.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2017/08/08/JavaScript数组随机排序/","permalink":"https://www.silenceboy.com/2017/08/08/JavaScript%E6%95%B0%E7%BB%84%E9%9A%8F%E6%9C%BA%E6%8E%92%E5%BA%8F/","excerpt":"","text":"12345678910111213141516//不断从原数组中随机取一个元素放进新数组，同时删除原数组中该值，递归重复至全部取出。function randomSort(arr, newArr) &#123; var newArr = newArr || [] if (arr.length == 1) &#123; newArr.push(arr[0]) return newArr; // 相当于递归退出 &#125; var random = Math.ceil(Math.random() * arr.length) - 1 newArr.push(arr[random]) arr.splice(random, 1) return randomSort(arr, newArr)&#125;randomSort([1, 2, 3, 4, 5, 6, 7]); //[2, 3, 1, 5, 6, 7, 4]randomSort([1, 2, 3, 4, 5, 6, 7]); //[3, 4, 2, 5, 1, 6, 7]","categories":[{"name":"算法","slug":"算法","permalink":"https://www.silenceboy.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/tags/javascript/"},{"name":"算法","slug":"算法","permalink":"https://www.silenceboy.com/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"JavaScript按概率随机生成事件","slug":"JavaScript按概率随机生成事件","date":"2017-08-02T04:20:03.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2017/08/02/JavaScript按概率随机生成事件/","permalink":"https://www.silenceboy.com/2017/08/02/JavaScript%E6%8C%89%E6%A6%82%E7%8E%87%E9%9A%8F%E6%9C%BA%E7%94%9F%E6%88%90%E4%BA%8B%E4%BB%B6/","excerpt":"","text":"12345678910111213141516171819202122232425/**在抽奖的活动中经常会用到这个算法，不同奖项的获取概率不同，要按概率去随机生成对应的奖品**/function random(arr1, arr2) &#123; var sum = 0, factor = 0, random = Math.random(); for(var i = arr2.length - 1; i &gt;= 0; i--) &#123; sum += arr2[i]; // 统计概率总和 &#125;; random *= sum; // 生成概率随机数 for(var i = arr2.length - 1; i &gt;= 0; i--) &#123; factor += arr2[i]; if(random &lt;= factor) return arr1[i]; &#125;; return null;&#125;;// testvar a = [&#x27;mac&#x27;, &#x27;iphone&#x27;, &#x27;vivo&#x27;, &#x27;OPPO&#x27;];var b = [0.1, 0.2, 0.3, 0.4];console.log(random(a, b));","categories":[{"name":"算法","slug":"算法","permalink":"https://www.silenceboy.com/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/tags/javascript/"},{"name":"算法","slug":"算法","permalink":"https://www.silenceboy.com/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"node.js之base64编码解码","slug":"node-js之base64编码解码","date":"2016-09-16T04:15:18.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/09/16/node-js之base64编码解码/","permalink":"https://www.silenceboy.com/2016/09/16/node-js%E4%B9%8Bbase64%E7%BC%96%E7%A0%81%E8%A7%A3%E7%A0%81/","excerpt":"","text":"利用buffer来进行编解码： 123456&gt; var a = new Buffer(&#x27;key1=value1&amp;key2=value2&#x27;).toString(&#x27;base64&#x27;);undefined&gt; a&#x27;a2V5MT12YWx1ZTEma2V5Mj12YWx1ZTI=&#x27;&gt; new Buffer(a, &#x27;base64&#x27;).toString()&#x27;key1=value1&amp;key2=value2&#x27; 可以在终端中执行以下命令查看解码后的内容： 1echo a2V5MT12YWx1ZTEma2V5Mj12YWx1ZTI= | base64 -D","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"},{"name":"base64","slug":"base64","permalink":"https://www.silenceboy.com/tags/base64/"}]},{"title":"JavaScript数组函数","slug":"JavaScript数组函数","date":"2016-08-25T01:54:30.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/08/25/JavaScript数组函数/","permalink":"https://www.silenceboy.com/2016/08/25/JavaScript%E6%95%B0%E7%BB%84%E5%87%BD%E6%95%B0/","excerpt":"","text":"下面总结了一些JavaScript中常用的数组操作方法。验证是不是数组用 arr instanceof Array 或者Array.isArray(arr)如果是返回truepush()在数组末尾添加并返回数组长度pop()移除数组的最后一项并返回移除的项shift()移除数组中的第一项并返回移除的项unshift()在数组前端添加任意个项并返回新的数组长度reverse()反转数组​join()将数组中的 值合并成字符串默认用,分割，可以自定义如join(‘|’);sort()升序排列数组，调用每个数组项的toString()转型方法，然后比较得到的字符串，以确定如何排序。可以接受一个比较函数作为参数如果对数值进行排序可以通过如下方法： 123456789101112function compare(value1,value2)&#123; //升序 ​if(value1&lt;value2)&#123; ​ ​return -1; ​&#125;else if(value1 &gt; value2)&#123; ​ ​return 1; ​&#125;else&#123; ​ ​return 0; ​&#125;&#125;var values=[0,1,5,10,15];values.sort(compare);console.log(values);//0,1,5,10,15 如需降序排列升序后使用reverse()即可concat()基于当前数组中的所有项创建一个新数组，如果传入参数，会将参数添加到数组的末尾slice()基于当前数组中的一或多个项创建一个新数组，接受一或两个参数，即要返回项的起始和结束位置，在只有一个参数的情况下返回从该参数指定位置开始到当前数组末尾的所有项。如果有两个参数，该方法返回起始和结束位置之间的项，但不包括结束位置的项。 splice(): 删除：删除任意数量的项，只需指定两个参数：要删除的第一项的位置和要删除的项数。例如splice(0,2)会删除数组中的前两项。 插入：可以向指定位置插入任意数量的项，只需提供3个参数：起始位置、0（要删除的项数）和要插入的项。如果要插入多个项可以再传入第四、第五，任意多个项。 替换：可以向指定位置插入任意多个项，且同时删除任意数量的项，只需指定3个参数：其实位置、要删除的项数和要插入的任意数量的项。 indexOf()和lastIndexOf()：这两个方法都接收两个参数：要查找的项和（可选的）表示查找起点位置的索引。indexOf()从前向后查找，lastIndexOf()从后向前查找，没找到的情况下返回-1","categories":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/tags/javascript/"}]},{"title":"html5网页录音和语音识别","slug":"html5网页录音和语音识别","date":"2016-08-23T01:48:15.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/08/23/html5网页录音和语音识别/","permalink":"https://www.silenceboy.com/2016/08/23/html5%E7%BD%91%E9%A1%B5%E5%BD%95%E9%9F%B3%E5%92%8C%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/","excerpt":"","text":"#背景 在输入方式上，人们总是在追寻一种更高效，门槛更低的方式，来降低用户使用产品的学习成本。语音输入也是一种尝试较多的方式，有些直接使用语音（如微信语音聊天），有些需要将语音转化为文字（语音识别）。接下来的内容是一次在pc浏览器上进行语音识别输入的一种尝试。 ### 实现 调研阶段，chrome是支持语音识别的。它支持了一系列的接口，可以进行语音识别。参考HTML5的Speech API相关标准的现状但是使用这些接口有一些困难，连不上服务器。此路不通。 那么，可以使用笨点儿的方法，先录音再上传到指定语音识别服务器，进行语音识别。这里使用的是百度语音开放平台的语音识别接口，支持8k,16k的单声道的wav文件，或者pcm。尝试了8k的识别效果，跟16k的差了好远。就使用了16k,单声道wav文件，上传到语音识别服务器。 关于录音，主要步骤就是使用navigator.getUserMedia来获取用户的输入设备，成功之后使用webkitAudioContext来创建音频实例。在录音结束之后，将录音的流导出为文件，上传即可。录音的可以参考这个recorder.js，只要稍微做一下修改就可以应用。其中需要处理音频采样率，默认的采样率为44.1k,这里需要做一个转换，具体方法可以参考HTML5网页录音和压缩,边猜边做 在浏览器扩展中，没有明确的方式去获取用户对录音的授权。可以在扩展的optionpage里面申请授权，之后在扩展的所有页面都有权限了。在较新的chrome浏览器里测过可以用。参考这里：How do I give webkitGetUserMedia permission in a Chrome Extension popup window#demo 这里有一个chrome扩展的demo，实现了通过语音采样，生成wav文件上传到语音识别服务器的功能。其中做了一个比较简单的端点检测，通过音量的大小来确定输入的完成。http://github.com/veizz/speech_io一些思考 * demo其实是用来参与公司举办的一届hackathon比赛，主要实现了语音在线识别，文字播报等功能。在后期还有想法加入了一些自然语音处理的功能，可以识别一些输入指令。如『打开百度首页』、『上淘宝买衣服』等功能。会打开指定网站，自动填写输入词，执行搜索。还可以做一些小功能，比如说语音输入『查询天气』、『买电影票』等常用功能，在popup的窗口里面打开等。一切的想法都看起来很美好，但在大家都熟悉了打字输入的今天，还有多少人愿意使用语音识别做为输入方式？而对于不会打字的人，能否使用标准的普通话来进行语音识别的输入？ * 采样率的处理是通过js的文件操作来实现的。html5支持的fileapi强大如此，怪不得有人用js做视频解码器，不考虑性能的话，看起来很美好啊 #参考 http://www.cnblogs.com/jz1108/archive/2012/05/21/2511447.htmlhttps://dvcs.w3.org/hg/speech-api/raw-file/tip/speechapi.htmlhttp://codeartists.com/post/36746402258/how-to-record-audio-in-chrome-with-native-html5-apishttp://stackoverflow.com/questions/13076272/how-do-i-give-webkitgetusermedia-permission-in-a-chrome-extension-popup-windowhttp://www.cnblogs.com/blqw/p/3782420.htmlhttp://ibillxia.github.io/blog/2013/05/22/audio-signal-processing-time-domain-Voice-Activity-Detection/http://stackoverflow.com/questions/13333378/how-can-javascript-upload-a-blobhttp://www.web-tinker.com/article/20498.html","categories":[{"name":"html5","slug":"html5","permalink":"https://www.silenceboy.com/categories/html5/"}],"tags":[{"name":"html5","slug":"html5","permalink":"https://www.silenceboy.com/tags/html5/"},{"name":"录音","slug":"录音","permalink":"https://www.silenceboy.com/tags/%E5%BD%95%E9%9F%B3/"},{"name":"语言识别","slug":"语言识别","permalink":"https://www.silenceboy.com/tags/%E8%AF%AD%E8%A8%80%E8%AF%86%E5%88%AB/"}]},{"title":"nodejs通过later实现定时执行任务","slug":"nodejs通过later实现定时执行任务","date":"2016-08-08T01:45:57.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/08/08/nodejs通过later实现定时执行任务/","permalink":"https://www.silenceboy.com/2016/08/08/nodejs%E9%80%9A%E8%BF%87later%E5%AE%9E%E7%8E%B0%E5%AE%9A%E6%97%B6%E6%89%A7%E8%A1%8C%E4%BB%BB%E5%8A%A1/","excerpt":"","text":"大多数情况我们都选用使用Linux的cron来控制定时执行的任务。当我们要维护多台计算机，几十个，几百个定时任务的时候，用cron会带来非常大的运维成本。可能写到程序中，就是一个不错的选择了。nodejs有一个later的插件可以简单实现该功能。如果已经安装过npm，可以直接执行npm install later安装该插件。如果没有请先安装npm。 12345678910111213141516var later = require(&#x27;later&#x27;);var basic = &#123;h:[00],m:[00]&#125;; //设置每天凌晨执行var composite=[ basic];var sched=&#123; schedules:composite&#125;;later.date.localTime(); //设置本地时区//var occurrences = later.schedule(sched).next(10);//for(var i=0;i&lt;10;i++)&#123;// console.log(occurrences[i]);//&#125;var t=later.setInterval(function()&#123; console.log(&quot;asdasd&quot;);&#125;,sched); 可根据自己的需求进行更改。","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"}]},{"title":"MySQL查询表内重复记录","slug":"MySQL查询表内重复记录","date":"2016-08-04T02:02:52.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/08/04/MySQL查询表内重复记录/","permalink":"https://www.silenceboy.com/2016/08/04/MySQL%E6%9F%A5%E8%AF%A2%E8%A1%A8%E5%86%85%E9%87%8D%E5%A4%8D%E8%AE%B0%E5%BD%95/","excerpt":"","text":"1、查找表中多余的重复记录，重复记录是根据单个字段（user_id）来判断 1select * from tbl_user where user_id in (select user_id from people group by user_id having count(user_id) &gt; 1) 2、删除表中多余的重复记录，重复记录是根据单个字段（user_id）来判断，只留有一个记录 1delete from tbl_user where user_id in (select user_id from people group by user_id having count(user_id) &gt; 1) and min(id) not in (select id from people group by user_id having count(user_id)&gt;1) 3、查找表中多余的重复记录（多个字段） 1select * from table where (user_id,lesson_id) in (select user_id,lesson_id from table group by user_id,lesson_id having count(*) &gt; 1) 4、删除表中多余的重复记录（多个字段），只留有id最小的记录 1delete from table where (user_id,lesson_id) in (select user_id,lesson_id from table group by user_id,lesson_id having count(*) &gt; 1) and id not in (select min(id) from table group by user_id,lesson_id having count(*)&gt;1) 5、查找表中多余的重复记录（多个字段），不包含id最小的记录 1select * from table where (user_id,lesson_id) in (select user_id,lesson_id from table group by user_id,lesson_id having count(*) &gt; 1) and id not in (select min(id) from table group by user_id,lesson_id having count(*)&gt;1)","categories":[{"name":"mysql","slug":"mysql","permalink":"https://www.silenceboy.com/categories/mysql/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://www.silenceboy.com/tags/mysql/"}]},{"title":"服务器时区问题","slug":"服务器时区问题","date":"2016-08-04T01:42:46.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/08/04/服务器时区问题/","permalink":"https://www.silenceboy.com/2016/08/04/%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%97%B6%E5%8C%BA%E9%97%AE%E9%A2%98/","excerpt":"","text":"进入测试发现对应的时间不对，查了一下服务器的时间，发现服务器的时区为世界标准时间，简称UTC不属于任意时区，UTC时区默认比北京时间少8个小时。查看服务器时间： 这是服务器上的时区。中国的时区应该为CST。那么如何修改服务器时区为中国时区呐，很简单。 1cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 执行该命令，之后再查看时间： 现在的时间即为正确的中国时间。","categories":[{"name":"linux","slug":"linux","permalink":"https://www.silenceboy.com/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://www.silenceboy.com/tags/linux/"}]},{"title":"JavaScript获取时间戳与时间戳转化","slug":"JavaScript获取时间戳与时间戳转化","date":"2016-08-03T01:57:19.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/08/03/JavaScript获取时间戳与时间戳转化/","permalink":"https://www.silenceboy.com/2016/08/03/JavaScript%E8%8E%B7%E5%8F%96%E6%97%B6%E9%97%B4%E6%88%B3%E4%B8%8E%E6%97%B6%E9%97%B4%E6%88%B3%E8%BD%AC%E5%8C%96/","excerpt":"","text":"Javascript 获取当前时间戳（毫秒级别）： 第一种方法： 1var timestamp1 = Date.parse( new Date()); 结果：1470220594000 第二种方法： 1var timestamp2 = ( new Date()).valueOf(); 结果：1470220608533 第三种方法： 1var timestamp3 = new Date().getTime(); 结果：1470220608533 第一种获取的时间戳是精确到秒，第二种和第三种是获取的时间戳精确到毫秒。 获取指定时间的时间戳：1new Date(&quot;2016-08-03 00:00:00&quot;).getTime; 时间戳转化成时间：12345678910function timetrans(date)&#123; var date = new Date(date*1000);//如果date为13位不需要乘1000 var Y = date.getFullYear() + &#x27;-&#x27;; var M = (date.getMonth()+1 &lt; 10 ? &#x27;0&#x27;+(date.getMonth()+1) : date.getMonth()+1) + &#x27;-&#x27;; var D = (date.getDate() &lt; 10 ? &#x27;0&#x27; + (date.getDate()) : date.getDate()) + &#x27; &#x27;; var h = (date.getHours() &lt; 10 ? &#x27;0&#x27; + date.getHours() : date.getHours()) + &#x27;:&#x27;; var m = (date.getMinutes() &lt;10 ? &#x27;0&#x27; + date.getMinutes() : date.getMinutes()) + &#x27;:&#x27;; var s = (date.getSeconds() &lt;10 ? &#x27;0&#x27; + date.getSeconds() : date.getSeconds()); return Y+M+D+h+m+s;&#125;","categories":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/tags/javascript/"}]},{"title":"js利用clipboardData在网页中实现截屏粘贴的功能","slug":"js利用clipboardData在网页中实现截屏粘贴的功能","date":"2016-07-23T01:54:44.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/07/23/js利用clipboardData在网页中实现截屏粘贴的功能/","permalink":"https://www.silenceboy.com/2016/07/23/js%E5%88%A9%E7%94%A8clipboardData%E5%9C%A8%E7%BD%91%E9%A1%B5%E4%B8%AD%E5%AE%9E%E7%8E%B0%E6%88%AA%E5%B1%8F%E7%B2%98%E8%B4%B4%E7%9A%84%E5%8A%9F%E8%83%BD/","excerpt":"","text":"最近在做一个将屏幕截图直接粘贴发送的功能，于是对此做了一些研究，下面是具体的实现代码：html代码如下，在这里只是简单的做了一个textare框用作演示 123456789101112&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width&quot;&gt; &lt;title&gt;截屏粘贴&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;textarea onpaste =&quot;paste()&quot;&gt; &lt;/textarea&gt;&lt;/body&gt;&lt;/html&gt; 具体实现在JavaScript中： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546function paste(event)&#123; var clipboardData = event.clipboardData; console.log(clipboardData); var items,item,types; if( clipboardData )&#123; items = clipboardData.items; if( !items )&#123; return; &#125; // 保存在剪贴板中的数据类型 types = clipboardData.types || []; for(var i=0 ; i &lt; types.length; i++ )&#123; if( types[i] === &#x27;Files&#x27; )&#123; item = items[i]; break; &#125; &#125; // 判断是否为图片数据 if( item &amp;&amp; item.kind === &#x27;file&#x27; &amp;&amp; item.type.match(/^image\\//i) )&#123; // 读取该图片 var file = item.getAsFile(), reader = new FileReader(); reader.readAsDataURL(file); console.log(reader); //下面是讲粘贴的图片内容传送到后端进行处理，如果直接前端处理可以不要后边的代码 var xhr = new XMLHttpRequest(); xhr.open(&#x27;post&#x27;, &#x27;/pasteImage&#x27;,true); xhr.setRequestHeader(&#x27;Content-Type&#x27;, &#x27;application/json&#x27;); reader.onload = function()&#123; console.log(reader.result); xhr.send(JSON.stringify(&#123; file: reader.result &#125;)); &#125;; //接收返回数据 xhr.onload = function()&#123; var response = JSON.parse(xhr.responseText); if(response.code == 200)&#123; // &#125;else&#123; // &#125; &#125; &#125; &#125;&#125;","categories":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/tags/javascript/"}]},{"title":"JSON.stringify 函数参数分析","slug":"JSON-stringify-函数参数分析","date":"2016-07-16T01:50:57.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/07/16/JSON-stringify-函数参数分析/","permalink":"https://www.silenceboy.com/2016/07/16/JSON-stringify-%E5%87%BD%E6%95%B0%E5%8F%82%E6%95%B0%E5%88%86%E6%9E%90/","excerpt":"","text":"JSON.stringify是将 JavaScript 值转换为 JavaScript 对象表示法 。语法为：JSON.stringify(value [, replacer] [, space])很多人都只会用到第一个参数，所以导致很多人不知道后两个参数是什么意思，下面对三个参数进行分析： value必需。 要转换的 JavaScript 值（通常为对象或数组）。 replacer可选。 用于转换结果的函数或数组。如果 replacer 为函数，则 JSON.stringify 将调用该函数，并传入每个成员的键和值。 使用返回值而不是原始值。 如果此函数返回 undefined，则排除成员。 根对象的键是一个空字符串：””。如果 replacer 是一个数组，则仅转换该数组中具有键值的成员。 成员的转换顺序与键在数组中的顺序一样。 当 value 参数也为数组时，将忽略 replacer 数组。 space可选。 向返回值 JSON 文本添加缩进、空格和换行符以使其更易于读取。如果省略 space，则将生成返回值文本，而没有任何额外空格。如果 space 是一个数字，则返回值文本在每个级别缩进指定数目的空格。 如果 space 大于 10，则文本缩进 10 个空格。如果 space 是一个非空字符串（例如“t”），则返回值文本在每个级别中缩进字符串中的字符。如果 space 是长度大于 10 个字符的字符串，则使用前 10 个字符。","categories":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/tags/javascript/"}]},{"title":"nodejs读写excel内容","slug":"nodejs读写excel内容","date":"2016-07-16T01:47:27.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/07/16/nodejs读写excel内容/","permalink":"https://www.silenceboy.com/2016/07/16/nodejs%E8%AF%BB%E5%86%99excel%E5%86%85%E5%AE%B9/","excerpt":"","text":"支持读写Excel的node.js模块 node-xlsx: 基于Node.js解析excel文件数据及生成excel文件，仅支持xlsx格式文件； excel-parser: 基于Node.js解析excel文件数据，支持xls及xlsx格式文件； excel-export : 基于Node.js将数据生成导出excel文件，生成文件格式为xlsx； node-xlrd: 基于node.js从excel文件中提取数据，仅支持xls格式文件。 我将展示通过node-xlsx提取上传上来的excel文件里的数据，以及生成新的excel文件。代码如下： 12345678910111213141516171819202122232425var xlsx = require(&#x27;node-xlsx&#x27;);var fs = require(&#x27;fs&#x27;);//读取文件内容var obj = xlsx.parse(__dirname+&#x27;/test.xlsx&#x27;);var excelObj=obj[0].data;console.log(excelObj);var data = [];for(var i in excelObj)&#123; var arr=[]; var value=excelObj[i]; for(var j in value)&#123; arr.push(value[j]); &#125; data.push(arr);&#125;var buffer = xlsx.build([ &#123; name:&#x27;sheet1&#x27;, data:data &#125; ]);//将文件内容插入新的文件中fs.writeFileSync(&#x27;test1.xlsx&#x27;,buffer,&#123;&#x27;flag&#x27;:&#x27;w&#x27;&#125;);","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"},{"name":"excel","slug":"excel","permalink":"https://www.silenceboy.com/tags/excel/"}]},{"title":"web聊天系统的消息通知问题","slug":"web聊天系统的消息通知问题","date":"2016-07-06T06:42:58.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/07/06/web聊天系统的消息通知问题/","permalink":"https://www.silenceboy.com/2016/07/06/web%E8%81%8A%E5%A4%A9%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%B6%88%E6%81%AF%E9%80%9A%E7%9F%A5%E9%97%AE%E9%A2%98/","excerpt":"","text":"web消息提示无非三种方式：声音提示，桌面弹窗和title闪烁提醒。下面做一一介绍。 声音提示注意声音提示前提示已经加载了声音文件，有文章写的很多是临时create一个audio对象，然后audio.src,这样做是非常不好的，因为你每次调用声音的时候都会去后台请求一下这个声音文件。所以先加载出来是最好的方法。 &lt;audio id=&quot;chat-audio&quot; src=&quot;audio/system.wav&quot; display=&quot;none&quot;&gt;&lt;/audio&gt; function playAudio() &#123; document.getElementById(&#39;chat-audio&#39;).play(); //pause()方法也可以暂停，具体可查html5的audio标签 &#125; //调用方式 playAudio(); 桌面弹窗function palyDeskNotice(theTitle, options) &#123; if (Notification.permission !== &quot;granted&quot;) &#123; //先判断一下用户是否已经开启了桌面提示的权限，如果没有则提醒用户开启 window.Notification.requestPermission(function(permission) &#123; if (permission === &quot;granted&quot;) showNotice(theTitle, options); &#125;); &#125; else &#123; showNotice(theTitle, options); &#125; &#125; function showNotice(theTitle, options) &#123; //这个就是桌面弹窗 var desknotice = new Notification(theTitle, options); desknotice.onclick = function() &#123; //当用户点击弹窗的时候，要定位到聊天窗口 window.focus(); desknotice.close(); &#125;; //页面退出时关闭提醒 window.onbeforeunload = function() &#123; desknotice.close(); &#125; //弹窗3秒后自动消失 setTimeout(desknotice.close.bind(desknotice), 3000); &#125; //调用方式 palyDeskNotice(&#39;来自xxx&#39;, &#123; body: &#39;内容&#39;, icon: &quot;images/xxx.jpg&quot; &#125;); title闪烁提醒的原理var NewMsgNoticeflag = false,//闪烁标识 newMsgNotinceTimer = null; function newMsgCount() &#123; if (NewMsgNoticeflag) &#123; NewMsgNoticeflag = false; document.title = &#39;【☏新消息】您有新的即时消息&#39;; &#125; else &#123; NewMsgNoticeflag = true; document.title = &#39;【 】您有新的即时消息&#39;; &#125; &#125; //兼容性 var hiddenProperty = &#39;hidden&#39; in document ? &#39;hidden&#39; : &#39;webkitHidden&#39; in document ? &#39;webkitHidden&#39; : &#39;mozHidden&#39; in document ? &#39;mozHidden&#39; : null; var visibilityChangeEvent = hiddenProperty.replace(/hidden/i, &#39;visibilitychange&#39;); var onVisibilityChange = function() &#123; if (!document[hiddenProperty]) &#123; clearInterval(newMsgNotinceTimer); newMsgNotinceTimer = null; document.title = &#39;beta-即时消息系统&#39;; //窗口没有消息的时候默认的title内容 &#125; &#125; document.addEventListener(visibilityChangeEvent, onVisibilityChange); //调用方式 if (!newMsgNotinceTimer) newMsgNotinceTimer = setInterval(&quot;newMsgCount()&quot;, 200);","categories":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/tags/javascript/"}]},{"title":"JavaScript让时间显示为多久以前","slug":"JavaScript让时间显示为多久以前","date":"2016-06-02T06:41:28.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/06/02/JavaScript让时间显示为多久以前/","permalink":"https://www.silenceboy.com/2016/06/02/JavaScript%E8%AE%A9%E6%97%B6%E9%97%B4%E6%98%BE%E7%A4%BA%E4%B8%BA%E5%A4%9A%E4%B9%85%E4%BB%A5%E5%89%8D/","excerpt":"","text":"在做论坛的功能时，要求帖子的发帖时间显示几秒前，几分钟前，几小时前。。。这种功能，于是就把获取到的发帖时间做了如下处理： 12345678910111213141516171819function gettime(createtime)&#123; var now=Date.parse(new Date())/1000; var limit=now-createtime; var content=&quot;&quot;; if(limit&lt;60)&#123; content=&quot;刚刚&quot;; &#125;else if(limit&gt;=60 &amp;&amp; limit&lt;3600)&#123; content=Math.floor(limit/60)+&quot;分钟前&quot;; &#125;else if(limit&gt;=3600 &amp;&amp; limit&lt;86400)&#123; content=Math.floor(limit/3600)+&quot;小时前&quot;; &#125;else if(limit&gt;=86400 &amp;&amp; limit&lt;2592000)&#123; content=Math.floor(limit/86400)+&quot;天前&quot;; &#125;else if(limit&gt;=2592000 &amp;&amp; limit&lt;31104000)&#123; content=Math.floor(limit/2592000)+&quot;个月前&quot;; &#125;else&#123; content=&quot;很久前&quot;; &#125; return content;&#125;","categories":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/tags/javascript/"}]},{"title":"express如何解决request entity too large问题","slug":"express如何解决request-entity-too-large问题","date":"2016-05-30T03:21:33.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/05/30/express如何解决request-entity-too-large问题/","permalink":"https://www.silenceboy.com/2016/05/30/express%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3request-entity-too-large%E9%97%AE%E9%A2%98/","excerpt":"","text":"通过js向后台post一些文件信息时，会出现如下图所示的错误。这是express框架的问题，默认的很小，可以通过设置：app.use(express.json({limit: ‘5mb’}));解决该问题。","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"}]},{"title":"Nodejs接收图片base64格式保存为文件","slug":"Nodejs接收图片base64格式保存为文件","date":"2016-05-27T03:19:30.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/05/27/Nodejs接收图片base64格式保存为文件/","permalink":"https://www.silenceboy.com/2016/05/27/Nodejs%E6%8E%A5%E6%94%B6%E5%9B%BE%E7%89%87base64%E6%A0%BC%E5%BC%8F%E4%BF%9D%E5%AD%98%E4%B8%BA%E6%96%87%E4%BB%B6/","excerpt":"","text":"base64的形式为“data:image&#x2F;png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0。。。。”；当接收到上边的内容后，需要将data:image&#x2F;png;base64,这段内容过滤掉，过滤成：“iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0。。。”；然后进行保存。 1234567891011121314app.post(&#x27;/upload&#x27;, function(req, res)&#123; //接收前台POST过来的base64 var imgData = req.body.imgData; //过滤data:URL var base64Data = imgData.replace(/^data:image\\/\\w+;base64,/, &quot;&quot;); var dataBuffer = new Buffer(base64Data, &#x27;base64&#x27;); fs.writeFile(&quot;image.png&quot;, dataBuffer, function(err) &#123; if(err)&#123; res.send(err); &#125;else&#123; res.send(&quot;保存成功！&quot;); &#125; &#125;);&#125;);","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"},{"name":"base64","slug":"base64","permalink":"https://www.silenceboy.com/tags/base64/"}]},{"title":"javascript在网页中实现粘贴qq截图功能","slug":"javascript在网页中实现粘贴qq截图功能","date":"2016-05-27T01:41:34.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/05/27/javascript在网页中实现粘贴qq截图功能/","permalink":"https://www.silenceboy.com/2016/05/27/javascript%E5%9C%A8%E7%BD%91%E9%A1%B5%E4%B8%AD%E5%AE%9E%E7%8E%B0%E7%B2%98%E8%B4%B4qq%E6%88%AA%E5%9B%BE%E5%8A%9F%E8%83%BD/","excerpt":"","text":"这篇文章主要介绍了在网页中实现读取剪贴板粘贴截图功能,即可以把剪贴板的截图Ctrl+V粘贴到网页的一个输入框中,例如QQ截图、旺旺截图或者其它截图软件。具体代码如下。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;!DOCTYPE HTML&gt;&lt;html lang=&quot;en-US&quot;&gt;&lt;head&gt;&lt;meta charset=&quot;UTF-8&quot;&gt;&lt;title&gt;利用 clipboardData 在网页中实现截屏粘贴的功能&lt;/title&gt;&lt;style type=&quot;text/css&quot;&gt;#box&#123; width:200px; height:200px; border:1px solid #ddd; &#125;&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;利用 clipboardData 在网页中实现截屏粘贴的功能&lt;/h1&gt; &lt;hr /&gt;&lt;div&gt;&lt;input type=&quot;text&quot; id=&quot;testInput&quot; placeholder=&quot;截屏后粘贴到输入框中&quot; size=&quot;30&quot; /&gt;&lt;/div&gt;&lt;script type=&quot;text/javascript&quot;&gt;(function()&#123; var imgReader = function( item )&#123; var blob = item.getAsFile(), reader = new FileReader(); // 读取文件后将其显示在网页中 reader.onload = function( e )&#123; var img = new Image(); img.src = e.target.result; document.body.appendChild( img ); &#125;; // 读取文件 reader.readAsDataURL( blob ); &#125;; document.getElementById( &#x27;testInput&#x27; ).addEventListener( &#x27;paste&#x27;, function( e )&#123; // 添加到事件对象中的访问系统剪贴板的接口 var clipboardData = e.clipboardData, i = 0, items, item, types; if( clipboardData )&#123; items = clipboardData.items; if( !items )&#123; return; &#125; item = items[0]; // 保存在剪贴板中的数据类型 types = clipboardData.types || []; for( ; i &lt; types.length; i++ )&#123; if( types[i] === &#x27;Files&#x27; )&#123; item = items[i]; break; &#125; &#125; // 判断是否为图片数据 if( item &amp;&amp; item.kind === &#x27;file&#x27; &amp;&amp; item.type.match(/^image\\//i) )&#123; imgReader( item ); &#125; &#125; &#125;);&#125;)(); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;","categories":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/tags/javascript/"}]},{"title":"js获取内容中的url链接，并设置a标签","slug":"js获取内容中的url链接，并设置a标签","date":"2016-05-24T01:39:27.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/05/24/js获取内容中的url链接，并设置a标签/","permalink":"https://www.silenceboy.com/2016/05/24/js%E8%8E%B7%E5%8F%96%E5%86%85%E5%AE%B9%E4%B8%AD%E7%9A%84url%E9%93%BE%E6%8E%A5%EF%BC%8C%E5%B9%B6%E8%AE%BE%E7%BD%AEa%E6%A0%87%E7%AD%BE/","excerpt":"","text":"12345var regexp = /(http:\\/\\/|https:\\/\\/)((\\w|=|\\?|\\.|\\/|\\&amp;|-)+)/g;content = content.replace(regexp, function($url)&#123; return &quot;&lt;a href=&#x27;&quot; + $url + &quot;&#x27; target=&#x27;_blank&#x27;&gt;&quot; + $url + &quot;&lt;/a&gt;&quot;;&#125;);console.log(content);","categories":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/categories/javascript/"}],"tags":[{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/tags/javascript/"},{"name":"正则表达式","slug":"正则表达式","permalink":"https://www.silenceboy.com/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"}]},{"title":"vi/vim多行注释和取消注释","slug":"vi-vim多行注释和取消注释","date":"2016-05-21T01:37:05.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/05/21/vi-vim多行注释和取消注释/","permalink":"https://www.silenceboy.com/2016/05/21/vi-vim%E5%A4%9A%E8%A1%8C%E6%B3%A8%E9%87%8A%E5%92%8C%E5%8F%96%E6%B6%88%E6%B3%A8%E9%87%8A/","excerpt":"","text":"多行注释： 进入命令行模式，按ctrl + v进入 visual block模式（可视快模式），然后按j, 或者k选中多行，把需要注释的行标记起来 按大写字母I，再插入注释符，例如&#x2F;&#x2F; 按esc键就会全部注释了（我的是按两下） 取消多行注释： 进入命令行模式，按ctrl + v进入 visual block模式（可视快模式），按小写字母l横向选中列的个数，例如 &#x2F;&#x2F; 需要选中2列 按字母j，或者k选中注释符号 按d键就可全部取消注释","categories":[{"name":"vim","slug":"vim","permalink":"https://www.silenceboy.com/categories/vim/"}],"tags":[{"name":"vim","slug":"vim","permalink":"https://www.silenceboy.com/tags/vim/"},{"name":"vi","slug":"vi","permalink":"https://www.silenceboy.com/tags/vi/"}]},{"title":"nodejs如何获取form表单post方法提交的数据","slug":"nodejs如何获取form表单post方法提交的数据","date":"2016-05-18T05:41:40.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/05/18/nodejs如何获取form表单post方法提交的数据/","permalink":"https://www.silenceboy.com/2016/05/18/nodejs%E5%A6%82%E4%BD%95%E8%8E%B7%E5%8F%96form%E8%A1%A8%E5%8D%95post%E6%96%B9%E6%B3%95%E6%8F%90%E4%BA%A4%E7%9A%84%E6%95%B0%E6%8D%AE/","excerpt":"","text":"首先通过：npm install formidable安装所需的包。下面给出一个测试的例子： 12345678910111213141516171819202122232425262728var formidable = require(&#x27;formidable&#x27;), http = require(&#x27;http&#x27;), util = require(&#x27;util&#x27;);http.createServer(function(req, res) &#123; if (req.url == &#x27;/upload&#x27; &amp;&amp; req.method.toLowerCase() == &#x27;post&#x27;) &#123; // parse a file upload var form = new formidable.IncomingForm(); form.parse(req, function(err, fields, files) &#123; res.writeHead(200, &#123;&#x27;content-type&#x27;: &#x27;text/plain&#x27;&#125;); res.write(&#x27;received upload:\\n\\n&#x27;); res.end(util.inspect(&#123;fields: fields, files: files&#125;)); &#125;); return; &#125; // show a file upload form res.writeHead(200, &#123;&#x27;content-type&#x27;: &#x27;text/html&#x27;&#125;); res.end( &#x27;&lt;form action=&quot;/upload&quot; enctype=&quot;multipart/form-data&quot; method=&quot;post&quot;&gt;&#x27;+ &#x27;&lt;input type=&quot;text&quot; name=&quot;title&quot;&gt;&lt;br&gt;&#x27;+ &#x27;&lt;input type=&quot;file&quot; name=&quot;upload&quot; multiple=&quot;multiple&quot;&gt;&lt;br&gt;&#x27;+ &#x27;&lt;input type=&quot;submit&quot; value=&quot;Upload&quot;&gt;&#x27;+ &#x27;&lt;/form&gt;&#x27; );&#125;).listen(8080); 通过formidable可以成功获取表单 提交的内容。","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"}]},{"title":"连接Buffer对象的正确方法","slug":"连接Buffer对象的正确方法","date":"2016-04-12T05:39:41.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/04/12/连接Buffer对象的正确方法/","permalink":"https://www.silenceboy.com/2016/04/12/%E8%BF%9E%E6%8E%A5Buffer%E5%AF%B9%E8%B1%A1%E7%9A%84%E6%AD%A3%E7%A1%AE%E6%96%B9%E6%B3%95/","excerpt":"","text":"1234567891011121314151617181920212223var buffers = [];var nread = 0;readStream.on(&#x27;data&#x27;, function (chunk) &#123; buffers.push(chunk); nread += chunk.length;&#125;);readStream.on(&#x27;end&#x27;, function () &#123; var buffer = null; switch(buffers.length) &#123; case 0: buffer = new Buffer(0); break; case 1: buffer = buffers[0]; break; default: buffer = new Buffer(nread); for (var i = 0, pos = 0, l = buffers.length; i &lt; l; i++) &#123; var chunk = buffers[i]; chunk.copy(buffer, pos); pos += chunk.length; &#125; break; &#125;&#125;);","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"},{"name":"buffer","slug":"buffer","permalink":"https://www.silenceboy.com/tags/buffer/"}]},{"title":"html5 实现网页截屏 页面生成图片","slug":"html5-实现网页截屏-页面生成图片","date":"2016-03-21T05:37:41.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/03/21/html5-实现网页截屏-页面生成图片/","permalink":"https://www.silenceboy.com/2016/03/21/html5-%E5%AE%9E%E7%8E%B0%E7%BD%91%E9%A1%B5%E6%88%AA%E5%B1%8F-%E9%A1%B5%E9%9D%A2%E7%94%9F%E6%88%90%E5%9B%BE%E7%89%87/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142&lt;!DOCTYPE html&gt;&lt;html&gt; &lt;head&gt; &lt;meta name=&quot;layout&quot; content=&quot;main&quot;&gt; &lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; src=&quot;http://html2canvas.hertzen.com/build/html2canvas.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot; &gt; $(document).ready( function()&#123; $(&quot;.example1&quot;).on(&quot;click&quot;, function(event) &#123; event.preventDefault(); html2canvas(document.body, &#123; allowTaint: true, taintTest: false, onrendered: function(canvas) &#123; canvas.id = &quot;mycanvas&quot;; //document.body.appendChild(canvas); //生成base64图片数据 var dataUrl = canvas.toDataURL(); var newImg = document.createElement(&quot;img&quot;); newImg.src = dataUrl; document.body.appendChild(newImg); &#125; &#125;); &#125;); &#125;); &lt;/script&gt; &lt;/head&gt; &lt;body&gt; Hello! &lt;div class=&quot;&quot; style=&quot;background-color: #abc;&quot;&gt; html5页面截图 &lt;/div&gt; &lt;textArea id=&quot;textArea&quot; col=&quot;20&quot; rows=&quot;10&quot; &gt;&lt;/textArea&gt; &lt;input class=&quot;example1&quot; type=&quot;button&quot; value=&quot;截图&quot;&gt; 生成界面如下： &lt;/body&gt;&lt;/html&gt;","categories":[{"name":"html5","slug":"html5","permalink":"https://www.silenceboy.com/categories/html5/"}],"tags":[{"name":"html5","slug":"html5","permalink":"https://www.silenceboy.com/tags/html5/"}]},{"title":"php根据IP地址获取地理位置","slug":"php根据IP地址获取地理位置","date":"2016-01-24T14:35:15.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/01/24/php根据IP地址获取地理位置/","permalink":"https://www.silenceboy.com/2016/01/24/php%E6%A0%B9%E6%8D%AEIP%E5%9C%B0%E5%9D%80%E8%8E%B7%E5%8F%96%E5%9C%B0%E7%90%86%E4%BD%8D%E7%BD%AE/","excerpt":"","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&lt;?phpheader(&quot;Content-type: text/html; charset=utf-8&quot;); //获取IP地址的方法function getIP()&#123; if (isset($_SERVER)) &#123; if (isset($_SERVER[&#x27;HTTP_X_FORWARDED_FOR&#x27;])) &#123; $realip = $_SERVER[&#x27;HTTP_X_FORWARDED_FOR&#x27;]; &#125; elseif (isset($_SERVER[&#x27;HTTP_CLIENT_IP&#x27;])) &#123; $realip = $_SERVER[&#x27;HTTP_CLIENT_IP&#x27;]; &#125; else &#123; $realip = $_SERVER[&#x27;REMOTE_ADDR&#x27;]; &#125; &#125; else &#123; if (getenv(&quot;HTTP_X_FORWARDED_FOR&quot;)) &#123; $realip = getenv( &quot;HTTP_X_FORWARDED_FOR&quot;); &#125; elseif (getenv(&quot;HTTP_CLIENT_IP&quot;)) &#123; $realip = getenv(&quot;HTTP_CLIENT_IP&quot;); &#125; else &#123; $realip = getenv(&quot;REMOTE_ADDR&quot;); &#125; &#125; return $realip;&#125;echo $ip = getIP();//通过php的file_get_contents()方法获取地理位置//新浪接口根据ip查询所在区域信息$res0 = file_get_contents(&quot;http://int.dpool.sina.com.cn/iplookup/iplookup.php?format=json&amp;ip=$ip&quot;);$res0 = json_decode($res0,true);print_r($res0);echo &quot;&lt;br/&gt;&quot;;//淘宝接口根据ip查询所在区域信息$res1 = file_get_contents(&quot;http://ip.taobao.com/service/getIpInfo.php?ip=$ip&quot;);$res1 = json_decode($res1,true);print_r($res1);echo &quot;&lt;br/&gt;&quot;;//通过php的curl获取地理位置//新浪根据IP获取地理位置API$url = &#x27;http://int.dpool.sina.com.cn/iplookup/iplookup.php?format=json&amp;ip=$ip&#x27;; $ch = curl_init($url); curl_setopt($ch,CURLOPT_ENCODING ,&#x27;utf8&#x27;); curl_setopt($ch, CURLOPT_TIMEOUT, 10); curl_setopt($ch, CURLOPT_RETURNTRANSFER, true) ; // 获取数据返回 $location = curl_exec($ch); $location = json_decode($location); print_r($location);curl_close($ch); $loc = &quot;&quot;; if($location===FALSE) return &quot;&quot;; if (empty($location-&gt;desc)) &#123; $loc = $location-&gt;province.$location-&gt;city.$location-&gt;district.$location-&gt;isp; &#125;else&#123; $loc = $location-&gt;desc; &#125; echo $loc; //腾讯根据IP获取地理位置API$url = &#x27;http://ip.qq.com/cgi-bin/searchip?searchip1=$ip&#x27;; $ch = curl_init($url); curl_setopt($ch,CURLOPT_ENCODING ,&#x27;gb2312&#x27;); curl_setopt($ch, CURLOPT_TIMEOUT, 10); curl_setopt($ch, CURLOPT_RETURNTRANSFER, true) ; // 获取数据返回 $result = curl_exec($ch); $result = mb_convert_encoding($result, &quot;utf-8&quot;, &quot;gb2312&quot;); // 编码转换，否则乱码 curl_close($ch); preg_match(&quot;@&lt;span&gt;(.*)&lt;/span&gt;&lt;/p&gt;@iU&quot;,$result,$ipArray); $loc = $ipArray[1]; echo $loc;","categories":[{"name":"php","slug":"php","permalink":"https://www.silenceboy.com/categories/php/"}],"tags":[{"name":"php","slug":"php","permalink":"https://www.silenceboy.com/tags/php/"}]},{"title":"express简单测试连接mysql","slug":"express简单测试连接mysql","date":"2016-01-06T14:11:16.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2016/01/06/express简单测试连接mysql/","permalink":"https://www.silenceboy.com/2016/01/06/express%E7%AE%80%E5%8D%95%E6%B5%8B%E8%AF%95%E8%BF%9E%E6%8E%A5mysql/","excerpt":"","text":"使用express应用生成器生成express模板后，先写package.json &#123; &quot;name&quot;: &quot;mysql-test&quot;, &quot;version&quot;: &quot;0.0.1&quot;, &quot;private&quot;: true, &quot;scripts&quot;: &#123; &quot;start&quot;: &quot;node ./bin/www&quot; &#125;, &quot;dependencies&quot;: &#123; &quot;body-parser&quot;: &quot;~1.13.2&quot;, &quot;cookie-parser&quot;: &quot;~1.3.5&quot;, &quot;debug&quot;: &quot;~2.2.0&quot;, &quot;ejs&quot;: &quot;~2.3.3&quot;, &quot;express&quot;: &quot;~4.13.1&quot;, &quot;morgan&quot;: &quot;~1.6.1&quot;, &quot;serve-favicon&quot;: &quot;~2.3.0&quot;, &quot;mysql&quot;:&quot;*&quot; &#125; &#125; npm install安装依赖 新建立两个文件夹，models和config 写一个config配置文件，去连接mysql的: module.exports = &#123; mysql_dev: &#123; host: &#39;localhost&#39;, user: &#39;user&#39;, password: &#39;your password&#39;, database: &#39;your db name&#39;, connectionLimit: 10, supportBigNumbers: true &#125; &#125;; 再写上一个database.js文件： var mysql = require(&#39;mysql&#39;); var config = require(&#39;../config/config&#39;); var pool = mysql.createPool(config.mysql_dev); exports.pool = pool; 在models里建立一个User.js文件作为model： var db = require(&#39;./database&#39;); var User = function() &#123;&#125;; User.prototype.find = function(id, callback) &#123; var sql = &quot;SELECT * FROM users WHERE id =?&quot;; // get a connection from the pool db.pool.getConnection(function(err, connection) &#123; if (err) &#123; callback(true); return; &#125; // make the query connection.query(sql, [id], function(err, results) &#123; if (err) &#123; callback(true); return; &#125; callback(false, results); &#125;); &#125;); &#125;; module.exports = User; 最后在app.js里引入，再调用： var User = require(&#39;./models/User&#39;); //....... app.get(&#39;/users/:userid&#39;,function(req,res)&#123; var userid = req.params.userid; var user = new User(); user.find(userid,function(err,result)&#123; if(err)&#123; res.send(&#39;not found&#39;); &#125; res.send(result.length === 1 ? result[0]:result); &#125;); &#125;); 这样就简单地完成一个后端的node.js分级结构，前端提供rest请求。","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://www.silenceboy.com/tags/mysql/"},{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"},{"name":"express.js","slug":"express-js","permalink":"https://www.silenceboy.com/tags/express-js/"}]},{"title":"Chrome 浏览器中的插件Vimium","slug":"Chrome-浏览器中的插件Vimium","date":"2015-12-24T02:44:59.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2015/12/24/Chrome-浏览器中的插件Vimium/","permalink":"https://www.silenceboy.com/2015/12/24/Chrome-%E6%B5%8F%E8%A7%88%E5%99%A8%E4%B8%AD%E7%9A%84%E6%8F%92%E4%BB%B6Vimium/","excerpt":"","text":"像使用vim一样使用浏览器，完全键盘操作，脱离鼠标。 Vimium 常用的按键功能解释：j：向下细微滚动窗口 k：向上细微滚动窗口J：(Shift+j的意思，以下大写全部表示加Shift) 下一个标签页 K：上一个标签页d：向下滚动半个屏幕 u：向上移动半个屏幕g+g（连续按两下g）：回到顶部G：到达页面底部H：后退 L： 前进f：将当前网页上的所有可见链接&#x2F;输入框分配一个快捷键，输入后就可以打开或者跳转到对应的输入框。如果按的是F，那么将在新窗口中打开页面（见上图）g+i：将光标 定位到输入框，如果有多个可以按Tab键切换x：关闭当前页面 X：恢复刚刚关闭的页面o：相当于Chrome中的地址栏，可以匹配历史记录、收藏夹并在当前窗口打开，或者直接打开一个网址或者搜索一个关键字（Chrome在全屏的时候地址栏死都出不来，有了它就解决这个一直困扰我的问题了！～），如果按的是O，则可以在新窗口中打开，非常非常方便！g+s：查看网页的源代码r：重新载入当前网页（顺便提一句，这点上新浪微博和它是一样的，光标没有定位在发送框时，即便没有安装这个插件你也可以用j&#x2F;k来控制页面上下滚动，用r在刷新，用f或者p来定位到发送框。而Gmail的快捷键如j,k上下移动光标也是类似，有兴趣大家可以再自己去了解一下一些常用web应用的快捷键）Vimium &#x2F; Vimperator 的快捷键远远不止我上面写的这些，想要把它玩得炉火纯青的话，按 shift+&#x2F; （chrome）或者是进入设置页面（firefox+chrome），可以找到更详细的的帮助。甚至，你还可以在设置中按照你自己的习惯替换掉一些键。","categories":[{"name":"chrome","slug":"chrome","permalink":"https://www.silenceboy.com/categories/chrome/"}],"tags":[{"name":"chrome","slug":"chrome","permalink":"https://www.silenceboy.com/tags/chrome/"},{"name":"vim","slug":"vim","permalink":"https://www.silenceboy.com/tags/vim/"}]},{"title":"phpmailer发送邮件 SMTP Error: Could not authenticate 错误","slug":"phpmailer发送邮件-SMTP-Error-Could-not-authenticate-错误","date":"2015-12-24T02:40:37.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2015/12/24/phpmailer发送邮件-SMTP-Error-Could-not-authenticate-错误/","permalink":"https://www.silenceboy.com/2015/12/24/phpmailer%E5%8F%91%E9%80%81%E9%82%AE%E4%BB%B6-SMTP-Error-Could-not-authenticate-%E9%94%99%E8%AF%AF/","excerpt":"","text":"今天在使用phpmailer发送smtp邮件时提示 SMTP Error: Could not authenticate 错误，其中密码帐号都是正确的，邮箱也设置开启了SMTP功能。 上谷歌百度了一遍，有的说是服务器禁用了端口，有的说把class.phpmailer.php中的 代码如下 function IsSMTP() &#123; $this-&gt;Mailer = &#39;smtp&#39;; &#125; 改为 function IsSMTP() &#123; $this-&gt;Mailer = &#39;SMTP&#39;; &#125; （我的问题通过以上修改解决）如果解决不了还有一些解决方法可供参考：这个错误说明虚拟主机不支持PHPMailer默认调用的fsockopen函数，找到class.smtp.php文件，搜索fsockopen，就找到了这样一段代码： 代码如下 // connect to the smtp server $this-&gt;smtp_conn = @fsockopen($host,// the host of the server $port,// the port to use $errno, // error number if any $errstr, // error message if any $tval); // give up after ? secs 方法1：将fsockopen函数替换成pfsockopen函数 首先，在php.ini中去掉下面的两个分号 ;extension=php_sockets.dll ;extension=php_openssl.dll 然后重启一下 因为pfsockopen的参数与fsockopen基本一致，所以只需要将@fsockopen替换成@pfsockopen就可以了。 方法2：使用stream_socket_client函数 一般fsockopen()被禁，pfsockopen也有可能被禁，所以这里介绍另一个函数stream_socket_client()。 stream_socket_client的参数与fsockopen有所不同，所以代码要修改为： 代码如下 $this-&gt;smtp_conn = stream_socket_client(&quot;tcp://&quot;.$host.&quot;:&quot;.$port, $errno, $errstr, $tval); 这样就可以了。","categories":[{"name":"php","slug":"php","permalink":"https://www.silenceboy.com/categories/php/"}],"tags":[{"name":"php","slug":"php","permalink":"https://www.silenceboy.com/tags/php/"}]},{"title":"php中如何使用phpredis","slug":"php中如何使用phpredis","date":"2015-12-24T02:34:10.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2015/12/24/php中如何使用phpredis/","permalink":"https://www.silenceboy.com/2015/12/24/php%E4%B8%AD%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8phpredis/","excerpt":"","text":"安装redis服务：下载地址：http://redis.io/download，下载最新文档版本。本教程使用的最新文档版本为 2.8.17，下载并安装： $ wget http://download.redis.io/releases/redis-2.8.17.tar.gz $ tar xzf redis-2.8.17.tar.gz $ cd redis-2.8.17 $ make make完后 redis-2.8.17目录下会出现编译后的redis服务程序redis-server,还有用于测试的客户端程序redis-cli,两个程序位于安装目录 src 目录下：下面启动redis服务. $ cd src $ ./redis-server 注意这种方式启动redis 使用的是默认配置。也可以通过启动参数告诉redis使用指定配置文件使用下面命令启动。 $ cd src $ ./redis-server redis.conf redis.conf是一个默认的配置文件。我们可以根据需要使用自己的配置文件。启动redis服务进程后，就可以使用测试客户端程序redis-cli和redis服务交互了。 比如： $ cd src $ ./redis-cli redis&gt; set foo bar OK redis&gt; get foo &quot;bar&quot; 安装PHP redis 驱动装 PHP redis 驱动：下载地址为:https://github.com/nicolasff/phpredis。首先git clone 项目到本地，切换到phpredis目录下在shell中输入 phpize 然后 .&#x2F;configure 进行配置（ps:可能找不到phpize，phpize是属于php-devel的内容，因此在centos中只要运行如下命令：yum install php-devel 然后就会安装上phpize了。）接下来就是最后的make 和make install了，make 之后记得跑一下 make test，在make install中遇到点权限问题，所以要加上sudo这样就完成了phpredis的编译工作，接下来我们需要来配置了。 然后，在PHP.INI 配置文件中添加一条extension &#x3D; redis.so 就OK 对了，别忘了重启Apache","categories":[{"name":"php","slug":"php","permalink":"https://www.silenceboy.com/categories/php/"}],"tags":[{"name":"php","slug":"php","permalink":"https://www.silenceboy.com/tags/php/"},{"name":"redis","slug":"redis","permalink":"https://www.silenceboy.com/tags/redis/"}]},{"title":"PHP获取IP地址以及IP地址所在位置","slug":"PHP获取IP地址以及IP地址所在位置","date":"2015-12-22T02:29:45.000Z","updated":"2021-09-27T07:27:40.000Z","comments":true,"path":"2015/12/22/PHP获取IP地址以及IP地址所在位置/","permalink":"https://www.silenceboy.com/2015/12/22/PHP%E8%8E%B7%E5%8F%96IP%E5%9C%B0%E5%9D%80%E4%BB%A5%E5%8F%8AIP%E5%9C%B0%E5%9D%80%E6%89%80%E5%9C%A8%E4%BD%8D%E7%BD%AE/","excerpt":"","text":"获取IP地址： 12345678910111213141516171819202122function getIP()&#123; if (isset($_SERVER)) &#123; if (isset($_SERVER[&#x27;HTTP_X_FORWARDED_FOR&#x27;])) &#123; $realip = $_SERVER[&#x27;HTTP_X_FORWARDED_FOR&#x27;]; &#125; elseif (isset($_SERVER[&#x27;HTTP_CLIENT_IP&#x27;])) &#123; $realip = $_SERVER[&#x27;HTTP_CLIENT_IP&#x27;]; &#125; else &#123; $realip = $_SERVER[&#x27;REMOTE_ADDR&#x27;]; &#125; &#125; else &#123; if (getenv(&quot;HTTP_X_FORWARDED_FOR&quot;)) &#123; $realip = getenv( &quot;HTTP_X_FORWARDED_FOR&quot;); &#125; elseif (getenv(&quot;HTTP_CLIENT_IP&quot;)) &#123; $realip = getenv(&quot;HTTP_CLIENT_IP&quot;); &#125; else &#123; $realip = getenv(&quot;REMOTE_ADDR&quot;); &#125; &#125; return $realip;&#125;echo $ip = getIP(); 新浪接口根据ip查询所在区域信息 1234$res0 = file_get_contents(&quot;http://int.dpool.sina.com.cn/iplookup/iplookup.php?format=json&amp;ip=$ip&quot;);$res0 = json_decode($res0,true);print_r($res0);echo &quot;&lt;br/&gt;&quot;; 淘宝接口根据ip查询所在区域信息 1234$res1 = file_get_contents(&quot;http://ip.taobao.com/service/getIpInfo.php?ip=$ip&quot;);$res1 = json_decode($res1,true);print_r($res1);echo &quot;&lt;br/&gt;&quot;;","categories":[{"name":"php","slug":"php","permalink":"https://www.silenceboy.com/categories/php/"}],"tags":[{"name":"php","slug":"php","permalink":"https://www.silenceboy.com/tags/php/"}]}],"categories":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/categories/AI/"},{"name":"方法论","slug":"方法论","permalink":"https://www.silenceboy.com/categories/%E6%96%B9%E6%B3%95%E8%AE%BA/"},{"name":"autosar","slug":"autosar","permalink":"https://www.silenceboy.com/categories/autosar/"},{"name":"mcp","slug":"mcp","permalink":"https://www.silenceboy.com/categories/mcp/"},{"name":"shell","slug":"shell","permalink":"https://www.silenceboy.com/categories/shell/"},{"name":"python","slug":"python","permalink":"https://www.silenceboy.com/categories/python/"},{"name":"ASIL","slug":"ASIL","permalink":"https://www.silenceboy.com/categories/ASIL/"},{"name":"SoC","slug":"SoC","permalink":"https://www.silenceboy.com/categories/SoC/"},{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/categories/docker/"},{"name":"代码扫描","slug":"代码扫描","permalink":"https://www.silenceboy.com/categories/%E4%BB%A3%E7%A0%81%E6%89%AB%E6%8F%8F/"},{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/categories/go/"},{"name":"linux","slug":"linux","permalink":"https://www.silenceboy.com/categories/linux/"},{"name":"mac","slug":"mac","permalink":"https://www.silenceboy.com/categories/mac/"},{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/categories/nodejs/"},{"name":"mysql","slug":"mysql","permalink":"https://www.silenceboy.com/categories/mysql/"},{"name":"npm","slug":"npm","permalink":"https://www.silenceboy.com/categories/npm/"},{"name":"git","slug":"git","permalink":"https://www.silenceboy.com/categories/git/"},{"name":"ssh","slug":"ssh","permalink":"https://www.silenceboy.com/categories/ssh/"},{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/categories/javascript/"},{"name":"算法","slug":"算法","permalink":"https://www.silenceboy.com/categories/%E7%AE%97%E6%B3%95/"},{"name":"shadowsock","slug":"shadowsock","permalink":"https://www.silenceboy.com/categories/shadowsock/"},{"name":"html5","slug":"html5","permalink":"https://www.silenceboy.com/categories/html5/"},{"name":"vim","slug":"vim","permalink":"https://www.silenceboy.com/categories/vim/"},{"name":"php","slug":"php","permalink":"https://www.silenceboy.com/categories/php/"},{"name":"chrome","slug":"chrome","permalink":"https://www.silenceboy.com/categories/chrome/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://www.silenceboy.com/tags/AI/"},{"name":"方法论","slug":"方法论","permalink":"https://www.silenceboy.com/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/"},{"name":"Agent","slug":"Agent","permalink":"https://www.silenceboy.com/tags/Agent/"},{"name":"RAG","slug":"RAG","permalink":"https://www.silenceboy.com/tags/RAG/"},{"name":"fidl","slug":"fidl","permalink":"https://www.silenceboy.com/tags/fidl/"},{"name":"arxml","slug":"arxml","permalink":"https://www.silenceboy.com/tags/arxml/"},{"name":"mcp","slug":"mcp","permalink":"https://www.silenceboy.com/tags/mcp/"},{"name":"shell","slug":"shell","permalink":"https://www.silenceboy.com/tags/shell/"},{"name":"python","slug":"python","permalink":"https://www.silenceboy.com/tags/python/"},{"name":"ASIL","slug":"ASIL","permalink":"https://www.silenceboy.com/tags/ASIL/"},{"name":"SoC","slug":"SoC","permalink":"https://www.silenceboy.com/tags/SoC/"},{"name":"docker","slug":"docker","permalink":"https://www.silenceboy.com/tags/docker/"},{"name":"代码扫描","slug":"代码扫描","permalink":"https://www.silenceboy.com/tags/%E4%BB%A3%E7%A0%81%E6%89%AB%E6%8F%8F/"},{"name":"go","slug":"go","permalink":"https://www.silenceboy.com/tags/go/"},{"name":"linux","slug":"linux","permalink":"https://www.silenceboy.com/tags/linux/"},{"name":"mac","slug":"mac","permalink":"https://www.silenceboy.com/tags/mac/"},{"name":"天空卫士","slug":"天空卫士","permalink":"https://www.silenceboy.com/tags/%E5%A4%A9%E7%A9%BA%E5%8D%AB%E5%A3%AB/"},{"name":"jdk","slug":"jdk","permalink":"https://www.silenceboy.com/tags/jdk/"},{"name":"ubuntu","slug":"ubuntu","permalink":"https://www.silenceboy.com/tags/ubuntu/"},{"name":"nodejs","slug":"nodejs","permalink":"https://www.silenceboy.com/tags/nodejs/"},{"name":"M1","slug":"M1","permalink":"https://www.silenceboy.com/tags/M1/"},{"name":"i3wm","slug":"i3wm","permalink":"https://www.silenceboy.com/tags/i3wm/"},{"name":"i3","slug":"i3","permalink":"https://www.silenceboy.com/tags/i3/"},{"name":"mysql","slug":"mysql","permalink":"https://www.silenceboy.com/tags/mysql/"},{"name":"java","slug":"java","permalink":"https://www.silenceboy.com/tags/java/"},{"name":"npm","slug":"npm","permalink":"https://www.silenceboy.com/tags/npm/"},{"name":"scp","slug":"scp","permalink":"https://www.silenceboy.com/tags/scp/"},{"name":"zsh","slug":"zsh","permalink":"https://www.silenceboy.com/tags/zsh/"},{"name":"git","slug":"git","permalink":"https://www.silenceboy.com/tags/git/"},{"name":"pm2","slug":"pm2","permalink":"https://www.silenceboy.com/tags/pm2/"},{"name":"ssh","slug":"ssh","permalink":"https://www.silenceboy.com/tags/ssh/"},{"name":"jira","slug":"jira","permalink":"https://www.silenceboy.com/tags/jira/"},{"name":"portainer","slug":"portainer","permalink":"https://www.silenceboy.com/tags/portainer/"},{"name":"gitlab","slug":"gitlab","permalink":"https://www.silenceboy.com/tags/gitlab/"},{"name":"javascript","slug":"javascript","permalink":"https://www.silenceboy.com/tags/javascript/"},{"name":"算法","slug":"算法","permalink":"https://www.silenceboy.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"hexo","slug":"hexo","permalink":"https://www.silenceboy.com/tags/hexo/"},{"name":"express.js","slug":"express-js","permalink":"https://www.silenceboy.com/tags/express-js/"},{"name":"middleware","slug":"middleware","permalink":"https://www.silenceboy.com/tags/middleware/"},{"name":"tailf","slug":"tailf","permalink":"https://www.silenceboy.com/tags/tailf/"},{"name":"gitlab-runner","slug":"gitlab-runner","permalink":"https://www.silenceboy.com/tags/gitlab-runner/"},{"name":"sql server","slug":"sql-server","permalink":"https://www.silenceboy.com/tags/sql-server/"},{"name":"sms","slug":"sms","permalink":"https://www.silenceboy.com/tags/sms/"},{"name":"shadowsock","slug":"shadowsock","permalink":"https://www.silenceboy.com/tags/shadowsock/"},{"name":"dockerfile","slug":"dockerfile","permalink":"https://www.silenceboy.com/tags/dockerfile/"},{"name":"base64","slug":"base64","permalink":"https://www.silenceboy.com/tags/base64/"},{"name":"html5","slug":"html5","permalink":"https://www.silenceboy.com/tags/html5/"},{"name":"录音","slug":"录音","permalink":"https://www.silenceboy.com/tags/%E5%BD%95%E9%9F%B3/"},{"name":"语言识别","slug":"语言识别","permalink":"https://www.silenceboy.com/tags/%E8%AF%AD%E8%A8%80%E8%AF%86%E5%88%AB/"},{"name":"excel","slug":"excel","permalink":"https://www.silenceboy.com/tags/excel/"},{"name":"正则表达式","slug":"正则表达式","permalink":"https://www.silenceboy.com/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"},{"name":"vim","slug":"vim","permalink":"https://www.silenceboy.com/tags/vim/"},{"name":"vi","slug":"vi","permalink":"https://www.silenceboy.com/tags/vi/"},{"name":"buffer","slug":"buffer","permalink":"https://www.silenceboy.com/tags/buffer/"},{"name":"php","slug":"php","permalink":"https://www.silenceboy.com/tags/php/"},{"name":"chrome","slug":"chrome","permalink":"https://www.silenceboy.com/tags/chrome/"},{"name":"redis","slug":"redis","permalink":"https://www.silenceboy.com/tags/redis/"}]}